




==============================
EOS architecture specification
==============================

This is the architecture specification for EOS project. This document specifies
the architecture of EOS software, hardware and also processes involved in
development of EOS, its deployment, maintenance and evolution.

Overall system description
==========================

The goal of EOS is to develop a generic object store solution. An EOS deployment
consists of sofware (to be developed as part of the project) installed on
Seagate hardware assembled on the customer's premises (so called *private
cloud*). Access to the data is through the industry standard interfaces: `S3
<https://en.wikipedia.org/wiki/Amazon_S3>`_ and `NFS
<https://en.wikipedia.org/wiki/Network_File_System>`_. EOS includes `RAS
<https://en.wikipedia.org/wiki/Reliability,_availability_and_serviceability>`_,
provisioning, cluster management and monitoring UI.

**Add some high level marketing justification.**

High level decomposition
========================

At the highest level, EOS consists of software, hardware, and processes for its
development, deployment, maintenance and evolution.

Top-level software components are:

    - mero [`mero <component.mero_>`_]: a scalable storage software providing object store and
      key-value store interfaces;

    - halon [`halon <component.halon_>`_]: a high-availability sub-system that monitors and manages
      other software and hardware components;

    - s3 [`s3 <component.s3_>`_]: an implementation of s3 server using mero to store data
      and meta-data;

    - nfs [`nfs <component.nfs_>`_]: an implementation of nfs server, based on open source
      `Ganesha <https://github.com/nfs-ganesha/nfs-ganesha/wiki>`_, using mero
      to store data and meta-data;

    - provisioner [`provisioner <component.provisioner_>`_];

    - RAS [`ras <component.ras_>`_];

    - management interface (cli and gui) [`managementui <component.managementui_>`_];

    - operating system [`os <component.os_>`_].

The software components listed above are *deployed* software, that is, they
execute on the EOS deployments. In addition, EOS includes *non-deployed software*:

    - test scripts;

    - RE infrastructure;

    - continuous intergration infrastructure;

    - debugging tools;

    - development tools, like VM images.

EOS hardware is organised hierarchically into:

    - a deployment [`deployment <entity.deployment_>`_], which is a collection of

    - clusters [`cluster <entity.cluster_>`_], each corresponding to an s3 region
      [`s3.region <entity.s3.region_>`_], consisting of

    - sites [`site <entity.site_>`_], also known as *data centres*, corresponding to an s3
      *availability zone*, consisting of

    - racks [`rack <entity.rack_>`_], consisting of

      - top-of-the-rack network switches [`switch <entity.switch_>`_],

      - management server nodes (CMU) [`cmu <entity.cmu_>`_],

      - storage servers [`ss <entity.ss_>`_],

      - data enclosures [`enclosure <entity.enclosure_>`_], including

        - PODS controllers [`pods <entity.pods_>`_] to which attached are

	  - storage devices.

Hardware units are *failure domains* [`failuredomain <entity.failuredomain_>`_].

Requirements
============

Architectural diagrams
======================

Component and connector
-----------------------

Use-depends diagram (software components)
-----------------------------------------

Data flow, control flow
-----------------------

Deployment
----------

Components
==========

EOS software is decomposed into components, also called modules. Modules
interact with each other through defined interfaces or protocols, can be
planned, estimated and developed independently.

This section contains informal overview of the components and can be read in
sequential order to get an overview of the eos. Formal descriptions are given in
the appropriate entries of the `Encyclopædia`_.

.. _component.eos:

eos
---

Top-level component, containing all deployed EOS software.

:Parent:
:Sub-components: [`mero <component.mero_>`_], [`s3 <component.s3_>`_], [`nfs <component.nfs_>`_], [`halon <component.halon_>`_],
                 [`provisioner <component.provisioner_>`_], [`ras <component.ras_>`_], [`managementui <component.managementui_>`_], [`os <component.os_>`_]

.. _component.mero:

mero
----

Mero is a distributed storage system, targeting exascale configurations. Its
main roots are Lustre file system, NFSv4 and database technology. Mero is not,
strictly speaking, a file system: an emerging consensus is that traditional file
system properties (hierarchical directory namespace, strong POSIX consistency
guarantees, etc.) are no longer desirable or achievable at exascale. Nor Mero is
an object store. Instead, Mero is a more general storage system, providing an
optional file system and object store interfaces. This allows wider range of
deployments, including cloud.

Mero controls a cluster consisting of nodes connected by network. Some nodes
have persistent storage attached to them. Mero makes distinction between various
types of persistent store:

    - rotational drives. They have low cost per bit, good durability, widely
      available. Their drawbacks include high latency (due to rotational and
      seek delays) and ever decreasing bandwidth-to-capacity ratio;

    - flash drives. They are more expensive per bit, have limited capacity per
      drive, no seek latency, good throughput;

    - non-volatile memory of various types, including PCI-attached flash
      devices, battery-backed memory and phase-change memory. Even more
      expensive and even faster.

Some nodes are running applications [`application <entity.application_>`_], which are processes
external to Mero. Applications issue requests to query or manipulate cluster
state. An application can be a traditional user space application, running
standalone on a node, or a large MPI job running on multiple nodes, or a cluster
management utility monitoring the state of the system, or S3 [`s3 <entity.s3_>`_] or NFS
[`nfs <entity.nfs_>`_] or CIFS daemon exporting Mero to non-Mero clients.

.. image:: images/mero.layers.png

The ultimate effect of operation execution consists of updates and queries to
state on multiple nodes.

Remote execution requires network communication. Mero rpc layer [`rpc <entity.rpc_>`_]
handles certain network failures (message loss) and provides convenient
interface to send items, which are internally packed in network messages.

Distributed transaction manager [`dtm <entity.dtm_>`_] allows operations to be grouped into
distributed transactions, which are guaranteed to be atomic in the face of
certain failures.

Main novel ideas of Mero, that distinguish it from other cluster file-systems or
object stores are:

    - fdmi [`fdmi <entity.fdmi_>`_];

    - file operations log [`fol <entity.fol_>`_];

    - fops [`fop <entity.fop_>`_];

    - distributed transaction manager [`dtm <entity.dtm_>`_];

    - resource manager [`rm <entity.rm_>`_];

    - layouts [`layout <entity.layout_>`_], including parity de-clustering [`pdclust <entity.pdclust_>`_];

    - containers [`container <entity.container_>`_];

    - addb [`addb <entity.addb_>`_].

:Entity: [`mero <entity.mero_>`_]
:Parent: [`eos <component.eos_>`_]
:Sub-components: [`mero.clovis <component.mero.clovis_>`_], [`mero.rpc <component.mero.rpc_>`_], [`mero.reqh <component.mero.reqh_>`_],
                 [`mero.be <component.mero.be_>`_], [`mero.stob <component.mero.stob_>`_], [`mero.dtm <component.mero.dtm_>`_],
                 [`mero.pdclust <component.mero.pdclust_>`_], [`mero.ios <component.mero.ios_>`_], [`mero.dix <component.mero.dix_>`_],
                 [`mero.cob <component.mero.cob_>`_], [`mero.cas <component.mero.cas_>`_], [`mero.cm <component.mero.cm_>`_], [`mero.conf <component.mero.conf_>`_],
                 [`mero.lib <component.mero.lib_>`_], [`mero.addb <component.mero.addb_>`_], [`mero.fol <component.mero.fol_>`_],
                 [`mero.layout <component.mero.layout_>`_], [`mero.pool <component.mero.pool_>`_], [`mero.spiel <component.mero.spiel_>`_],
                 [`mero.balloc <component.mero.balloc_>`_], [`mero.fdmi <component.mero.fdmi_>`_], [`mero.lf <component.mero.lf_>`_],
                 [`mero.fid <component.mero.fid_>`_], [`mero.rm <component.mero.rm_>`_], [`mero.xcode <component.mero.xcode_>`_], [`mero.net <component.mero.net_>`_]
:References:
   - `Mero in prose
     <https://docs.google.com/document/d/1weQya-S3b9uW7whD0I1du2MoXoo90gx6w1kCFvGpavQ>`_
   - `Mero technical (presentation)
     <https://docs.google.com/presentation/d/1VrqkzxklEXnthfsNl3GZhgGaeWl4yTW43LP87EmhzPc>`_
   - `Mero whitepaper
     <https://docs.google.com/document/d/1xM7yYPjPH_AuxiN7-J54s2LANv87BvkqLtdL9bOU-7I>`_

.. _component.s3:

s3
--

:Entity: [`s3 <entity.s3_>`_]
:Parent: [`eos <component.eos_>`_]
:Sub-components:
:References:

.. _component.nfs:

nfs
---

:Entity: [`nfs <entity.nfs_>`_]
:Parent: [`eos <component.eos_>`_]
:Sub-components:
:References:

.. _component.halon:

halon
-----

:Entity: [`halon <entity.halon_>`_]
:Parent: [`eos <component.eos_>`_]
:Sub-components:
:References:

.. _component.provisioner:

provisioner
-----------

:Entity: [`provisioner <entity.provisioner_>`_]
:Parent: [`eos <component.eos_>`_]
:Sub-components:
:References:

.. _component.ras:

ras
---

:Entity: [`ras <entity.ras_>`_]
:Parent: [`eos <component.eos_>`_]
:Sub-components:
:References:

.. _component.managementui:

management ui
-------------

:Entity: [`managementui <entity.managementui_>`_]
:Parent: [`eos <component.eos_>`_]
:Sub-components:
:References:

.. _component.os:

os
--

:Entity: [`os <entity.os_>`_]
:Parent: [`eos <component.eos_>`_]
:Sub-components:
:References:

.. _component.mero.clovis:

mero.clovis
-----------

clovis [`clovis <entity.clovis_>`_] is the external interface exported by mero. Clovis consists
of two main parts:

    - access interface, object IO, key-value indices and distributed
      transactions and

    - fdmi interface [`fdmi <entity.fdmi_>`_] used to extend the core system with new
      functionality.

Clovis is used by applications [`application <entity.application_>`_] including *native* applications
(directly using clovis) and *front-ends*, which are applications exporting
conventional interfaces such as posix, nfs, cifs, s3, etc.

A typical application uses clovis by linking with libmero.so shared library
[`mero.library <entity.mero.library_>`_] and using functions defined in clovis.h header file. A linux
kernel component uses clovis by loading mero kernel module.

:Entity: [`clovis <entity.clovis_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References: `Clovis presentation
             <https://docs.google.com/presentation/d/1gh3qtPK6l69VjORMWkogMCO24kGEJ4dDjIdcNGkSFtk>`_

.. _component.mero.rpc:

mero.rpc
--------

:Entity: [`rpc <entity.rpc_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.reqh:

mero.reqh
---------

:Entity: [`reqh <entity.reqh_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.be:

mero.be
-------

:Entity: [`be <entity.be_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.stob:

mero.stob
---------

:Entity: [`stob <entity.stob_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.dtm:

mero.dtm
--------

:Entity: [`dtm <entity.dtm_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.pdclust:

mero.pdclust
------------

:Entity: [`pdclust <entity.pdclust_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.ios:

mero.ios
--------

:Entity: [`ios <entity.ios_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.dix:

mero.dix
--------

:Entity: [`dix <entity.dix_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.cob:

mero.cob
--------

:Entity: [`cob <entity.cob_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.cas:

mero.cas
--------

:Entity: [`cas <entity.cas_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.cm:

mero.cm
-------

:Entity: [`cm <entity.cm_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.conf:

mero.conf
---------

:Entity: [`conf <entity.conf_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.lib:

mero.lib
--------

:Entity: [`lib <entity.lib_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.addb:

mero.addb
---------

:Entity: [`addb <entity.addb_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.fol:

mero.fol
--------

:Entity: [`fol <entity.fol_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.layout:

mero.layout
-----------

:Entity: [`layout <entity.layout_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.pool:

mero.pool
---------

:Entity: [`pool <entity.pool_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.spiel:

mero.spiel
----------

:Entity: [`spiel <entity.spiel_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.balloc:

mero.balloc
-----------

:Entity: [`balloc <entity.balloc_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.fdmi:

mero.fdmi
---------

:Entity: [`fdmi <entity.fdmi_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.lf:

mero.lf
-------

:Entity: [`lf <entity.lf_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.fid:

mero.fid
--------

:Entity: [`fid <entity.fid_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.rm:

mero.rm
-------

:Entity: [`rm <entity.rm_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.xcode:

mero.xcode
----------

:Entity: [`xcode <entity.xcode_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

.. _component.mero.net:

mero.net
--------

:Entity: [`net <entity.net_>`_]
:Parent: [`mero <component.mero_>`_]
:Sub-components:
:References:

Use cases
=========

.. _usecase.product.life.cycle:

product.life.cycle
------------------

.. uml::

  state "Requirements\n from management" as req
  state Discussion
  state "Architecture specification" as arch
  state Planning
  state Execution

  Discussion: - features
  Discussion: - informal requirements
  Discussion: - informal usecases

  arch: - formal requirements
  arch: - formal usecases
  arch: - components
  arch: - list of options

  Planning: - schedule
  Planning: - list of tasks
  Planning: - list of resources
  Planning: - estimates

  Execution: - HLD
  Execution: - DLD
  Execution: - coding
  Execution: - dev testing
  Execution: - release
  Execution: - QA testing
  Execution: - Patents

  [*] --> req

  req --> Discussion
  Discussion --> req

  Discussion --> arch
  arch --> Discussion

  arch --> Planning
  Planning --> arch

  Planning --> Execution
  Execution --> Planning

.. _usecase.development.cycle:

development.cycle
-----------------

Development process of a software task goes through a number of phases,
described below.

.. uml::

  participant core
  participant team
  participant ci

  core  -> team: overview
  loop learning and clarification
   core <-> team: q&a
  end
  loop
   core  -> team: specification
   core <-> team: q&a
  end
  loop plan adjustment
   core <-  team: execution plan
   core <-> team: discussion
  end
  core  -> team: approval
  loop dld cycle
   team  -> team: dld
   team  -> team: internal inspection
   core <-  team: dld
   core  -> core: dld inspection
   core  -> team: defects
   core <-> team: discussion
  end
  core  -> team: approval
  core  -> core: patents
  loop code cycle
   loop
     core <-  team: intermediate code
     core  -> team: comments
   end
   loop code and ci cycle
     loop code inspection
       team  -> team: ut
       team  -> team: internal inspection
       core <-  team: code
       core <-  core: code inspection
       core  -> team: defects
       core <-> team: discussion
     end
     core  -> team: approval
     team  -> ci: submit
     team <-  ci: failures
     team  -> team: fixes
   end
   core <-  team: ci ok
   core  -> core: land
  end
  core  -> team: approved

Where

**core** are the people responsible for specifying the software task to be done,
controlling the execution and quality. Different members of the core can be
responsible for different aspects of core activity: producing a specification,
inspecting designs and code.

**team** are the people writing the detailed designs, code, doing the testing,
 etc.

**ci** is the continuous integration infrastructure.

**specification** is a document or a set of documents describing the software
task with sufficient detail to start the development. A specification can be
either a high-level design specification [`hld <entity.hld_>`_] or a modification
specification [`modification <entity.modification_>`_]. An hld is used for development of a new
feature [`feature <entity.feature_>`_] or component [`component <entity.component_>`_]. A modification is used for
changing an existing component.

**execution plan** [`execution.plan <entity.execution.plan_>`_] specifies the timeline of the task
execution and required resources. It includes:

    - estimation for execution phases (dld, coding, testing, etc.);

    - division of implementation into smaller sub-tasks usable for continuous
      integration;

    - testing plan.

**dld** [`dld <entity.dld_>`_]

**dld inspection** [`dldinsp <entity.dldinsp_>`_]

**code inspection** [`codeinsp <entity.codeinsp_>`_]

The sequence diagram above is simplified in certain aspects for readability. For
example, it assumes that a dld never changes as a result of coding or testing.

.. _usecase.deployment.life.cycle:

deployment.life.cycle
---------------------

.. uml::

  state Developers
  state Sales
  state Manufacturing
  state Shipment
  state "Assemble on site" as assembly
  state Operation
  state EOL

  Developers --> Sales
  Sales --> Developers
  Sales --> Manufacturing
  Manufacturing --> Shipment
  Shipment --> assembly
  assembly --> Manufacturing
  assembly --> Operation
  Operation --> Sales
  Operation --> EOL

.. _usecase.durability.bitrot:

durability.bitrot
-----------------

This is a case when a bit rot happens on a drive. It's not detected by the drive
itself and there is no error code returned on attempt to read the rotten data.

Bit rot is detected when someone (either application or BSC service) reads the
rotten data. After it's detected BSC service will try to
`repair (bsc.repair) <bsc.repair_>`_ the missing piece using the SNS parity
algorithm.

This diagram describes how the bit rot is detected when a Clovis application
tries to read the data.

.. uml::

  participant drive
  participant ioservice
  participant BSC
  participant Clovis
  participant Application

  note over drive: bit rot happens
  Application -> Clovis: read I/O request
  Clovis -> ioservice: read I/O request
  ioservice -> drive: read stob I/O
  drive -> ioservice: OK
  ioservice -> ioservice: check DI checksum: FAILED
  ioservice -> Clovis: read I/O error
  note over Clovis: continue with degraded read
  Clovis -> Application: read I/O request data
  ioservice -> BSC: repair request
  note over BSC: not execute the request right now
  BSC -> ioservice:

bsc.repair
~~~~~~~~~~

Then BSC service handles the rotten data:

.. uml::

  participant drive
  participant ios_node1
  participant ios_node2
  participant BSC

  note over BSC: repair request for a unit (D or P)

  note over BSC: mark the unit as BAD\n(so subsequent reads return I/O failure)
  BSC -> ios_node1: read a unit
  BSC -> ios_node2: read a unit
  ios_node1 -> BSC: done
  note over BSC: parity calculations for available units
  ios_node2 -> BSC: done
  note over BSC: parity calculations for the unit
  note over BSC: DI check for the unit\n(compare againts the stored data)
  BSC -> drive: write new calculated data to the unit
  drive -> BSC: done
  note over BSC: the repair request is done
  note over drive, BSC
  Error cases
  1. > K failures in the PG for the unit being repaired
  2. Process restart when handling the repair request
  3. DI mismatch (no decision yet)
  4. Intermittent write I/O failure (expander reset etc.) (no decision yet).
  5. Persistence of the repair request (decision: YES).
  6. DI checksum for the DIX keys/values (no decision yet).
  endnote

The rotten data can be overwritten:

.. uml::

  participant Application
  participant Clovis
  participant ioservice
  participant stob

  Application -> Clovis: write I/O request for a unit
  Clovis -> ioservice: write I/O request for the unit
  ioservice -> stob: write I/O request
  note over stob: move the unit to the BAD list
  note over stob: do the write I/O normally
  stob -> ioservice: done
  ioservice -> Clovis: done
  Clovis -> Application: done

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.durability.misdirected.write:

durability.misdirected.write
-----------------------------

This is the case when a write is misdirected to some random physical location
and the metadata maps the objects logical location to different physical location.
In this case the checksum calculated during a typical read io does not match the
one saved in the metadata.

Following diagram illustrates how the misdirected write are handled.

.. uml::

   participant client as C
   participant ios1 as I1
   participant drive1 as D1
   participant ios2 as I2
   participant ios3 as I3
   participant bsc1 as B1
   participant bsc2 as B2
   participant bsc3 as B3

   C -> I1: write data
   I1 -> D1: write submit
   D1 -> D1: write misdirected
   D1 -> I1: write success
   I1 -> C: write success
   C -> I1: Read data
   I1 -> D1: read submit
   D1 -> I1: return data
   I1 -> I1: calculate data checksum
   I1 -> I1: verify calculated checksum with stored checksum
   I1 -> I1: checksum mismatch
   I1 -> B1: submit scrub request
   I1 -> C: return io error
   C -> I2: read remaining data
   C -> I3: read remaining data
   C -> C: reconstruct missing data
   B1 -> B2: read data
   B1 -> B3: read data
   B1 -> B1: reconstruct missing data
   B1 -> I1: write data and checksum to new location
   I1 -> D1: write data to new location
   D1 -> D1: write success
   D1 -> I1: write success
   I1 -> B1: write success
   B1 -> I1: read data and verify checksum

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.metadata-internal.bitrot:

metadata-internal.bitrot
------------------------

This is the case of managing the durability of internal metadata.

.. uml::

   participant Halon as H
   participant mdStore as MD1
   participant ios1 as I1
   participant ios2 as I2
   participant ios3 as I3

   note over MD1: bit rot happens
   MD1 -> H: Notify halon about md failure
   H -> I2: notify I1 failure
   H -> I3: notify I1 failure
   H -> I2: Start direct rebalance
   H -> I3: Start direct rebalance
   == I1 md is restored using direct rebalance ==
   I2 -> H: rebalance complete
   I3 -> H: rebalance complete
   H -> I2: notify I1 online
   H -> I2: notify I1 online

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.clovis.multi-site.create:

clovis.multi-site.create
------------------------

This is the case when clovis create objects on multi-site configured cluster and
target drives (dix) from some of the sites failed to write. Create operation
return with success.

.. uml::

  skinparam ArrowColor Blue
  skinparam BoxPadding 10
  skinparam ParticipantPadding 5

  box "Client"
  participant ClovisApp as ca
  participant Clovis as cc
  end box

  box "<font color=red>Site 1 (Drive Failure)"
  participant IOS1 as i1
  participant DIX1 as d1
  end box

  box "Site2"
  participant IOS2 as i2
  participant DIX2 as d2
  end box

  ca -> cc : Create Object
  note over cc : Get pool version\nas per policy from\nspecified pool\notherwise scan\nall pools
  cc -> d1 : Create meta-data(cob)
  cc -> d2 : Create meta-data(cob)
  note over d1 : MD drive failed
  ca <- cc : Create Object Return
  cc <-[#red]- d1 : meta-data(cob) Fail
  cc <- d2 : meta-data(cob)
  ca <- cc : Create Object executed
  ca <- cc : Create Object stable

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.clovis.multi-site.write:

clovis.multi-site.write
-----------------------

This is the case when clovis writes objects on multi-site configured cluster and
target drives (dix & data) from some of the sites failed to write. Write operation
return with success.

.. uml::

  skinparam ArrowColor Blue
  skinparam BoxPadding 10
  skinparam ParticipantPadding 5

  box "Client"
  participant ClovisApp as ca
  participant Clovis as cc
  end box

  box "<font color=red>Site 1 (Drive Failure)"
  participant IOS1 as i1
  participant DIX1 as d1
  end box

  box "Site2"
  participant IOS2 as i2
  participant DIX2 as d2
  end box

  ca -> cc : Open Object
  cc -> d1 : Open meta-data(cob)
  cc -> d2 : Open meta-data(cob)
  ca <- cc : Open Object Return
  cc <-[#red]- d1 : meta-data(cob) Fail
  cc <- d2 : meta-data(cob)
  ca <- cc : Open Object executed

  ca -> cc : Write Object Chunk 0
  cc -> i1 : Write Object Chunk
  cc -> i2 : Write Object Chunk
  ca <- cc : Write Object Chunk 0 Return
  ...
  ca -> cc : Write Object Chunk n
  cc -> i1 : Write Object Chunk
  cc -> i2 : Write Object Chunk
  ca <- cc : Write Object Chunk n Return
  ...
  cc <-[#red]- i1 : Object Data Chunk 0 Fail
  cc <- i2 : Object Data Chunk 0
  ...
  cc <-[#red]- i1 : Object Data Chunk n Fail
  cc <- i2 : Object Data Chunk n
  ca <- cc : Write Object Chunk 0 executed
  ...
  ca <- cc : Write Object Chunk n executed

  ca -> cc : Sync Init
  cc -> i1 : Sync op
  cc -> d1 : Sync op
  cc -> i2 : Sync op
  cc -> d2 : Sync op
  ca <- cc : sync Init Return
  cc <-[#red]- i1 : Sync op Fail
  cc <-[#red]- d1 : Sync op Fail
  cc <- i2 : Sync op Fail
  cc <- d2 : Sync op
  ca <- cc : Sync Init executed
  ca <- cc : Sync Init stable

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.clovis.multi-site.read:

clovis.multi-site.read
----------------------

This is the case when clovis reads objects on multi-site configured cluster and
some of the sites are down. Read operation return with success.

.. uml::

  skinparam ArrowColor Blue
  skinparam BoxPadding 10
  skinparam ParticipantPadding 5

  box "Client"
  participant ClovisApp as ca
  participant Clovis as cc
  end box

  box "<font color=red>Site 1 (Failure)"
  participant IOS1 as i1
  participant DIX1 as d1
  end box

  box "Site2"
  participant IOS2 as i2
  participant DIX2 as d2
  end box

  ca -> cc : Open Object
  cc -> d2 : Read meta-data(cob)
  ca <- cc : Read Object Return
  cc <- d2 : meta-data(cob)
  ca <- cc : Open Object executed

  ca -> cc : Read Object Chunk 0
  cc -> i2 : Read Object Chunk
  ca <- cc : Read Object Chunk 0 Return
  ...
  ca -> cc : Read Object Chunk n
  cc -> i2 : Read Object Chunk
  ca <- cc : Read Object Chunk n Return
  ...
  cc <- i2 : Object Data Chunk 0
  ...
  cc <- i2 : Object Data Chunk n
  ca <- cc : Read Object Chunk 0 executed
  ...
  ca <- cc : Read Object Chunk n executed

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.clovis.multi-site.delete:

clovis.multi-site.delete
------------------------

This is the case when clovis deletes objects on multi-site configured cluster and
target drives (dix & data) from some of the sites failed to write. Delete
operation return with success.

.. uml::

  skinparam ArrowColor Blue
  skinparam BoxPadding 10
  skinparam ParticipantPadding 5

  box "Client"
  participant ClovisApp as ca
  participant Clovis as cc
  end box

  box "<font color=red>Site 1 (Drive Failure)"
  participant IOS1 as i1
  participant DIX1 as d1
  end box

  box "Site2"
  participant IOS2 as i2
  participant DIX2 as d2
  end box

  ca -> cc : Delete Object
  cc -> d1 : Delete meta-data(cob)
  cc -> d2 : Delete meta-data(cob)
  ca <- cc : Delete Object Return
  cc <-[#red]- d1 : meta-data(cob) Fail
  cc <- d2 : meta-data(cob)
  ca <- cc : Delete Object executed
  ca <- cc : Delete Object stable

.. _usecase.s3.access.single.regions.multisite.replication:

s3.access.single.regions.multisite.replication
----------------------------------------------

1. Customer should be able to deploy Single S3 region with multiple sites.

2. Customer should be able to define implicit replication policy (Sync or Async) for Multiple Sites (AvZ) within a Single Region.
    2.1 Sync replication (1, k) (k parity and k spares required)
    2.2 Async replication (1, k) (k parity and k spares required)

3. Customer should be able to deploy multiple Availability zones (multiple sites under single region).

4. Customer should be able to define deployments with
    4.1 One Region: 1 Site where customer needs just 1 Site(AvZ) per region.
    4.2 or One Region : ‘n’ Sites (Avz)

5. In multisite deployment, client application should be able to write to local site and the data should be eventually available in remote site. (Partitioning use case)

6. In multisite deployment, client application should be able to read from local site.

7. In multisite deployment, (S3/clovis) client application on failure should be able to read from remote available site.

.. image:: images/s3.access.single.regions.multisite.replication.png
`diagram source <https://docs.google.com/document/d/1H4kHFApZhEfbOhd8wbmA_j6DS72BLg_sr0qTuuabAtA/edit?usp=sharing>`_

:Feasibility:

1. Sync replication is feasible if sites have minimal latency (near to local transfer)

    1.1 Active-Passive

    1.2 Degraded operations on Site partition (Read only)

    1.3 Better failover (latest data available)

2. Async replication feasible for higher latency across sites

    2.1 Active - Active

    2.2 Possibility of consistency issues for sites to converge

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.access.single.regions.multisite.erasurecoding:

s3.access.single.regions.multisite.erasurecoding
------------------------------------------------

1. Customer should be able to deploy Single S3 region with multiple sites.

2. Customer should be able to define erasure coding policy for Multiple Sites within a Single Region.
    2.1 (n, k) (n data, k parity and k spares sites required) (total n + 2k sites)

3. In single region multisite deployment, client application should be able to perform degraded write to available sites.

4. In single region multisite deployment, client application should be able to read from available sites and tolerate 'k' site failures.

.. image:: images/s3.access.single.regions.multisite.erasurecoding.png
`diagram source <https://docs.google.com/document/d/1H4kHFApZhEfbOhd8wbmA_j6DS72BLg_sr0qTuuabAtA/edit?usp=sharing>`_

:Feasibility:

1. Feasible with very low latency across sites
2. Better Availability with site failure
3. Very high cost solution due to ‘n’ sites?

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.access.multi.regions.crr.single.site:

s3.access.multi.regions.crr.single.site
---------------------------------------

1. Customer should be able to deploy Multiple S3 regions with each region having single site.

2. Customer should be able to define explicit replication policy (Async) for Multiple regions. Explicit is cross-region replication within S3-mero.

.. image:: images/s3.access.multi.regions.crr.single.site.png
`diagram source <https://docs.google.com/document/d/1H4kHFApZhEfbOhd8wbmA_j6DS72BLg_sr0qTuuabAtA/edit?usp=sharing>`_

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.access.multi.regions.crr.multisite:

s3.access.multi.regions.crr.multisite
-------------------------------------

1. Customer should be able to deploy Multiple S3 regions with each region having single site.

2. Customer should be able to define explicit replication policy (Async) for Multiple regions. Explicit is cross-region replication within S3-mero.

.. image:: images/s3.access.multi.regions.crr.multisite.png
`diagram source <https://docs.google.com/document/d/1H4kHFApZhEfbOhd8wbmA_j6DS72BLg_sr0qTuuabAtA/edit?usp=sharing>`_

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.access.regions.mixed.policies:

s3.access.regions.mixed.policies
--------------------------------

Customer should be able to define custom data protection policy, example erasure coding in local site and replication across sites. [CONFIRM requirement?]. (Advantage increases probability of local access over remote, over the cost of more storage use)

.. image:: images/s3.access.regions.mixed.policies.png
`diagram source <https://docs.google.com/document/d/1H4kHFApZhEfbOhd8wbmA_j6DS72BLg_sr0qTuuabAtA/edit?usp=sharing>`_

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.bucket.put.tagging:

s3.bucket.put.tagging
---------------------

Adding a set of tags to an existing bucket.

.. uml::

  title PUT Bucket Tagging
  ' skinparam backgroundColor #red
  skinparam sequenceArrowThickness 2
  skinparam maxmessagesize 60
  ' skinparam sequenceParticipant underline
  ' skinparam ParticipantPadding 20
  skinparam BoxPadding 10
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-Dix" as E

  A->B:PUT Bucket Tag
  B->C:PUT Bucket Tag
  C->D: Do Authentication/Authorization
  C<--D: 200 OK
  C->E: Get Bucket keyval
  E-->C: Response Success
  C->E: PUT KeyVal(Bucket Tag)
  E-->C: Response 204/Error
  C-->B: Response 204/Error
  B-->A: Response 204/Error

.. _usecase.s3.bucket.get.tagging:

s3.bucket.get.tagging
---------------------

Return the tag set associated with the bucket.

.. uml::

  title GET Bucket Tagging
  ' skinparam backgroundColor #red
  skinparam sequenceArrowThickness 2
  skinparam maxmessagesize 60
  ' skinparam sequenceParticipant underline
  ' skinparam ParticipantPadding 20
  skinparam BoxPadding 10
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-Dix" as E

  A->B:GET Bucket Tag
  B->C:GET Bucket Tag
  C->D: Do Authentication/Authorization
  C<--D: OK
  C->E: Get Bucket keyval
  E-->C: Response Success/Error
  rnote over "C"
  Convert to XML
  endrnote

  C-->B: Response Success/Error
  B-->A: Response Success/Error

.. _usecase.s3.bucket.delete.tagging:

s3.bucket.delete.tagging
------------------------

Remove a tag set from the specified bucket.

.. uml::

  title DELETE Bucket Tagging
  ' skinparam backgroundColor #red
  skinparam sequenceArrowThickness 2
  skinparam maxmessagesize 60
  ' skinparam sequenceParticipant underline
  ' skinparam ParticipantPadding 20
  skinparam BoxPadding 10
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-Dix" as E

  A->B:DELETE Bucket Tag
  B->C:DELETE Bucket Tag
  C->D: Do Authentication/Authorization
  C<--D: OK
  C->E: Update/Put Bucket keyval
  E-->C: Response Success/Error
  C-->B: Response Success/Error
  B-->A: Response Success/Error

.. _usecase.s3.acl:

s3.acl
------

ACLs (access control lists) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which accounts or groups are granted access and the type of access.

Below diagram describes how to create acl associated with an s3 bucket

.. uml::

  title Bucket ACL creation in S3
  participant "Client" as A
  participant "LB" as B
  participant "S3" as C
  participant "S3auth" as D
  A->B:Create Bucket
  B->C:Create Bucket
  C->D:Do auth
  C<--D:200 OK
  note over C:Bucket is created with\ndefault Bucket ACL \nFULL_CONTROL to owner.
  B<--C:200 OK
  A<--B:200 OK
  A->B:Create Bucket ACL
  B->C:Create Bucket ACL
  C->D:Do auth
  C<--D:200 OK
  note over C:Bucket ACL updated/created.
  B<--C:200 OK
  A<--B:200 OK

This diagram describes acl creation associated with s3 object.

.. uml::

  title Object ACL creation in S3
  participant "Client" as A
  participant "LB" as B
  participant "S3" as C
  participant "S3auth" as D
  A->B:PUT Object ACL(Request Body)
  B->C:PUT Object ACL(Request body)
  C->D:Do auth
  C<--D:200 OK
  note over C:Object ACL created/updated.
  B<--C:200 OK
  A<--B:200 OK
  A->B:PUT Object (Canned ACL, Request header)
  B->C:PUT Object (Canned ACL, Request header)
  C->D:Do auth
  C<--D:200 OK
  note over C:Object created with canned ACL.\n and default ACL owner FULL_CONTROL
  B<--C:200 OK
  A<--B:200 OK

:Entities:
* Resources
    * Bucket
    * Object
* Account/Users
    * Owner
    * Grantee
* Permissions
    * Object
       * READ: Allows grantee to read object data and metadata
       * READ_ACP: Allows grantee to read object ACL
       * WRITE_ACP: Allows grantee to write object ACL
       * FULL_CONTROL: Allowd grantee the READ, READ_ACP, WRITE_ACP permissions for object
    * Bucket
       * READ: Allows grantee to list objects in bucket
       * WRITE: Allows grantee to create, overwrite, and delete any objects in the bucket
       * READ_ACP: Allows grantee to read bucket ACL
       * WRITE_ACP: Allows grantee to write bucket ACL
       * FULL_CONTROL: Allowd grantee the READ, WRITE, READ_ACP, WRITE_ACP permissions on the bucket
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.bucketpolicy:

s3.bucketpolicy
---------------

A bucket policy is a resource-based Identity and Access Management (IAM) policy. Bucket policy is used to grant other accounts or IAM users access permissions for the bucket and the objects in it.

.. image:: images/s3.policy_evaluation.png

Below diagram describes bucket policy creation in s3.

.. uml::

  title Bucket Policy creation in S3
  participant "Client" as A
  participant "LB" as B
  participant "S3" as C
  participant "S3auth" as D
  A->B:PUT Bucket Policy
  B->C:PUT Bucket Policy
  C->D:Do auth
  C<--D:200 OK
  note over C:Bucket Policy created/updated\n and saved with bucket metadata.
  B<--C:200 OK
  A<--B:200 OK

Following diagram describes how an bucket policy is evaluated in s3.

.. uml::

  title Policy Evaluation/Autorization in S3
  participant "Client" as A
  participant "LB" as B
  participant "S3" as C
  participant "S3auth" as D
  A->B:Get Object
  B->C:Get Object
  C->D:Do auth
  C<--D:200 OK
  C<--C:Gather Request context
  D<-C:Authorize(list of policy/acl)
  C<--D:200 OK
  B<--C:200 OK
  A<--B:200 OK

:Entities:
* Resources
    * Bucket
    * Object
* Actions
    * Permissions for Operations

     - s3:AbortMultipartUpload
     - s3:DeleteObject
     - s3:DeleteObjectTagging
     - s3:DeleteObjectVersion
     - s3:DeleteObjectVersionTagging
     - s3:GetObject
     - s3:GetObjectAcl
     - s3:GetObjectTagging
     - s3:GetObjectVersion
     - s3:GetObjectVersionAcl
     - s3:GetObjectVersionTagging
     - s3:ListMultipartUploadParts
     - s3:PutObject
     - s3:PutObjectAcl
     - s3:PutObjectTagging
     - s3:PutObjectVersionAcl
     - s3:PutObjectVersionTagging
     - s3:RestoreObject

* Effect
    * Allow or Deny
* Principal
    * Account/User

:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.iamuserpolicy:

s3.iamuserpolicy
---------------

IAM user policy specify what operations are allowed or denied on resources of service like S3. IAM user policies are attached to IAM users, groups, or roles. It defines what a user can do in a environment.
IAM policy are used for managing access to multiple services in a environment. In EOS access management is required only for S3 service, hence IAM polcy is currently not supported. All S3 service access management can by done using ACL and Bucket policy.

.. _usecase.s3.crr:

s3.crr
------

Cross-region replication (CRR) enables automatic, asynchronous copying of objects across buckets in different Regions. Buckets configured for cross-region replication can be owned by the same account or by different accounts.

.. image:: images/s3.crr.png

In case of setting s3 policies for CRR, there may be two approaches which can be
followed.

:Approach 1:
S3server sends REST API request to remote S3server(S3server in another Region)
to get remote bucket metadata or to trigger COPY Object. This approach is secure.

:Approach 2:
S3server uses Mero to get remote bucket metadata or to trigger COPY Object.

Approach 1 is recommended as it seems to be secure.

.. _usecase.s3.crr.put.policies:

s3.crr.put.policies
-------------------

:Approach 1:

This is the case when we want to enable Cross Region replication(CRR), we add
a replication configuration to our source bucket, the configuration tells the
S3Server to replicate objects as specified. In the replication configuration we
provide the destination bucket and the objects we want to replicate. PUT Bucket
Policies either creates a replication policies or replaces existing one.

This diagram describes how the s3 CRR policy is applied to the source bucket.

.. uml::

  skinparam BoxPadding 10
  box "Region1"
  participant "S3Client1"
  participant "Haproxy1"
  participant "S3Server1"
  participant "S3AuthServer1"
  participant "Mero-IOS1"
  participant "Mero-Dix1"
  end box

  box "Region2"
  participant "S3Server2"
  participant "S3AuthServer2"
  participant "Mero-IOS2"
  participant "Mero-Dix2"
  end box
  == Apply PUT CRR Policy on source bucket(Bucket1)==
  "S3Client1"->"Haproxy1":PUT Replication Policy
  "Haproxy1"->"S3Server1":PUT Replication Policy
  "S3Server1"->"S3AuthServer1": Do Authentication/Authorization
  "S3AuthServer1"-->"S3Server1": Response (Success)
  "S3Server1"->"Mero-Dix1":Get-KeyVal(Bucket1)
  "S3Server1"<--"Mero-Dix1": Response(Success)
  "S3Server1"->"S3Server2":GetVersion by S3server
  "S3Server2"->"S3AuthServer2": Do Auth
  "S3AuthServer2"-->"S3Server2": Response(Success)
  "S3Server1"<--"S3Server2": Response(Success)
  rnote over "S3Server1"
  Validation:
    1) Check whether on source(Bucket1) and destination bucket(Bucket2), versioning enabled or not.
    2) Check buckets are in different region or not.
    3) Permission to replicate objects from the source to the destination bucket.
  endrnote
    "S3Server1"->"Mero-Dix1":Put-KeyVal(Bucket1)
    "S3Server1"<--"Mero-Dix1": Response(Success)
    "Haproxy1"<--"S3Server1":Response (Success)
    "S3Client1"<--"Haproxy1":Response - OK
  == CRR Policy applied on Source Bucket(Bucket1) ==

.. _usecase.s3.crr.put.object:

s3.crr.put.object
-----------------

:Approach 1:

This is the case when we PUT object to the source bucket
which has CRR policy set. Async replication of the uploaded
object takes place to the remote bucket,after the object is uploaded
locally.

Below diagram describes how replication happens to the destination bucket
once user has already applied the replication policy to the source bucket.

.. uml::

  title Cross Region Replication - PUT Object
  ' skinparam backgroundColor #red
  skinparam sequenceArrowThickness 2
  ' skinparam roundcorner 20
  ' skinparam maxmessagesize 60
  ' skinparam sequenceParticipant underline
  ' skinparam ParticipantPadding 20
  skinparam BoxPadding 10
  box "Region 1"
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "Mero-ios" as D
  participant "Mero-dix" as E
  end box

  box "Region 2"
  participant "S3Server" as F
  participant "Mero-ios" as G
  participant "Mero-dix" as H
  end box
  == Object creation on region1 start ==
  A->B:PUT Object
  activate B
  B->C:PUT Object
  activate C
  C->E:Get-KV(B)
  C<--E: Response(success)
  |||
  C->D:Create object
  C<--D:Response Ok
  |||
  note over C, D
  Write Data
  end note
  |||
  C->E:Put-KV(O)
  C<--E: PUT - Kv Response(success)
  |||
  B<--C:PUT Object Response - OK
  A<--B:PUT Object Response - OK
  note over C, C
  Check for CRR policy enabled or not.
  Do below validation, if enabled.
  1. Bucket versioning on Source bucket.
  end note
  |||
  C->E:PUT source and dest object in KV for replication - Put-KV
  C<--E: PUT - Kv Response(success)
  |||
  deactivate B
  == Object creation on region1 end ==
  ...
  == Replicate object to region2 start ==
  |||
  C->E:GET source and dest object in KV for replication - GET-KV
  C<--E: GET - KV Response(success)
  |||
  C->F:GET Bucket(B') Metadata
  C<--F: Response GET Bucket(B') Metadata
  note over C, C
  Check for Bucket versioning on Dest. bucket
  end note
  |||
  C->D:open object
  C<--D:Response Ok
  |||
  loop Replicate the data to Specified region Bucket
  |||
  C->D:read object
  C<--D:Response Ok
  |||
  C->F:PUT object
  F->H:Get-KV(B')
  F<--H: Response(success)
  |||
  F->G:Create object if it doesn't exists
  F<--G:Response Ok
  |||
  note over F, G
  Write Data
  end note
  |||
  F->H:Put-KV(O)
  F<--H: PUT - Kv Response(success)
  |||
  C<--F:Response Ok
  |||
  end
  C->E:PUT-KV(O)
  C<--E: PUT - KV Response(success)
  |||
  C->E: update repilication status in source object metedata: PUT-KV
  C<--E: PUT - Kv Response(success)
  |||
  C->E:Delete source and dest object in KV for replication - Delete-KV
  C<--E: Delete - KV Response(success)
  |||
  destroy C
  == Replicate object to region2 end ==

:Approach 2:

This is the case when we PUT object to the source bucket
which has CRR policy set. Async replication of the uploaded
object takes place to the remote bucket,after the object is uploaded
locally. In this approach communication happens between local Mero(Region1) and remote
Mero(Region 2) to get the remote metadata or object replication.

Below diagram describes how replication happens to the destination bucket
once user has already applied the replication policy to the source bucket.

.. uml::

  title Cross Region Replication - PUT Object
  ' skinparam backgroundColor #red
  skinparam sequenceArrowThickness 2
  ' skinparam roundcorner 20
  ' skinparam maxmessagesize 60
  ' skinparam sequenceParticipant underline
  ' skinparam ParticipantPadding 20
  skinparam BoxPadding 10
  box "Region 1"
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "Mero-ios" as D
  participant "Mero-dix" as E
  end box

  box "Region 2"
  participant "S3Server" as F
  participant "Mero-ios" as G
  participant "Mero-dix" as H
  end box
  == Object creation on region1 start ==
  A->B:PUT Object
  B->C:PUT Object
  |||
  C->E:Get bucket metadata: Get-KV(B)
  C<--E: Get bucket metadata: Response(success)
  |||
  C->D:Create object
  C<--D:Response - Ok
  |||
  loop Write data
  |||
  C->D: Write Data
  C<--D: Write Data - Response -Ok
  |||
  end
  |||
  C->E: Save object metadata: PUT - KV(O)
  C<--E: Save object metadata: PUT - KV Response(success)
  |||
  B<--C:PUT Object Response - OK
  A<--B:PUT Object Response - OK
  note over C, C
  Check for CRR policy enabled or not.
  Do below validation, if enabled.
  1. Bucket versioning on Source bucket.
  end note
  |||
  C->E:PUT source(B/O) and dest object(B'/O) in KV for replication - PUT-KV
  C<--E: PUT - KV Response(success)
  |||
  == Object creation on region1 end ==
  ...
  == Replicate object to region2 start ==
  |||
  C->E:GET source and dest object in KV for replication - Put-KV
  C<--E: GET - KV Response(success)
  |||
  C->H:GET Bucket(B') Metadata
  C<--H: Response GET Bucket(B') Metadata
  note over C, C
  Check for Bucket versioning on Dest. bucket
  end note
  |||
  C->D:open object
  C<--D:Response Ok
  |||
  C->D: copy object
  note over D, H
  Replcate object using mero
  end note
  C<--D: copy object - Response - Ok
  |||
  C->E: update repilication status in source object metedata: PUT-KV
  C<--E: PUT - KV Response(success)
  |||
  C->E:Delete source and dest object in KV for replication - Delete-KV
  C<--E: Delete - KV Response(success)
  |||
  == Replicate object to region2 end ==

.. _usecase.s3.crr.get.policies:

s3.crr.get.policies
-------------------

This is the case when user wants to know what CRR policies
are set on the source bucket

.. uml::

  title Cross Region Replication - PUT Object
  box "Region1"
  participant "S3Client"
  participant "Haproxy"
  participant "S3Server"
  participant "S3AuthServer"
  participant "Mero-IOS1"
  participant "Mero-Dix1"
  end box


  "S3Client"->"Haproxy":GET Replication Policy
  "Haproxy"->"S3Server":GET Replication Policy
  "S3Server"->"S3AuthServer": Do Authentication/Authorization
  "S3AuthServer"-->"S3Server": Response (Success)
  "S3Server"->"Mero-Dix1":Get-KeyVal(Bucket1)
  "S3Server"<--"Mero-Dix1": Response(Success)
  "Haproxy"<--"S3Server": Response(Success)
  "S3Client"<--"Haproxy": Response(Success)

.. _usecase.s3.crr.delete.policies:

s3.crr.delete.policies
----------------------

This is the case when user wants to delete the CRR policy
set to the source bucket earlier

.. uml::

  box "Region1"
  participant "S3Client"
  participant "Haproxy"
  participant "S3Server"
  participant "S3AuthServer"
  participant "Mero-IOS1"
  participant "Mero-Dix1"
  end box

  "S3Client"->"Haproxy":DELETE Replication Policy
  "Haproxy"->"S3Server":DELETE Replication Policy
  "S3Server"->"S3AuthServer": Do Authentication/Authorization
  "S3AuthServer"-->"S3Server": Response (Success)
  "S3Server"->"Mero-Dix1":Get-KeyVal(Bucket1)
  "S3Server"<--"Mero-Dix1": Response(Success)
  "S3Server"->"Mero-Dix1":Update-KeyVal(Bucket1)
  "S3Server"<--"Mero-Dix1": Response(Success)

.. _usecase.s3.versioning:

s3.versioning
-------------

Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.

s3.bucket.put.versioning
~~~~~~~~~~~~~~~~~~~~~~~~

PUT operation uses the versioning subresource to set the versioning state of an existing bucket. To set the versioning state, you must be the bucket owner.

You can set the versioning state with one of the following values:
	- Enabled:Enables versioning for the objects in the bucket. All objects added to the bucket receive a unique version ID.
	- Suspended: Disables versioning for the objects in the bucket. All objects added to the bucket receive the version ID null.

.. uml::

  title PUT Bucket versioning
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-ios" as E
  participant "Mero-dix" as F

  A->B:PUT Bucket versioning
  B->C:PUT Bucket versioning
  C->D: Do auth
  C<--D: 200 OK
  C->F: Get-KV(bucket)
  C<--F: Response - Get-KV(bucket)
  note over C: Update versioning metadata
  C->F: PUT-KV (bucket)
  C<--F: PUT KV (bucket) Response - OK
  B<--C: Response - 200 Ok
  A<--B: Response - 200 Ok

s3.bucket.get.versioning
~~~~~~~~~~~~~~~~~~~~~~~~

GET operation uses the versioning subresource to return the versioning state of a bucket. To retrieve the versioning state of a bucket, you must be the bucket owner.

.. uml::

  title GET Bucket versioning
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-ios" as E
  participant "Mero-dix" as F

  A->B:GET Bucket versioning
  B->C:GET Bucket versioning
  C->D: Do auth
  C<--D: 200 OK
  C->F: Get-KV(bucket)
  C<--F: Response - Get-KV(bucket)
  note over C: get bucket versioning metadata information
  B<--C: (Versioning info) Response - 200 Ok
  A<--B: (Versioning info) Response - 200 Ok

s3.object.put.versioning
~~~~~~~~~~~~~~~~~~~~~~~~

If you enable versioning for a bucket, S3 automatically generates a unique version ID for the object being stored. S3 returns this ID in the response using the x-amz-version-id response header. If versioning is suspended, S3 always uses null as the version ID for the object stored.

.. uml::

  title PUT Object versioning
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-ios" as E
  participant "Mero-dix" as F

  A->B:PUT Object
  B->C:PUT Object
  C->D: Do authentication
  C<--D: 200 OK
  C->F: Get-KV(bucket)
  C<--F: Response - Get-KV(bucket)
  note over C: Check versioning enabled or not.\n1. If enabled, generates a unique version ID\nfor the object being stored.\n2. If Disabled,  always use null as the version ID\nfor the object stored.
  C->E:Create object
  C<--E:Responce Ok
   loop Write data
    |||
    A->B: Incoming Write Data
    B->C: Incoming Write Data
    C->E: Write Data
    C<--E: Write Data - Response - Ok
    |||
    end
  alt versioning == true
    C -> C : generate unique VersionID
  else versioning == false
    C -> C : Set VersionID to null
  end
  C->F: Put-KV(Object Index: versionID-->metadata)
  C<--F: Response - Put-KV(Object Index: versionID-->metadata))=
  C->F: Put-KV (update latest object in object list index)
  C<--F: Response - Put-KV (update latest object in object list index)
  B<--C: (Versioning info) Response - 200 Ok
  A<--B: (Versioning info) Response - 200 Ok

s3.object.get.versioning
~~~~~~~~~~~~~~~~~~~~~~~~

By default, the GET operation returns the current version of an object. To return a different version, use the versionId subresource.

.. uml::

  title Get Object of specific version
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-ios" as E
  participant "Mero-dix" as F

  A->B:Get Object of specific version
  B->C:Get Object of specific version
  C->D: Do authentication
  C<--D: 200 OK
  C->F: Get-KV(bucket)
  C<--F: Response - Get-KV(bucket)
  C->F: Get-KV(object metadata)
  C<--F: Response - Get-KV(object metadata)
  alt given versioniID == avialable
      loop read data
      |||
      C->E: Read Data
      C<--E: Read Data - Response - Ok
      B<--C: send Data
      A<--B: send Data
      |||
      end
      B<--C: Response - 200 Ok
      A<--B: Response - 200 Ok
  else given versioniID != avialable
   B<--C: (NoSuchObject with given verison) Response - 404 Not Found 
   A<--B: (NoSuchObject with given verison) Response - 404 Not Found 
  end

s3.object.delete.versioning
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The DELETE operation removes the null version (if there is one) of an object and inserts a delete marker, which becomes the current version of the object. If there isn't a null version, S3 does not remove any objects.

To remove a specific version permanently, you must be the bucket owner and you must use the versionId subresource.

.. uml::

  title DELETE Object
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-ios" as E
  participant "Mero-dix" as F

  A->B:DELETE Object
  B->C:DELETE Object
  C->D: Do authentication
  C<--D: 200 OK
  C->F: Get-KV(bucket)
  C<--F: Response - Get-KV(bucket)
  C->F: Get-KV(object metadata)
  C<--F: Response Ok- Get-KV(object metadata)
  alt versioning == true
    C -> C : generate unique VersionID
    note over C: Insert delete marker with newly generated VersionID
    C->F: Put-KV(Object Index: versionID-->metadata)
    C<--F: Response - Put-KV(Object Index: versionID-->metadata))
  else versioning == false (currently is suspended, but previously enabled)
    C -> C : Set VersionID to null
    note over C:  Removes the null version (if there is one) of an object\n and inserts a delete marker.
    C->F: Put-KV(Object Index: versionID-->metadata)
    C<--F: Response - Put-KV(Object Index: versionID-->metadata))
  else versioning == false (Never enabled)
    note over C: Delete the object
    C->E: Delete the object
    C->E: Responce - OK
    C->F: Delete-KV(remove meatdata)
    C->F: Responce - OK
  end
  B<--C: Response - 200 Ok
  A<--B: Response - 200 Ok 

.. _usecase.s3.throttling:

s3.throttling
-------------

Throttling is a process that is used to control the usage of GET/PUT object APIs by accounts during a given period.

.. uml::

  title Throttling
  skinparam sequenceArrowThickness 2
  skinparam BoxPadding 10
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero" as E

  A->B:PUT/GET Object request
  B->C:PUT/GET Object request
  C->D: Check authentication
  C<--D: 200 OK
  note over C,C: Get account details
  C --> C : Check Account throttling
  alt !throttle
    C -> E : Process request
    C <-- E : Response - OK
    B <-- C : Response - OK
    A <-- B : Response - OK
  else throttle
    B <-- C : Response - SlowDown
    A <-- B : Response - SlowDown
  end

.. _usecase.s3.copy.object:

s3.copy.object
--------------

The copy operation creates a copy of an object that is already stored in Mero. One can create
a copy of the object upto some size (To be defined) in a single atomic operation. For copying an object that
is greater than some size (To be defined), one must use multipart upload API. Below are the some of the uses
of copy operation.

 - Create additional copies of objects
 - Rename objects by copying them and deleting original ones
 - Change object metadata

.. uml::

  title Copy Object in single atomic operation
  skinparam sequenceArrowThickness 2
  skinparam BoxPadding 10
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-ios" as E
  participant "Mero-dix" as F

  note over A, B: PUT /destinationObject HTTP/1.1\nHost: destinationBucket.s3.amazonaws.com\nx-amz-copy-source: /source_bucket/sourceObject
  A->B:PUT Object - Copy
  B->C:PUT Object - Copy
  C->D: Do auth
  C<--D: 200 OK
  C->F: Get-KV(source_bucket)
  C<--F: Response - Get-KV(source_bucket)
  C->F: Get-KV(sourceObject)
  C<--F: Response - Get-KV(sourceObject)
  C->F: Get-KV(destinationbucket)
  C<--F: Response - Get-KV(destinationbucket)
  note over C: validations:\n1. READ access to the source object\n2. WRITE access to the destination bucket
  C->E: Copy_Object (source_object_oid, destination_object_oid)
  C<--E: Response - Copy_Object
  C->F: PUT-KV (destination_bucket/destination_object)
  C<--F: PUT KV Response - OK
  B<--C: Response - 201 Ok
  A<--B: Response - 201 Ok

.. _usecase.s3.copy.part:

s3.copy.part
------------

For copying an existing object one may use the Upload Part (Copy) API and specify the source object by adding
the x-amz-copy-source request header in the request. First we initiate multipart upload, in response we
get upload id. Then upload a part by copying data from an existing object as data source.

.. uml::

  title Copy part (Multipart)
  skinparam sequenceArrowThickness 2
  skinparam BoxPadding 10
  participant "S3Client" as A
  participant "HAproxy" as B
  participant "S3Server" as C
  participant "S3Auth" as D
  participant "Mero-ios" as E
  participant "Mero-dix" as F

  A->B:Initiate Multipart Upload
  B->C:Initiate multipart Upload
  C->D: Do auth
  C<--D: 200 OK
  C->F: Get-KV(source_bucket)
  C<--F: Response - Get-KV(source_bucket)
  C->F: Get-KV(sourceObject)
  C<--F: Response - Get-KV(sourceObject)
  C->F: Get-KV(destinationbucket)
  C<--F: Response - Get-KV(destinationbucket)
  note over C: Check for write permission on destination bucket
  C->E: Create destination Object (oid2)
  E-->C: Response OK
  C->B: Upload ID
  B->A: Upload ID
  loop as many parts, first part copy request will be of part 1
  A->B: Part Copy (byte range/entire source)
  note over A: PUT /ObjectName?partNumber=PartNumber&uploadId=UploadId HTTP/1.1\nHost: BucketName.s3.seagate.com\nx-amz-copy-source: /source_bucket/sourceObject\n x-amz-copy-source-range:bytes=first-last
  B->C: Part Copy (byte range/entire source)
  C->D: Do Authentication/Authorization
  C->E: Get-KV (source object OID oid1)
  E-->C: Response ok
  C->E: Get-KV target object
  E-->C: Response Ok
  note over C: 1. Check read permission on source object and write permission on target object\n2. Compute offset based on part number
  C->E: Copy_Object(oid1, oid2, offset, bytes-size)
  C<--E: Response - Copy_Object
  C->F: PUT-KV (destination_bucket/destination_part details)
  C<--F: PUT KV Response - OK
  B<--C: Response - 200 Ok
  A<--B: Response - 200 Ok
  end
  A->B: Complete Multipart Upload
  B->C: Complete Multipart Upload
  note over C: Do part validation
  alt part validation successful case
  C->E: Remove part and multipart metadata
  E-->C: Response OK
  C->F: PUT-KV(destination_bucket/destination_object)
  F-->C: Response OK
  C-->B: Response OK
  B-->A: Response OK
  else some kind of failure
  C->E: Do cleanup -- delete oid2, multipart metadata, part metadata
  E-->C: Response OK
  C-->B: Error message
  B-->A: Error message
  end

.. _usecase.s3.hybridcloud.replication:

s3.hybridcloud.replication
--------------------------

Customer should be able to define explicit replication policy (Async) for external cloud as target (ex. AWS). Explicit is Bucket level replication to external cloud.

.. image:: images/s3.hybridcloud.replication.png
`diagram source <https://docs.google.com/document/d/1H4kHFApZhEfbOhd8wbmA_j6DS72BLg_sr0qTuuabAtA/edit?usp=sharing>`_

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.hybridcloud.tiering:

s3.hybridcloud.tiering
----------------------

Customer should be able to restore from bucket replicated to external cloud in case Object is lost from local S3-mero cluster. Should this be left to application??

.. image:: images/s3.hybridcloud.tiering.png
`diagram source <https://docs.google.com/document/d/1H4kHFApZhEfbOhd8wbmA_j6DS72BLg_sr0qTuuabAtA/edit?usp=sharing>`_

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.sse.s3managedkeys:

s3.sse.s3.managedkeys
---------------------

S3 client application should be able to enable server side encryption for each object.
S3 PUT api provides a mechanism to specify that the uploaded object should be stored
on disk in encrypted form with "**S3 managed keys**".

.. uml::

  skinparam SequenceMessageAlign center
  skinparam sequenceArrowThickness 2

  title __**"Server Side Encryption"**__
  participant "S3 client" as s3c
  participant "LB" as lb
  box "__S3 process__" #WhiteSmoke
  participant "S3Server" as s3server
  participant "clovis" as clovis
  end box

  participant "KMS" as kms

  box "__SSU__" #WhiteSmoke
  participant "Mero-ios" as ios
  participant "Drives" as drive
  participant "Mero-KVS" as dix
  end box

  == Upload object with SSE (encryption in S3) start ==
  s3c->lb:PUT Object
  lb->s3server:PUT Object
  |||

  s3server->clovis:get_kv(bucket)
  clovis->dix:get_kv(bucket)
  clovis<--dix: Response(success)
  s3server<--clovis: Response(success)
  |||

  s3server->clovis:get_kv(object)
  clovis->dix:get_kv(object)
  clovis<--dix: Response(success)
  s3server<--clovis: Response(success)
  |||

  s3server->kms:get_key(key_id)
  note over kms: key_id can be "bucket/objectname" or object_fid
  s3server<--kms:Response(success, enc_key)
  |||

  s3server -> s3server: encrypt(unit_size, enc_key)
  |||

  s3server->clovis:write(encrypted_data_chunk)
  clovis->ios:write(encrypted_data_chunk)
  s3server->clovis:write(encrypted_data_chunk)
  clovis->ios:write(encrypted_data_chunk)
  ios->drive:write(encrypted_data_chunk)
  ios->drive:write(encrypted_data_chunk)
  ios->clovis:write_success
  clovis->s3server:write_success
  ios->clovis:write_success
  clovis->s3server:write_success
  |||

  s3server->clovis:put_kv(object_metadata)
  clovis->dix:put_kv(object_metadata)
  dix<--clovis:put_kv_response(success)
  s3server<--clovis:put_kv_response(success)
  |||

  s3server->lb:PUT Object (success)
  lb->s3c:PUT Object (success)

  == Upload object with SSE (encryption in S3) end ==
  ...
  == Upload object with SSE (encryption in mero) start ==
  s3c->lb:PUT Object
  lb->s3server:PUT Object
  |||

  s3server->clovis:get_kv(bucket)
  clovis->dix:get_kv(bucket)
  clovis<--dix: Response(success)
  s3server<--clovis: Response(success)
  |||

  s3server->clovis:get_kv(object)
  clovis->dix:get_kv(object)
  clovis<--dix: Response(success)
  s3server<--clovis: Response(success)
  |||

  s3server->kms:get_key(key_id)
  note over kms: key_id can be object_fid
  s3server<--kms:Response(success, enc_key)
  |||

  s3server->ios:write(data_chunk, enc_key)
  ios->ios :encrypt(unit_size, enc_key)
  ios->drive :write(encrypted_data_chunk)
  s3server->ios:write(data_chunk)
  ios->ios :encrypt(unit_size, enc_key)
  ios->drive :write(encrypted_data_chunk)
  ios->s3server:write_success
  ios->s3server:write_success
  |||

  s3server->clovis:put_kv(object_metadata)
  clovis->dix:put_kv(object_metadata)

  clovis<-dix:put_kv_response(success)
  s3server<-clovis:put_kv_response(success)
  |||

  s3server->lb:PUT Object (success)
  lb->s3c:PUT Object (success)
  == Upload object with SSE (encryption in mero) end ==

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.s3.sse.user.managedkeys:

s3.sse.user.managedkeys
-----------------------

S3 client application should be able to enable server side encryption for each object.
S3 PUT api provides a mechanism to specify that the uploaded object should be stored
on disk in encrypted form using "**keys provided by user**".

.. uml::

  skinparam SequenceMessageAlign center
  skinparam sequenceArrowThickness 2

  title __**"Server Side Encryption"**__
  participant "S3 client" as s3c
  participant "LB" as lb
  box "__S3 process__" #WhiteSmoke
  participant "S3Server" as s3server
  participant "clovis" as clovis
  end box

  box "__SSU__" #WhiteSmoke
  participant "Mero-ios" as ios
  participant "Drives" as drive
  participant "Mero-KVS" as dix
  end box


  == Upload object with SSE (encryption in S3) start ==
  s3c->lb:PUT Object (x-amz-server-side​-encryption​-customer-key)
  lb->s3server:PUT Object (x-amz-server-side​-encryption​-customer-key)
  |||

  s3server->clovis:get_kv(bucket)
  clovis->dix:get_kv(bucket)
  clovis<--dix: Response(success)
  s3server<--clovis: Response(success)
  |||

  s3server->clovis:get_kv(object)
  clovis->dix:get_kv(object)
  clovis<--dix: Response(success)
  s3server<--clovis: Response(success)
  |||

  s3server -> s3server: encrypt(unit_size, enc_key)
  note over s3server
      enc_key = user provided key from
      (x-amz-server-side​-encryption​-customer-key)
  end note
  |||

  s3server->clovis:write(encrypted_data_chunk)
  clovis->ios:write(encrypted_data_chunk)
  s3server->clovis:write(encrypted_data_chunk)
  clovis->ios:write(encrypted_data_chunk)
  ios->drive:write(encrypted_data_chunk)
  ios->drive:write(encrypted_data_chunk)
  ios->clovis:write_success
  clovis->s3server:write_success
  ios->clovis:write_success
  clovis->s3server:write_success
  |||

  s3server->clovis:put_kv(object_metadata)
  clovis->dix:put_kv(object_metadata)
  dix<--clovis:put_kv_response(success)
  s3server<--clovis:put_kv_response(success)
  |||

  s3server->lb:PUT Object (success)
  lb->s3c:PUT Object (success)

  == Upload object with SSE (encryption in S3) end ==
  ...
  == Upload object with SSE (encryption in mero) start ==
  s3c->lb:PUT Object
  lb->s3server:PUT Object
  |||

  s3server->clovis:get_kv(bucket)
  clovis->dix:get_kv(bucket)
  clovis<--dix: Response(success)
  s3server<--clovis: Response(success)
  |||

  s3server->clovis:get_kv(object)
  clovis->dix:get_kv(object)
  clovis<--dix: Response(success)
  s3server<--clovis: Response(success)
  |||

  note over s3server: enc_key = (x-amz-server-side​-encryption​-customer-key)
  s3server->ios:write(data_chunk, enc_key)
  note over ios
      enc_key = user provided key from
      (x-amz-server-side​-encryption​-customer-key)
  end note
  ios->ios :encrypt(unit_size, enc_key)

  ios->drive :write(encrypted_data_chunk)
  s3server->ios:write(data_chunk, enc_key)
  ios->ios :encrypt(unit_size, enc_key)
  ios->drive :write(encrypted_data_chunk)
  ios->s3server:write_success
  ios->s3server:write_success
  |||

  s3server->clovis:put_kv(object_metadata)
  clovis->dix:put_kv(object_metadata)

  clovis<-dix:put_kv_response(success)
  s3server<-clovis:put_kv_response(success)
  |||

  s3server->lb:PUT Object (success)
  lb->s3c:PUT Object (success)
  == Upload object with SSE (encryption in mero) end ==

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.dix.kv_usage:

dix.kv_usage
------------

 Key-value pairs can be inserted into mero KVS using the clovis
 interface, which uses dix client to distribute this meta-data across
 catalogue (CAS) services.

.. uml::

 participant clovis
 participant cas1
 participant cas2
 participant cas3
 participant cas4
 participant halond

 clovis -> cas1 : put_kv()
 clovis -> cas2 : put_kv()
 clovis -> cas3 : put_kv()
 clovis <- cas1 : success
 clovis <- cas2 : success
 clovis <- cas3 : success
 note over clovis : return success to user
 clovis -> cas1 : get_kv() from first online CAS
 clovis <- cas1 : success
 note over clovis :return KV to user
 clovis -> cas1 : del_kv()
 clovis -> cas2 : del_kv()
 clovis -> cas3 : del_kv()
 clovis <- cas1 : success
 clovis <- cas2 : success
 clovis <- cas3 : success
 note over clovis : return success to user

.. _usecase.dix.delete:

dix.delete
----------

In case of transient or permanent failures of one of the CAS services
we support get(), put() and del() cases and requests from them are
going onto online CASes.

.. uml::

   participant clovis
   participant cas1
   participant cas2
   participant cas3
   participant cas4
   participant halond


   note over cas2: FAILED|OFFLINE
   clovis -> cas1 : del_kv(id1)
   clovis -> cas3 : del_kv(id1)
   clovis <- cas1 : success
   clovis <- cas3 : success

   note over clovis: return success to user

   ... cas2 is restarted ...

   halond -> cas2: Rebalancing
   note over cas2: REBALANCE

   halond -> cas1: direct_rebalance()
   halond -> cas2: direct_rebalance()
   halond -> cas3: direct_rebalance()
   halond -> cas4: direct_rebalance()
   clovis -> cas1: del_kv(id1)
   clovis -> cas2: del_kv(id1)
   clovis -> cas4: del_kv(id1)
   clovis <- cas1: success
   clovis <- cas2: success/failure
   clovis <- cas4: success
   halond <- cas1: direct_rebalance_comp()
   halond <- cas2: direct_rebalance_comp()
   halond <- cas3: direct_rebalance_comp()
   halond <- cas4: direct_rebalance_comp()
   halond -> cas2: Online

   note over cas2: ONLINE

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.dix.direct_rebalance:

dix.direct_rebalance
--------------------

This usecase assumes that CAS2 is permanetly failed and needs full
data restoration. During this process, dix.get(), put(), del()
operation continue to be handled on DIX and CAS side, untill more that
K CAS failures happen. In case of transient CAS failure DTM0 (which
has to be defined) has to orchestrate the process of redo-log replay
by means of creating distributed transaction.

.. uml ::

  participant clovis
  participant cas1
  participant cas2
  participant cas3
  participant cas4
  participant halond

  note over cas2 : FAILED
  clovis -> cas1 : put_kv(id1)
  clovis -> cas3 : put_kv(id1)
  clovis <- cas1 : success
  clovis <- cas3 : success
  note over clovis : return success to user
  ... cas2 is restarted ...
  halond -> cas2 : Rebalancing
  note over cas2 : REBALANCE \n (Start without any meta-data)
  halond -> cas1 : direct_rebalance()
  halond -> cas2 : direct_rebalance()
  halond -> cas3 : direct_rebalance()
  halond -> cas4 : direct_rebalance()
  clovis -> cas1 : put_kv(id2)
  clovis -> cas2 : put_kv(id2)
  clovis -> cas4 : put_kv(id2)
  clovis <- cas1 : success
  clovis <- cas2 : success
  clovis <- cas4 : success
  halond <- cas1 : direct_rebalance_comp()
  halond <- cas2 : direct_rebalance_comp()
  halond <- cas3 : direct_rebalance_comp()
  halond <- cas4 : direct_rebalance_comp()
  halond -> cas2 : Online
  note over cas2 : ONLINE

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.dix.mdraid:

dix.mdraid
----------

.. uml::

   participant Admin
   participant RC
   participant SSPL
   participant Provisioner
   participant CAS_1@node_1
   participant CAS_2@node_2
   participant CAS_N@node_N

   note over Admin, "CAS_N@node_N"
   Assumptions:
   - hardware configuration
    - node
      - 82 x data drives
      - 2  x metadata drives in mirror RAID1
   end note

   ref over "CAS_1@node_1" : I/O error for metadata drive
   CAS_1@node_1 -> CAS_1@node_1: CAS service crashes
   RC -> RC: detects keepalive timeout for CAS1

   RC -> SSPL: SMART check for two drives in the RAID1
   SSPL -> RC: SMART check failed for both drives

   RC -> Admin: notify that two metadata disks failed and have to be replaced
   ref over RC : wait while two metadata disks are replaced

   Admin -> RC: notify that two metadata disks for CAS_1 are replaced

   RC -> Provisioner: reassamble RAID1, make partitions, enable swap, mkfs.ext4, mount /var/mero
   Provisioner -> RC: done

   RC -> CAS_1@node_1: start m0mkfs
   CAS_1@node_1 -> RC: m0mkfs done

   RC -> CAS_1@node_1: start m0d
   CAS_1@node_1 -> RC: m0d started

   RC -> CAS_1@node_1: m0_spiel_dix_direct_rebalance_start()
   RC -> CAS_2@node_2: m0_spiel_dix_direct_rebalance_start()
   RC -> CAS_N@node_N: m0_spiel_dix_direct_rebalance_start()

   RC -> CAS_1@node_1: m0_spiel_dix_direct_rebalance_status()
   RC -> CAS_2@node_2: m0_spiel_dix_direct_rebalance_status()
   RC -> CAS_N@node_N: m0_spiel_dix_direct_rebalance_status()

   CAS_1@node_1 -> RC: DIX direct rebalance DONE
   CAS_2@node_2 -> RC: DIX direct rebalance DONE
   CAS_N@node_N -> RC: DIX direct rebalance DONE

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balance.capacity:

scaleout.balance.capacity
-------------------------

On addition of some new storage space, Halon, based on certain policy, triggers storage balance
operation in the pools. The idea is to balance the capacity across overall cluster.

Following sequnce diagram depicts a typical success scenario,

.. uml::

  participant ha as H
  participant rm as R
  box #LightBlue
      participant dix_1 as D1
      participant ios_1 as I1
  end box
  box #LightBlue
      participant dix_2 as D2
      participant ios_2 as I2
  end box
  box #LightBlue
      participant dix_3 as D3
      participant ios_3 as I3
  end box
  box #LightBlue
      participant dix_4 as D4
      participant ios_4 as I4
  end box
  note over H
      P = 4, K = 2
      for DIX
  end note
  == Repeatation ==
  H  -> I2: Elect a co-ordinator
  I2 -> H: Send ACK
  ==  ios: Iterate over gfids  ==
  I2 -> D2: Request next gob fid
  D2 -> I2: Send the gob fid if present.
  I2 -> R : Request lock for gob fid.
  R -> I2 : Grant the lock if available.
  I2 -> I2: Prepare layout
  I2 -> I1: Request obj data, inhibit older cob version.
  I2 -> I3: Request obj data, inhibit older cob version.
  I2 -> I2: Fetch obj data , inhibit older cob version.
  I2 -> I1: Write newer cob version and data.
  I2 -> I3: Write newer cob version and data.
  I2 -> I4: Write newer cob version and data.
  I2 -> D1: Update meta-data
  I2 -> D3: Update meta-data
  I2 -> D4: Update meta-data
  I2 -> I2: Delete older cob version.
  I2 -> I3: Delete older cob version.
  I2 -> I1: Delete older cob version.
  I2 -> R: Relinquish the lock.
  == Convey completion to HA ==
  I2 -> H: Completed local gfid collection.
  == HA: Stop if P - K successinve co-ordinates are done or balancing condition met ===

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balcap.read.drive.fail:

scaleout.balcap.read.drive.fail
-------------------------------

This case illustrates how the drive failure is handled by the balance capacity
service during a read.

.. uml::

   participant ha as H
   participant rm as R
   box #LightBlue
       participant dix_1 as D1
       participant ios_1 as I1
   end box
   box #LightBlue
       participant dix_2 as D2
       participant ios_2 as I2
   end box
   box #LightBlue
       participant dix_3 as D3
       participant ios_3 as I3
   end box
   box #LightBlue
       participant dix_4 as D4
       participant ios_4 as I4
   end box
   box #LightBlue
       participant dix_5 as D5
       participant ios_5 as I5
   end box
   note over H
       P = 4, K = 2
        for DIX
   end note
   == Repeatation ==
   H  -> I2: Elect a co-ordinator
   I2 -> H: Send ACK
   ==  ios: Iterate over gfids  ==
   I2 -> D2: Request next gob fid
   D2 -> I2: Send the gob fid if present.
   I2 -> R : Request lock for gob fid.
   R -> I2 : Grant the lock if available.
   I2 -> I2: Prepare layout
   I2 -> I1: Request obj data, inhibit older cob version.
   I1 -> I2: Data send
   I2 -> I3: Request obj data, inhibit older cob version.
   activate I3 #Red
   I3 -> I3: Read data failure
   I3 -> I2: Read failure
   deactivate I3 #Red
   I3 -> H : Notify failure to halon
   I2 -> I2: Re-construct missing I3 data
   I2 -> I2: Fetch obj data , inhibit older cob version.
   I2 -> I1: Write newer cob version and data.
   I1 -> I2: Write success
   I2 -> I3: Write newer cob version and data.
   I3 -> I2: Write success
   I2 -> I4: Write newer cob version and data.
   I4 -> I2: Write success
   I2 -> D1: Update meta-data
   D1 -> I2: Update success
   I2 -> D5: Update meta-data
   D5 -> I2: update success
   I2 -> D4: Update meta-data
   D4 -> I2: Update success
   I2 -> I2: Delete older cob version.
   I2 -> I1: Delete older cob version.
   I2 -> R: Relinquish the lock.
   == Convey completion to HA ==
   I2 -> H: Completed local gfid collection.
   == HA: Stop if P - K successinve co-ordinates are done or balancing condition met ===

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balcap.read.ios.fail:

scaleout.balcap.read.ios.fail
-----------------------------

This case illustrates how a node failure is handled during a read io in balance capacity
operation.

.. uml::

   participant ha as H
   participant rm as R
   box #LightBlue
       participant dix_1 as D1
       participant ios_1 as I1
   end box
   box #LightBlue
       participant dix_2 as D2
       participant ios_2 as I2
   end box
   box #LightBlue
       participant dix_3 as D3
       participant ios_3 as I3
   end box
   box #LightBlue
       participant dix_4 as D4
       participant ios_4 as I4
   end box
   box #LightBlue
       participant dix_5 as D5
       participant ios_5 as I5
   end box
   note over H
       P = 4, K = 2
        for DIX
   end note
   == Repeatation ==
   H  -> I2: Elect a co-ordinator
   I2 -> H: Send ACK
   ==  ios: Iterate over gfids  ==
   I2 -> D2: Request next gob fid
   D2 -> I2: Send the gob fid if present.
   I2 -> R : Request lock for gob fid.
   R -> I2 : Grant the lock if available.
   I2 -> I2: Prepare layout
   I2 -> I1: Request obj data, inhibit older cob version.
   I1 -> I2: Data send
   I2 -> I3: Request obj data, inhibit older cob version.
   destroy I3
   I2 -> I2: I3 request timeout
   I2 -> I2: Re-construct missing I3 data
   I2 -> I2: Fetch obj data , inhibit older cob version.
   I2 -> I1: Write newer cob version and data.
   I1 -> I2: Write success
   I2 -> I5: Write newer cob version and data to spare.
   I5 -> I2: Write success
   I2 -> I4: Write newer cob version and data.
   I4 -> I2: Write success
   I2 -> D1: Update meta-data
   D1 -> I2: Update success
   I2 -> D5: Update meta-data
   D5 -> I2: update success
   I2 -> D4: Update meta-data
   D4 -> I2: Update success
   I2 -> I2: Delete older cob version.
   I2 -> I1: Delete older cob version.
   I2 -> R: Relinquish the lock.
   == Convey completion to HA ==
   I2 -> H: Completed local gfid collection.
   == HA: Stop if P - K successinve co-ordinates are done or balancing condition met ===

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balcap.write.drive.fail:

scaleout.balcap.write.drive.fail
--------------------------------

This case illustrates how a drive failure is handled during a write io in the balance
capacity operation.

.. uml::

   participant ha as H
   participant rm as R
   box #LightBlue
       participant dix_1 as D1
       participant ios_1 as I1
   end box
   box #LightBlue
       participant dix_2 as D2
       participant ios_2 as I2
   end box
   box #LightBlue
       participant dix_3 as D3
       participant ios_3 as I3
   end box
   box #LightBlue
       participant dix_4 as D4
       participant ios_4 as I4
   end box
   box #LightBlue
       participant dix_5 as D5
       participant ios_5 as I5
   end box
   note over H
       P = 4, K = 2
        for DIX
   end note
   == Repeatation ==
   H  -> I2: Elect a co-ordinator
   I2 -> H: Send ACK
   ==  ios: Iterate over gfids  ==
   I2 -> D2: Request next gob fid
   D2 -> I2: Send the gob fid if present.
   I2 -> R : Request lock for gob fid.
   R -> I2 : Grant the lock if available.
   I2 -> I2: Prepare layout
   I2 -> I1: Request obj data, inhibit older cob version.
   I1 -> I2: Data send
   I2 -> I3: Request obj data, inhibit older cob version.
   I3 -> I2: Data send
   I2 -> I2: Fetch obj data , inhibit older cob version.
   I2 -> I1: Write newer cob version and data.
   I1 -> I2: Write success
   activate I3 #Red
   I2 -> I3: Write newer cob version and data.
   I3 -> I2: Write failure
   deactivate I3
   I3 -> H:  Notify halon of drive failure
   I2 -> I5: Write newer cob version and data to spare.
   I5 -> I2: Write success
   I2 -> I4: Write newer cob version and data.
   I4 -> I2: Write success
   I2 -> D1: Update meta-data
   D1 -> I2: Update success
   I2 -> D3: Update meta-data
   D3 -> I2: update success
   I2 -> D4: Update meta-data
   D4 -> I2: Update success
   I2 -> I2: Delete older cob version.
   I2 -> I3: Delete older cob version.
   I2 -> I1: Delete older cob version.
   I2 -> R: Relinquish the lock.
   == Convey completion to HA ==
   I2 -> H: Completed local gfid collection.
   == HA: Stop if P - K successinve co-ordinates are done or balancing condition met ===

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balcap.write.ios.fail:

scaleout.balcap.write.ios.fail
------------------------------

This case illustrates how a node failure is handled during a write io in balance capacity
operation.

.. uml::

   participant ha as H
   participant rm as R
   box #LightBlue
       participant dix_1 as D1
       participant ios_1 as I1
   end box
   box #LightBlue
       participant dix_2 as D2
       participant ios_2 as I2
   end box
   box #LightBlue
       participant dix_3 as D3
       participant ios_3 as I3
   end box
   box #LightBlue
       participant dix_4 as D4
       participant ios_4 as I4
   end box
   box #LightBlue
       participant dix_5 as D5
       participant ios_5 as I5
   end box
   note over H
       P = 4, K = 2
        for DIX
   end note
   == Repeatation ==
   H  -> I2: Elect a co-ordinator
   I2 -> H: Send ACK
   ==  ios: Iterate over gfids  ==
   I2 -> D2: Request next gob fid
   D2 -> I2: Send the gob fid if present.
   I2 -> R : Request lock for gob fid.
   R -> I2 : Grant the lock if available.
   I2 -> I2: Prepare layout
   I2 -> I1: Request obj data, inhibit older cob version.
   I1 -> I2: Data send
   I2 -> I3: Request obj data, inhibit older cob version.
   I3 -> I2: Data send
   I2 -> I2: Fetch obj data , inhibit older cob version.
   I2 -> I1: Write newer cob version and data.
   I1 -> I2: Write success
   destroy I3
   I2 -> I3: Write newer cob version and data.
   I2 -> I2: Write timeout
   H -> I1: Notify I3 failure
   H -> I2: Notify I3 failure
   H -> I4: Notify I3 failure
   H -> I5: Notify I3 failure
   I2 -> I5: Write newer cob version and data to spare.
   I5 -> I2: Write success
   I2 -> I4: Write newer cob version and data.
   I4 -> I2: Write success
   I2 -> D1: Update meta-data
   D1 -> I2: Update success
   I2 -> D5: Update meta-data
   D5 -> I2: update success
   I2 -> D4: Update meta-data
   D4 -> I2: Update success
   I2 -> I2: Delete older cob version.
   I2 -> I1: Delete older cob version.
   I2 -> R: Relinquish the lock.
   == Convey completion to HA ==
   I2 -> H: Completed local gfid collection.
   == HA: Stop if P - K successinve co-ordinates are done or balancing condition met ===

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balcap.write.drive.restore:

scaleout.balcap.write.drive.restore
-----------------------------------

This case illustrates how the data is restored to its original location after a failed
drive is restored.

.. uml::

   participant ha as H
   participant rm as R
   box #LightBlue
       participant dix_1 as D1
       participant ios_1 as I1
   end box
   box #LightBlue
       participant dix_2 as D2
       participant ios_2 as I2
   end box
   box #LightBlue
       participant dix_3 as D3
       participant ios_3 as I3
   end box
   box #LightBlue
       participant dix_4 as D4
       participant ios_4 as I4
   end box
   box #LightBlue
       participant dix_5 as D5
       participant ios_5 as I5
   end box
   note over H
       P = 4, K = 2
        for DIX
   end note
   == Restoring from I3 drive failure. Data was written on spare at I5. ==
   == meta data was updated successfully on respective dix nodes. ==
   I2 -> D1: Update meta-data
   D1 -> I2: Update success
   I2 -> D3: Update meta-data
   D3 -> I2: update success
   I2 -> D4: Update meta-data
   D4 -> I2: Update success
   I2 -> I2: Delete older cob version.
   I2 -> I3: Delete older cob version.
   I2 -> I1: Delete older cob version.
   I2 -> R: Relinquish the lock.
   == Balancing completed ==
   H -> I1: Notify drive online
   H -> I2: Notify drive online
   H -> I3: Notify drive online
   H -> I4: Notify drive online
   H -> I5: Notify drive online
   H -> I1: Start drive rebalance
   H -> I2: Start drive rebalance
   H -> I3: Start drive rebalance
   H -> I4: Start drive rebalance
   H -> I5: Start drive rebalance
   == all the sns rebalance services participate in drive rebalance ==
   I5 -> D5: fetch updated layout
   I5 -> I3: move data to appropriate location as per balance
   == rebalance complete ==
   I1 -> H: notify rebalance complete
   I2 -> H: notify rebalance complete
   I3 -> H: notify rebalance complete
   I4 -> H: notify rebalance complete
   I5 -> H: notify rebalance complete

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balcap.write.ios.restore:

scaleout.balcap.write.ios.restore
---------------------------------

This balance capacity case describes how a data is written to its original
location when a failed node is active again.

.. uml::

   participant ha as H
   participant rm as R
   box #LightBlue
       participant dix_1 as D1
       participant ios_1 as I1
   end box
   box #LightBlue
       participant dix_2 as D2
       participant ios_2 as I2
   end box
   box #LightBlue
       participant dix_3 as D3
       participant ios_3 as I3
   end box
   box #LightBlue
       participant dix_4 as D4
       participant ios_4 as I4
   end box
   box #LightBlue
       participant dix_5 as D5
       participant dtm_5 as DTM5
       participant ios_5 as I5
   end box
   note over H
       P = 4, K = 2
        for DIX
   end note
   == Restoring from I3 failure. Data and meta-data was written on spare at I5. ==
   I2 -> D1: Update meta-data
   D1 -> I2: Update success
   I2 -> D5: Update meta-data
   D5 -> I2: update success
   I2 -> D4: Update meta-data
   D4 -> I2: Update success
   I2 -> I2: Delete older cob version.
   I2 -> I1: Delete older cob version.
   I2 -> R: Relinquish the lock.
   == Balancing completed using spare ==
   H -> I1: Notify ios online
   H -> I2: Notify ios online
   H -> I4: Notify ios online
   H -> I5: Notify ios online
   DTM5 -> I2: redo operation
   I2 -> I3: Write newer cob version and data
   I3 -> I2: Write success
   == DTM0 redo complete ==

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balance.capacity.coordinator.fail:

scaleout.balance.capacity.coordinator.fail
------------------------------------------

This case describes the handling of coordinator failure.

.. uml::

   participant ha as H
   participant rm as R
   box #LightBlue
       participant dix_1 as D1
       participant ios_1 as I1
   end box
   box #LightBlue
       participant dix_2 as D2
       participant ios_2 as I2
   end box
   box #LightBlue
       participant dix_3 as D3
       participant ios_3 as I3
   end box
   box #LightBlue
       participant dix_4 as D4
       participant ios_4 as I4
   end box
   box #LightBlue
       participant dix_5 as D5
       participant ios_5 as I5
   end box
   note over H
       P = 4, K = 2
        for DIX
   end note
   == Repeatation ==
   H  -> I2: Elect a co-ordinator
   I2 -> H: Send ACK
   ==  ios: Iterate over gfids  ==
   I2 -> D2: Request next gob fid
   D2 -> I2: Send the gob fid if present.
   I2 -> R : Request lock for gob fid.
   R -> I2 : Grant the lock if available.
   I2 -> I2: Prepare layout
   I2 -> I1: Request obj data, inhibit older cob version.
   I1 -> I2: Data send
   I2 -> I3: Request obj data, inhibit older cob version.
   I3 -> I2: Data send
   I2 -> I2: Fetch obj data , inhibit older cob version.
   I2 -> I1: Write newer cob version and data.
   I1 -> I2: Write success
   destroy I2
   H -> I1: coordonator failure
   H -> I3: coordonator failure
   H -> I4: coordonator failure
   H -> I5: coordonator failure
   H -> I1: Elect new coordinator
   == I1 continues the balance operation ==
   I1 -> I3: Write newer cob version and data.
   I3 -> I1: Write success
   I1 -> I4: Write newer cob version and data.
   I4 -> I1: Write success
   I1 -> I5: Write newer cob version and data.
   I5 -> I1: Write success
   I1 -> R: Relinquish the lock.
   == Convey completion to HA ==

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.balance.capacity.mdupdate.fail:

scaleout.balance.capacity.mdupdate.fail
---------------------------------------

This case describes the case where meta-data update on one of the
io services failed due to transient failure of the coordinator.

.. uml::

   participant client as C
   participant ha as H
   participant rm as R
   participant dtm0
   box #LightBlue
       participant dix_1 as D1
       participant ios_1 as I1
   end box
   box #LightBlue
       participant dix_2 as D2
       participant ios_2 as I2
   end box
   box #LightBlue
       participant dix_3 as D3
       participant ios_3 as I3
   end box
   box #LightBlue
       participant dix_4 as D4
       participant ios_4 as I4
   end box
   box #LightBlue
       participant dix_5 as D5
       participant ios_5 as I5
   end box
   note over H
       P = 4, K = 2
        for DIX
   end note
   == Repeatation ==
   H  -> I2: Elect a co-ordinator
   I2 -> H: Send ACK
   ==  ios: Iterate over gfids  ==
   I2 -> D2: Request next gob fid
   D2 -> I2: Send the gob fid if present.
   I2 -> R : Request a group lock for gob fid.
   R -> I2 : Grant the lock if available.
   I2 -> I2: Prepare layout
   I2 -> I1: Request obj data, inhibit older cob version.
   I1 -> I2: Data send
   I2 -> I3: Request obj data, inhibit older cob version.
   I3 -> I2: Data send
   I2 -> I2: Fetch obj data, inhibit older cob version.
   I2 -> I1: Write newer cob version and data.
   I1 -> I2: Write success
   I2 -> I3: Write newer cob version and data.
   I3 -> I2: Write success
   I2 -> I4: Write newer cob version ad data.
   I4 -> I2: Write success
   I2 -> I1: update meta-data
   I2 -> I4: update meta-data
   == I2 fails transiently ==
   destroy I2
   H -> I1 : notify transient failure of I2
   H -> I3 : notify transient failure of I2
   H -> I4 : notify trnasient failure of I2
   H -> dtm0 : notify transient failure of I2
   == meta-data update to I3 couldn't happen ==
   == I2, I1, I4 are holding distributed group file locks ==
   == I2 comes back up ==
   C -> R : try to acquire file lock for read, can possibly read old meta-data on I2
   C -> C : Wait for lock
   dtm0 -> I2 : redo operation
   I2 -> I3 : update meta-data
   I2 -> R: Relinquish the lock.
   I1 -> R: Relinquish the lock.
   I3 -> R: Relinquish the lock.
   I4 -> R: Relinquish the lock.
   == Convey completion to HA ==
   R -> C : file lock granted
   C -> I2 : Read new meta-data and data

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.scaleout.poolmod:

scaleout.poolmod
----------------

.. uml::

    title Pool addition

    participant admin
    participant halonctl as hctl
    participant "Halon RC" as RC
    collections IOS as ios
    collections confd
    participant "Principal RM" as rm

    note over RC
        @pre HWmod (hardware addition) happened earlier
        @pre svcmod/procmod ?
    end note
    admin -> hctl: hctl mero pool add \\\n-f pool.yaml
    hctl -> hctl: parse pool.yaml\n("mini-facts")

    hctl -> RC: PoolAdd
    RC -> RC: add Pool\nto the RG

    group <font color=red>XXX FIXME: SNS should auto-quiesce\n<font color=red>when conf R lock is revoked
    RC -> ios: m0_spiel_repreb_quiesce()
    RC <-- ios: ok
    end group

    RC -> RC: alloc spiel_tx;\nm0_spiel_X_add;\nvalidate confc;\nhash(tx_to_str())

    == rconfc, phase 1 ==

    RC -> confd: conf_load_fop
    confd -> confd: store xcoded conf\nin RAM
    RC <-- confd: ok
    RC -> rm: borrow conf W\n(request exclusive lock)
    rm -> ios: revoke conf R
    ios -> ios: .rconfc_expire_cb\n(drains confc)
    ios -> rm: return conf R
    ios -> ios: restart rconfc
    ios -> rm: borrow conf R\n(request)

    rm -> confd: revoke conf R
    rm <- confd: return conf R
    rm <- confd: borrow conf R

    RC <- rm: grant (sub-let)\nconf W lock

    == rconfc, phase 2 ==

    RC -> confd: conf_flip
    confd -> confd: write new conf.xc;\nrestart rconfc
    RC <-- confd: flipped
    RC -> RM: give up (cancel)\nconf W lock

:Entities:
:Owners: mandar, vvv
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.be.paged.read:

scaleout.balance.io
-------------------

On addition of some new storage devices, Mero client creates new objects on less occupied pools.
Mero define pool selection policy which return less occupied pool (with respect to storage/
io_requests) for new objects. The idea is to balance the io request across overall cluster.

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

be.paged.read
-------------

.. uml::

   participant ReqH
   control User_FOM
   participant PageD
   control PageD_FOM

   ReqH -> User_FOM : tick()
   User_FOM -> PageD : reg_get(reg, op)
   PageD --> User_FOM : // page is in
   PageD -> PageD : pages[] = reg_to_pages(reg)
   PageD -> PageD : PD.address_space.page_is_in(pages)?
   PageD -> PageD : PD.request_queue.push(request(READ, pages[], op)
   PageD -> PageD : m0_fom_wait_on(fom, op)
   PageD -> PageD : M0_CO_YIELD()
   PageD --> User_FOM : return control
   User_FOM --> ReqH : return to ReqH, process next FOM

   note over ReqH, PageD_FOM
   Wait until PageD_FOM to wakeup and process pushed request
   end note

   ref over PageD_FOM : IDLE
   PageD_FOM -> PageD_FOM : request = PD.request_queue.pop()
   ref over PageD_FOM : READ
   PageD_FOM -> PageD_FOM : mmap(request.pages[i].addr, request.pages[i].size)
   PageD_FOM -> PageD_FOM : mlock(request.pages[i].addr, request.pages[i].size)
   PageD_FOM -> PageD_FOM : stob_io_fill(io, request.pages[i].offset, request.pages[i].size)
   PageD_FOM -> PageD_FOM : m0_fom_wait_on(fom, io->chan)
   PageD_FOM -> PageD_FOM : stob_io_launch(io)
   ref over PageD_FOM : READ_DONE
   PageD_FOM -> PageD_FOM : m0_be_op_done(request.op)
   ref over PageD_FOM : IDLE

   note over ReqH, PageD_FOM
   PageD_FOM processed given request, wait untill notification to arrive inside corresponding User_FOM
   end note

   ReqH -> ReqH : wakeup FOM waiting on request.op
   ReqH -> User_FOM : tick()
   User_FOM -> User_FOM : M0_CO_REENTER()
   User_FOM -> User_FOM : restore control over line after M0_CO_YIELD()

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.be.paged.write:

be.paged.write
--------------

.. uml::

   participant ReqH
   control TxGrp_FOM
   participant PageD
   control PageD_FOM

   ReqH -> TxGrp_FOM : tx_group_fom_tick()
   ref over TxGrp_FOM : TGS_PLACING
   TxGrp_FOM -> TxGrp_FOM : copy gr.tg_reg_area.regs data into corresponding cellar pages
   TxGrp_FOM -> TxGrp_FOM : pages[] = dirty_cellar_pages()
   TxGrp_FOM -> TxGrp_FOM : PD.request_queue.push(request(WRITE, pages[], op))
   TxGrp_FOM -> TxGrp_FOM : m0_be_op_tick_ret(op, fom, TGS_PLACED)

   note over ReqH, PageD_FOM
   Wait until PageD_FOM to wakeup and process pushed request
   end note

   ref over PageD_FOM : IDLE
   PageD_FOM -> PageD_FOM : request = PD.request_queue.pop()
   ref over PageD_FOM : WRITE
   PageD_FOM -> PageD_FOM : stob_io_fill(io, request.pages[i].offset, request.pages[i].size)
   PageD_FOM -> PageD_FOM : m0_fom_wait_on(fom, io->chan)
   PageD_FOM -> PageD_FOM : stob_io_launch(io)
   ref over PageD_FOM : WRITE_DONE
   PageD_FOM -> PageD_FOM : m0_be_op_done(request.op)
   ref over PageD_FOM : IDLE

   note over ReqH, PageD_FOM
   PageD_FOM processed given request, wait until notification to arrive inside corresponding TxGrp_FOM
   end note

   ReqH -> ReqH : wakeup FOM waiting on request.op in the next (TGS_PLACED) state
   ref over TxGrp_FOM : TGS_PLACED
   ReqH -> TxGrp_FOM : tx_group_fom_tick()

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.handle.pods.vdisk:

ras.fault.handle.pods.vdisk
---------------------------

PODS exposes virtual disks over erasure coded physical disks to survive one or more disk failures. MERO will use these virtual disks, as the underlying device, for both metadata and data. When one or more disks fail, PODS automatically handles those internally, by performing repairs. However as more number of disks starts to fail, PODS reduces to the vdisk capacity to compensate initially, or may even fail the vdisk.

SSPL integrated with PODS and subscribes to the events. In case of failure
events, SSPL detects and reports the same on the RabbitMQ channel. Halon
and Dashboard retrieve this informaion from RabbitMQ. Dashboard displays
an alert to user, while Halon records it and initiates repair process.

When administrator configures new vdisk and adds it to the pool, Halon
initiates rebalance. Once rebalance completes, Halon notifies SSPL
which in turn sends notification to Dashboard to clear the alert.

.. uml::

  participant PODS
  participant Systemd
  participant SSPL
  participant RabbitMQ
  participant Dashboard
  participant Halon
  participant Mero

  SSPL -> PODS: Subscribe for Events
  Halon -> RabbitMQ: Subscribe for Events
  Dashboard -> SSPL: Subscribe for Events
  note over PODS: vDisk failed
  PODS -> SSPL: vDisk failed Event
  SSPL -> RabbitMQ: Disk Fail Message
  SSPL -> Dashboard: OS Disk Failure Message
  note over Dashboard: Display Disk Failure Alert
  RabbitMQ -> Halon: Disk Fail Message
  note over Systemd: Disk Failure Detected
  Systemd -> SSPL: Disk Failure Event
  SSPL -> RabbitMQ: OS Disk Failure Message
  RabbitMQ -> Halon: OS Disk Failure Message
  note over Halon: Mark Disk Transient
  
  Halon -> Mero: Initiate Repair
  Mero -> Halon: Data Recovered to other vdisks
  note over Halon: Mark Disk Repaired

  note over PODS: new vDisk Created
  PODS -> SSPL: vDisk Created Event
  SSPL -> RabbitMQ: New Disk Install Message
  RabbitMQ -> Halon: New Disk Install Message
  note over Halon: New Disk Noted
  Halon -> Mero: Start Rebalance
  note over Mero: Rebalance
  Mero -> Halon: Rebalance Completed
  Halon -> SSPL: New Disk Configured
  SSPL -> RabbitMQ: New Disk Configured
  SSPL -> Dashboard: New Disk Configured
  note over Dashboard: Clear Disk Failure Alert

:Entities:
:Owners: ujjwal, vvv
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.handle.pods.fru:

ras.fault.handle.pods.fru
-------------------------

PODS FRUs like Fans, Power Supplies, Sideplane SAS expanders, disk drives, etc,
failure and rectification events are received by SSPL. SSPL observes PODS FRU
state change events and publishes the same on RabbitMQ channel and also records
them onto the syslog. Dashboard publishes failure alerts to notify administrators.
Administrators, with the help of Seagate support, can initiate the FRU replacement
process. As soon as a new component is added, SSPL receives a notification from PODS
and the same will be forwarded to Halon for a relevant action if applicable.

.. uml::

  participant PODS
  participant SSPL
  participant Syslog
  participant RabbitMQ
  participant Dashboard
  participant Halon

  SSPL -> PODS: Subscribe for Events
  Dashboard -> SSPL : Subscribe for Events
  Halon -> RabbitMQ: Subscribe for Events
  note over PODS: FRU(FAN) failed
  PODS -> SSPL: FRU(FAN) failed Event
  SSPL -> RabbitMQ: FRU(FAN) Fail Event
  SSPL -> Syslog: FRU(FAN) Action Alert
  SSPL -> Dashboard: FRU(FAN) Fail Event
  note over Dashboard: Display Action Alert
  RabbitMQ -> Halon: FRU(FAN) Fail Event
  note over Halon: Record Degraded State

  note over PODS: New FRU(FAN) installed
  PODS -> SSPL: New FRU:FAN  Event
  SSPL -> RabbitMQ: New FRU(FAN)  Event
  SSPL -> Dashboard: New FRU(FAN) Event
  note over Dashboard: Clear Alert
  RabbitMQ -> Halon: New FRU(FAN) Event
  note over Halon: Clear Degraded State

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.handle.pods.raid:

ras.fault.handle.pods.raid
--------------------------

SSPL subscribes for events from PODS. SSPL publishes change in raid disk state
onto the RabbitMQ channel. Dashboard and Halon receives this alert. After new
disk installation in PODS, SSPL is informed about the same. SSPL sends this
to Dashboard and Halon via rabbitMQ. Halon reacts and sends actuator request
to add this new disk to Raid array. SSPL processes the request and sends
relevant actuator response to Halon.

.. uml::

  participant PODS
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard

  SSPL -> PODS: Subscribe for Events
  Halon -> RabbitMQ: Subscribe for Events
  Dashboard -> SSPL: Subscribe for Events. 
  note over PODS: Raid disk failed
  PODS -> SSPL: Raid disk failed Event
  SSPL -> RabbitMQ: Raid disk Fail Event
  RabbitMQ -> Halon: Raid disk Fail Event
  note over Halon: Record Raid disk Degraded State
  SSPL -> Dashboard: Raid disk Fail Event
  note over Dashboard: Display Action Alert

  note over PODS: New disk installed
  PODS -> SSPL: New disk installed Event
  SSPL -> RabbitMQ: New disk installed Event
  RabbitMQ -> Halon: New disk installed Event
  note over Halon: Clear Degraded State
  SSPL -> Dashboard: New disk installed Event
  note over Dashboard: Clear Alert
  Halon -> RabbitMQ : actuator RAID add request 
  RabbitMQ -> SSPL : actuator RAID add request
  note over SSPL : process request
  SSPL -> RabbitMQ : Send Actuator response
  RabbitMQ -> Halon : Send Actuator response
  note over Halon : noted

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.handle.os.service:

ras.fault.handle.os.service
---------------------------

If a software service state changes, SSPL broadcasts event on to the rabbitMQ channel.
Halon which listens to the event, may take relevant action when required.

.. uml::

  participant systemd
  participant SSPL
  participant RabbitMQ
  participant Halon

  Halon -> RabbitMQ: Subscribe for events
  SSPL -> systemd: Subscribe to UDisk2 events
  systemd -> SSPL: Service down
  note over SSPL: Detect Service state change
  SSPL -> RabbitMQ: Service State change event
  RabbitMQ -> Halon: Service State change event
  note over Halon: Noted. Trigger recovery
  Halon -> RabbitMQ: Actuator Service Restart request
  RabbitMQ -> SSPL : Actuator Service Restart request
  SSPL -> systemd: Restart Service
  SSPL -> systemd: Query Service State
  systemd -> SSPL: Service ON
  SSPL -> RabbitMQ: Actuator Response
  RabbitMQ -> Halon: Actuator Response
  note over Halon: Noted

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.handle.os.mdraid:

ras.fault.handle.os.mdraid
--------------------------

SSPL monitors MDRAID array for any state changes like degraded state of any OS disk (which are part of Raid array) and broadcasts related events onto the rabbitMQ channel. From Dashboard ALert, Administrators will replace the disk and will initiate repair action. Halon will request to add this new OS disk to the array and SSPL will perform add operation using mdadm utility and relevant response will be sent to Halon and Halon will record the state.Also, relevant alerts will be sent to Dashboard with the help of Halon.

.. uml::

  participant OS
  participant MDRAID
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard
  actor Admin

  Halon -> RabbitMQ: Subscribe for messages
  Dashboard -> SSPL: Subscribe for messages
  OS -> MDRAID : Disk failed
  note over MDRAID: Mark MD Device Degraded
  SSPL -> MDRAID: Query MD device state
  MDRAID -> SSPL: MD Device Degraded
  SSPL -> RabbitMQ: MD Device Degraded Message
  RabbitMQ -> Halon: Os Disk Degrade Message
  note over Halon : noted
  SSPL -> Dashboard: OS Disk Degraded
  note over Dashboard: Display Action Alert
  Dashboard -> Admin: Action Alert
  note over Admin: Replace Disk
  Admin -> Halon: Recover OS Disk
  Halon -> OS: Reboot

  note over OS: New disk detected
  OS -> SSPL: New OS Disk
  SSPL -> RabbitMQ: New OS Disk Message
  RabbitMQ -> Halon: New OS Disk Message
  Halon -> SSPL : Actuator Cmd: Add Disk to MD Device
  note over SSPL : process request
  SSPL -> MDRAID: Add Disk
  SSPL -> RabbitMQ: Actuator response
  RabbitMQ -> Halon: Actuator response
  note over MDRAID: Initiate MD Device Recovery
  SSPL -> MDRAID: Query MD Device state
  MDRAID -> SSPL: Recovery Complete
  SSPL -> RabbitMQ: MD Device Recovered
  RabbitMQ -> Halon: MD Device Recovered
  note over Halon: Record State 
  SSPL -> Dashboard : OS Device Ok
  note over Dashboard : Clear Alert

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.handle.nw.switch:

ras.fault.handle.nw.switch
--------------------------

Network switch in Rack is monitored through remote management APIs. Admin can recover
network port physically. Dashboard will publish a fault alert in case a fault occurs
in the network switch.

.. uml::

  participant NWSwitch
  participant "SSPL SNMP Plugin"
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard
  actor Admin

  "SSPL SNMP Plugin" -> NWSwitch : Subscribe for events
  Dashboard -> SSPL : Subscribe for events
  note over NWSwitch: N/W port down
  NWSwitch -> "SSPL SNMP Plugin": N/W port down Event
  "SSPL SNMP Plugin" -> SSPL: N/W port down Event
  note over SSPL: Process Event
  SSPL -> RabbitMQ: N/W port down Event
  RabbitMQ -> Halon: N/W port down Event
  note over Halon: Noted
  SSPL -> Dashboard: N/W port down Event
  note over Dashboard: Display alert
  note over Admin: Replace Switch
  note over NWSwitch: N/W port up
  NWSwitch -> "SSPL SNMP Plugin": N/W port up Event
  "SSPL SNMP Plugin" -> SSPL: N/W port up Event
  note over SSPL: Process Event
  SSPL -> RabbitMQ: N/W port up Event
  RabbitMQ -> Halon: N/W port up Event
  SSPL -> Dashboard: N/W port up Event
  note over Dashboard: Clear alert 

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.handle.pdu.voltage:

ras.fault.handle.pdu.voltage
----------------------------

SSPL subscribe for any PDU events like PDU voltage drop through PDU exposed remote management apis. If any of PDU develops fault, same is propagated to SSPL & further components in chain as depicted in sequence diagram.

.. uml::

  Actor Admin
  participant PDUs
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard

  SSPL -> PDUs: subscribe for events
  Halon -> RabbitMQ : Subscribe for events
  Dashboard -> SSPL : Subscribe for events
  note over PDUs: PDU #n Voltage Drop
  PDUs -> SSPL : PDU #n Fault detected
  note over SSPL: Process Request.\nBroadcast
  SSPL -> RabbitMQ: PDU #n Voltage\nFault detected
  RabbitMQ -> Halon:PDU #n Voltage\nFault detected
  note over Halon: Noted\n No Action
  SSPL -> Dashboard: PDU #n Voltage\nFault detected
  note over Dashboard : Display Action Alert 
  note over SSPL: Recover PDU\nusing predefined\n recovery steps
  SSPL -> RabbitMQ :PDU recovered message
  RabbitMQ -> Halon:PDU recovered message
  Halon -> Dashboard :PDU recovered message
  note over Dashboard : Clear Alert
  Admin -> PDUs: If PDU unrecoverable\nReplace faulty PDU
  PDUs -> SSPL : PDU replace message
  SSPL -> RabbitMQ : PDU replace message
  RabbitMQ -> Halon : PDU replace message
  note over Halon : noted
  SSPL -> Dashboard : PDU replace message
  note over Dashboard : Clear Alert

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.handle.os.space:

ras.fault.handle.os.space
-------------------------

SSPL detects OS resources availability like drive free space capacity  and broadcast event to subscribed rabbitmq channel to take recovery action. SSPL can accept request to recover from low disk space availability by possibly clearing predefined temporary caches.

.. uml::

  participant OS
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard
  actor Admin

  Halon ->RabbitMQ : Subscribe for events
  Dashboard -> SSPL: Subscribe for events
  SSPL -> OS: query space info
  OS -> SSPL: space info
  note over SSPL:Low space detected
  SSPL -> RabbitMQ: message indicating low space
  RabbitMQ -> Halon: message indicating low space
  note over Halon: Process message
  SSPL -> Dashboard: low space event
  note over Dashboard: Display alert
  Admin -> Dashboard: control action
  Dashboard -> Halon: control action
  note over Halon: Process message
  Halon -> RabbitMQ: request to clear tmp files
  RabbitMQ -> SSPL: request to clear tmp files
  note over SSPL: Process request
  SSPL -> OS: command to clear temp files
  note over OS: Clear tmp files

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.alert.email:

ras.fault.alert.email
---------------------

Email will be sent by SSPL on configured faults. Sender and receiver email addresses and SMTP server will be configured in SSPL.

.. uml::

  participant Enclosure
  participant CLI
  participant SSPL
  participant "SMTP Server"
  participant "Admin Email Account"

  CLI -> SSPL: configure user email and SMTP server credentials
  note over SSPL: Add email subscription
  Enclosure -> SSPL: drive failure notification
  note over SSPL: Prepare email
  SSPL -> "SMTP Server": email with drive failure notification
  note over "SMTP Server": Send email
  "SMTP Server" -> "Admin Email Account": email with drive failure notification

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.alert.dashboard:

ras.fault.alert.dashboard
-------------------------

SSPL will propagate configured faults to Dashboard through RabbitMQ channel. Dashboard in turn will process notifications and will highlight in Dashboard view as per the criticality of the notification.

.. uml::

  participant Enclosure
  participant SSPL
  participant RabbitMQ
  participant "Halon"
  participant "Dashboard"

  Halon-> RabbitMQ : Subscribe for events
  Dashboard -> SSPL : Subscribe for events
  Enclosure -> SSPL: drive failure notification
  note over SSPL: Process message
  SSPL -> RabbitMQ: drive failure notification
  RabbitMQ -> Halon: drive failure notification
  note over Halon : noted
  SSPL -> Dashboard: drive failure
  note over Dashboard: Show alert

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.stats.os.cpu:

ras.stats.os.cpu
----------------

SSPL detects system resource cpu over usage by any particular service , generate stats for same based on current,historical values and broadcast stats to subscribed rabbitmq channel to take recovery action, if need be. SSPL can accept request to recover from over cpu usage by restarting the faulty service or any other predefined recovery steps.

.. uml::

  participant OS
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard

  Halon -> RabbitMQ: Subscribe to messages
  Dashboard -> SSPL: Subscribe to messages
  SSPL -> OS: Periodically monitors CPU data 
  note over SSPL: Process Data
  SSPL -> RabbitMQ : CPU information message
  RabbitMQ -> Halon : CPU information message
  note over Halon: Noted
  SSPL -> Dashboard:  CPU usage stats
  note over Dashboard: Display CPU stats 

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.stats.os.memory:

ras.stats.os.memory
-------------------

SSPL detects system resource main memory (RAM) over usage by any particular service , generate stats for same based on current,historical values and broadcast stats to subscribed rabbitmq channel to take recovery action, if need be. SSPL can accept request to recover from high memory usage by restarting the faulty service or any other predefined recovery steps.

.. uml::

  participant OS
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard
  Halon -> RabbitMQ: Subscribe to messages
  Dashboard -> SSPL: Subscribe to messages
  SSPL -> OS: Monitor memory releted information 
  note over SSPL: Process Data
  SSPL -> RabbitMQ: Memory usage message
  RabbitMQ -> Halon: Memory usage message
  note over Halon: Noted
  SSPL -> Dashboard  : Memory usage stats
  note over Dashboard: Display memory stats

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.support.bundle:

ras.support.bundle
------------------

Support bundle aims at obtaining all possible debug information from the EOS system in the event of system issues. The support bundle creation process includes two things:

1. Running debug commands and obtaining the output
2. Obtaining log files

The information is obtained from all the noes, in parallel. Finally information is stored in a compressed archive form, so that it can be sent to the support team.

.. uml::

  participant CLI
  participant CMU
  participant "SSU-1"
  participant "SSU-n"
  participant "S3Server-1"
  participant "S3Server-n"
  participant "Seagate Remote Location"

  CLI -> CMU: Create bundle request
  CMU -> "SSU-1": Collect Files
  note over "SSU-1": Collect Files
  CMU -> "SSU-n": Collect Files
  note over "SSU-n": Collect Files
  CMU -> "S3Server-1": Collect Files
  note over "S3Server-1": Collect Files
  CMU -> "S3Server-n": Collect Files
  note over "S3Server-n": Collect Files
  CMU <-- "SSU-1": Collected Files
  CMU <-- "SSU-n": Collected Files
  CMU <-- "S3Server-1": Collected Files
  CMU <-- "S3Server-n": Collected Files
  note over CMU: Compressed & Prepare Bundle
  CLI <-- CMU: result

  CLI -> CMU: upload bundle request
  CMU -> "Seagate Remote Location": support bundle archive
  note over "Seagate Remote Location": Store bundle archive
  "Seagate Remote Location" -> CMU: upload completion message
  CMU -> CLI: upload completion message

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.support.callhome:

ras.support.callhome
--------------------

SSPL will log support ticket to Seagate’s support management system. That ticket will be displayed to some dashboard to Seagate’s internal support person who is going to provide support. Also SSPL will send paging message or SMS to a support person of a customer.

.. uml::

  participant Enclosure
  participant SSPL
  participant "Seagate Support Center"
  actor "Support Person"

  Enclosure -> SSPL: SNMP trap for fault
  note over SSPL: Process message
  SSPL -> "Seagate Support Center": create support ticket
  note over "Seagate Support Center": log ticket
  "Seagate Support Center" -> SSPL: ack for ticket creation
  SSPL -> "Support Person": pager/sms notification

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.simulate.pods.expander.reset:

ras.fault.simulate.pods.expander.reset
--------------------------------------

User triggers simulated sas expanders reset request using SSPL CLI, to which SSPL responds
with dummy failure event data.

.. uml::

  participant "SSPL CLI"
  participant SSPL
  participant RabbitMQ
  participant Halon

  Halon -> RabbitMQ: subscribe to messages
  Dashboard -> SSPL : subscribe to messages
  "SSPL CLI" -> SSPL: simulation request for expander reset
  note over SSPL: Process Request
  SSPL -> RabbitMQ: drive unavailable messages for all drive
  RabbitMQ -> Halon: drive unavailable messages for all drive
  note over Halon: Noted
  SSPL -> Dashboard: drive unavailable messages for all drive
  note over Dashboard : Display Action Alert
  SSPL -> RabbitMQ: drive available messages for all drive
  RabbitMQ -> Halon: drive available messages for all drive
  note over Halon: Noted
  SSPL -> Dashboard: drive available messages for all drive
  note over Dashboard : Display Action Alert

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.simulate.nw.interface:

ras.fault.simulate.nw.interface
-------------------------------

User triggers simulated network interface failure request using sspl cli, to which SSPL
responds with dummy failure event data.

.. uml::

  participant "SSPL CLI"
  participant SSPL
  participant RabbitMQ
  participant Dashboard

  Halon -> RabbitMQ: Subscribe for events
  Dashboard -> SSPL: Subscribe for events
  "SSPL CLI" -> SSPL: Simulate high latency issue
  note over SSPL: Generate message
  SSPL -> RabbitMQ: high latency issue message
  RabbitMQ -> Halon: high latency issue message
  note over Halon : noted
  SSPL -> Dashboard: high latency issue message
  note over Dashboard : Display action alert

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.simulate.pods.raid:

ras.fault.simulate.pods.raid
----------------------------

Using SSPL CLI, we can trigger SNMP trap simulation to fail one of the raid disk.SSPL will process the request and simulation response will be received by Halon through RabbitMQ and failure alert will be displayed in the Dashboard.

.. uml::

  participant "SSPL CLI"
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard
  Halon -> RabbitMQ: Subscribe for Events
  Dashboard -> SSPL : Subscribe for Events. 
  "SSPL CLI" -> SSPL: Simulate Raid disk failure
  note over SSPL : process request
  SSPL -> RabbitMQ: Disk Fail message
  RabbitMQ -> Halon: Disk Fail message
  note over Halon: noted
  SSPL -> Dashboard: Disk Fail message
  note over Dashboard: Display Action Alert

  SSPL -> RabbitMQ: Raid Degrade message
  RabbitMQ -> Halon: Raid Degrade message
  note over Halon: noted
  SSPL -> Dashboard: Raid Degrade message
  note over Dashboard: Display Action Alert

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.simulate.os.mdraid:

ras.fault.simulate.os.mdraid
----------------------------

User simulates uninstall  of an OS MD Raid disk through SSPL-CLI, SSPL will process that request and simulated response will be sent to Halon through RabbitMQ. SImilar flow will be there in case of raid os disk install. And finally, through CLI, raid add request will be performed and relevant simulation response will be sent to Halon which will be noted by Halon.

.. uml::

  Actor User
  participant "SSPL CLI"
  participant SSPL
  participant RabbitMQ
  participant Halon
  participant Dashboard

  Halon -> RabbitMQ: subscribe for events
  Dashboard -> SSPL : Subscribe for events
  User -> "SSPL CLI" : Simulate OS raid disk uninstall
  "SSPL CLI" -> SSPL :Simulate OS raid disk uninstall
  note over SSPL: Process Request
  SSPL -> RabbitMQ: Disk Fail message
  RabbitMQ -> Halon:Disk Fail message
  note over Halon: Noted
  SSPL -> Dashboard: Disk Fail message
  note over Dashboard: Display Action Alert
  SSPL -> RabbitMQ: Raid Degrade message
  RabbitMQ -> Halon:Raid Degrade message
  note over Halon: Noted
  SSPL -> Dashboard: Raid Degrade message
  note over Dashboard: Display Action Alert

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.fault.predict.pods.vdisk:

ras.fault.predict.pods.vdisk
----------------------------

TBD. There is a dependency on study of PODS.

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.security.user:

ras.security.user
-----------------

Halon will send a login request for authentication containing user credentials to SSPL. SSPL will contact Authentication Server to check validity of supplied user credentials and get a security token if user is valid. That security token will be returned back to Halon and can be used with subsequent actuator requests. When SSPL receives that security token with some request, it will send that token with detail of action requested by Halon, to authentication server again. Authentication server will validate that token and authority of user for requested action. If token is valid and user is allowed for that action, it will send back a response to SSPL indicating that user is allowed to perform action. SSPL will then perform action and send response back to Halon.

.. uml::

  participant Halon
  participant RabbitMQ
  participant SSPL
  participant "Authentication Server"

  Halon -> RabbitMQ: authentication request with user credentials
  RabbitMQ -> SSPL: authentication request with user credentials
  note over SSPL: Extract credentials
  SSPL -> "Authentication Server": check user validity
  note over "Authentication Server": Validate user
  "Authentication Server" -> SSPL: security token
  note over SSPL: Prepare response
  SSPL -> RabbitMQ: response with valid security token
  RabbitMQ -> Halon: response with valid security token

  Halon -> RabbitMQ: actuator request to restart service, with security token
  RabbitMQ -> SSPL: actuator request with security token
  note over SSPL: Extract token
  SSPL -> "Authentication Server": security token and actuator action
  note over "Authentication Server": Validate token and check authorization for action
  "Authentication Server" -> SSPL: user is authorized
  note over SSPL: Perform actuator action
  SSPL -> RabbitMQ: actuator response
  RabbitMQ -> Halon: actuator response

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.security.encryption:

ras.security.encryption
-----------------------

This use case shows how encryption is done for messages sent and received in SSPL. Encryption will use public key cryptography infrastructure. SSPL will have its own public/private keypair and consumers will have one such key pair shared among them.

At the time of sending a message, SSPL will encrypt message using public key of a consumer. When consumer receives this encrypted message it will decrypt the same message and proceed with further action if decryption is successful.

.. uml::

  participant PODS
  participant SSPL
  participant "Encryption Module"
  participant RabbitMQ
  participant Halon
  participant Dashboard

  Halon -> RabbitMQ: subscribe for messages
  Dashboard -> SSPL: subscribe for messages
  PODS -> SSPL: disk failure snmp trap
  note over SSPL: Process message
  SSPL -> "Encryption Module": disk failure message to encrypt
  note over "Encryption Module": Encrypt message using consumer's public key
  "Encryption Module" -> SSPL: encrypted disk failure message
  SSPL -> RabbitMQ: Encrypted drive failure message
  RabbitMQ -> Halon: Encrypted drive failure message
  SSPL -> Dashboard: Encrypted drive failure message
  note over Dashboard: Display alert

  Halon -> "Encryption Module": encrypted drive failure message
  note over "Encryption Module": Descrypt message using consumer's private key
  "Encryption Module" -> Halon: decrypted disks failure message
  note over Halon: noted

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.iem.log:

ras.iem.log
-----------

The IEM is a message that uniquely identifies an event with a source location it is generated from. SSPL provides an actuator to log IEM to Syslog.

.. uml::

  participant S3
  participant Mero
  participant Halon
  participant RabbitMQ
  participant SSPL
  participant Syslog

  S3 -> RabbitMQ: IEM Request
  Mero -> RabbitMQ: IEM Request
  Halon -> RabbitMQ: IEM Request
  RabbitMQ -> SSPL: All individual IEM Requests queued
  note over SSPL: Process IEM Request
  SSPL -> Syslog: log formatted message

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.ras.iem.email:

ras.iem.email
-------------

The IEM is a message that uniquely identifies an event with a source location it is generated from. SSPL provides an actuator to send email with IEM.

.. uml::

  participant S3
  participant Mero
  participant Halon
  participant RabbitMQ
  participant "SSPL CLI"
  participant SSPL
  participant "SMTP Server"
  participant "Admin email account"


  "SSPL CLI" -> SSPL: configure email for IEM
  Halon -> RabbitMQ: IEM Request
  S3 -> RabbitMQ: IEM Request
  Mero -> RabbitMQ: IEM Request
  RabbitMQ -> SSPL: All individual  IEM Request queued
  note over SSPL: Extract IEM detail
  SSPL -> "SMTP Server": email with IEM
  note over "SMTP Server": Send Email
  "SMTP Server" -> "Admin email account": email with IEM

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.mount:

nfs.mount
---------

.. uml::

   skinparam maxMessageSize 150
   skinparam wrapWidth 200
   title Mount workflow
   box "Client" #Yellow
   participant NFS_Client
   endbox
   box "NFS Ganesha" #LightBlue
   participant NFS_Server
   participant NFS_worker_thread
   participant NFS_Export
   end box
   box "KVSFS" #LightGreen
   participant FSAL_KVSFS
   participant KVSNS
   participant KVSAL
   end box
   database CLOVIS

   NFS_Client->NFS_Server: Mount request
   NFS_Server->NFS_worker_thread: Dispatch NFS mount procedure req
   activate NFS_worker_thread #FFBBBB
   NFS_worker_thread->NFS_Export: Get export by path
   rnote over NFS_worker_thread
   1. Get an export entry from its path
   2. Search the export list for the entry
   3. Check Client access
   4. Get cached inode entry for root handle
      end note

   NFS_worker_thread->NFS_Export: Look up path for the export and return FSAL handle
   NFS_Export -> FSAL_KVSFS: kvsfs_lookup_path (only "/" is exported)
   FSAL_KVSFS -> KVSNS: kvsns_get_root( )
   KVSNS -> FSAL_KVSFS : root inode (currently hardcoded 2)
   FSAL_KVSFS -> KVSNS: kvsns_getattr(cred, root)
   KVSNS -> KVSAL : kvsal_get_stat( )
   KVSAL -> CLOVIS : M0_CLOVIS_IC_GET

   rnote over FSAL_KVSFS
   1. Allocate and fill handle.
   2. Set FSAL specific ops.
   end note
   FSAL_KVSFS --> NFS_Export : Error code
   FSAL_KVSFS ->NFS_worker_thread : FSAL handle
   rnote over NFS_worker_thread
   Convert FSAL handle to file handle
   end rnote
   NFS_worker_thread -> NFS_Client : File handle.
   NFS_worker_thread --> NFS_Client : Error code
   deactivate NFS_worker_thread

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.unmount:

nfs.unmount
-----------

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.export.create:

nfs.export.create
-----------------

.. uml::

   skinparam maxMessageSize 100
   skinparam wrapWidth 200
   title Export workflow
   box "NFS Ganesha" #LightBlue
   participant NFS_Server
   participant FSAL_PSEUDO
   participant NFS_Export
   participant FSAL
   end box
   box "KVSFS" #LightGreen
   participant FSAL_KVSFS
   participant KVSNS
   participant KVSAL
   participant EXTSTORE
   end box
   database CLOVIS

   NFS_Server -> FSAL_PSEUDO: Init/Register PSEUDO FSAL
   FSAL_PSEUDO -> NFS_Server

   NFS_Server -> NFS_Export: ReadExports()
   activate NFS_Export #FFBBBB

   note over NFS_Export
   1. Read Export entries from parsed config file
   2. Fill config structure from parse tree.
   3. For each FSAL block,load and init FSAL module.
   end note
   NFS_Export -> FSAL: fsal_load_init(KVSFS)
   FSAL -> FSAL_KVSFS: kvsfs_load()
   FSAL -->NFS_Export: error code
   note over NFS_Export
   Create an export and pass FSAL sub-block to it for fsal specific processing
   end note
   NFS_Export -> FSAL :  fsal->m_ops.create_export()
   FSAL-> FSAL_KVSFS :  kvsfs_create_export()
   NFS_Export --> NFS_Server : Error Code
   note over FSAL_KVSFS
   Allocate an export point and return a handle to it to be kept in the fsal export list.
   end note
   FSAL_KVSFS -> KVSNS: ksvsns_start
   note over KVSNS
   1. Start the KVSNS library
   2. Read the config information for the library from the libkvsns config file
   end note
   KVSNS -> KVSAL: kvsal_init()
   note over KVSAL
   1. Get clovis configuration from cfg file
   2. Initialise clovis instance
   end note
   KVSAL-> CLOVIS: init_clovis()
   KVSNS -> EXTSTORE:extstore_init()
   KVSNS --> FSAL_KVSFS: return 0
   FSAL_KVSFS --> FSAL : Error code
   FSAL ---> NFS_Export : Error code
   note over NFS_Export
   Commit an export block
   end note
   deactivate NFS_Export
   NFS_Export --> NFS_Server: Error code


:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.export.delete:

nfs.export.delete
-----------------

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.export.list:

nfs.export.list
---------------

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.data.read:

nfs.data.read
-------------

NFS client opens a file and issues read request to NFS server. NFS server
forwards the request to FSAL along with the file inode number which in turn
is received by KVSNS [`nfs <entity.nfs_>`_]. KVSNS queries KV store to get the inode
attributes for the inode. Then it performs actual read operation by calling
`Ext-Store <Ext-Store>`_ read API. Finally, it updates the timestamp
attributes in the inode structure and puts the structure back in KV store.

.. uml::

  title Read workflow
  participant NFS_Client
  box "NFS Ganesha" #LightBlue
      participant NFS_Server
      participant FSAL_KVSFS
  end box
  box "KVSFS" #LightGreen
      participant KVSNS
      participant KVSAL
      participant EXTSTORE
  end box
  database CLOVIS

  NFS_Client -> NFS_Server: read()
  NFS_Server -> FSAL_KVSFS: kvsfs_read()
  FSAL_KVSFS -> KVSNS: kvsns_read()
  rnote over KVSNS: Get stat info for the given inode from KV store
  activate KVSNS
  KVSNS -> EXTSTORE: extstore_read()
  activate EXTSTORE
  note over EXTSTORE: Get the clovis object id for corresponding inode
  EXTSTORE -> CLOVIS: m0kvs_get(key = "ino.data", val)
  CLOVIS -> EXTSTORE: value for the key
  note over EXTSTORE: Do actual read operation
  EXTSTORE -> CLOVIS: m0store_pread()
  rnote over CLOVIS: Read the given object from given object id
  CLOVIS -> EXTSTORE: read size
  rnote over EXTSTORE: Update fields in struct stats and return read size
  rnote over KVSNS: Set struct stat for given inode in KV Store
  EXTSTORE -> KVSNS: read size
  deactivate EXTSTORE
  KVSNS -> FSAL_KVSFS: read size
  deactivate KVSNS
  FSAL_KVSFS -> NFS_Server: read size
  NFS_Server -> NFS_Client: read size

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.data.write:

nfs.data.write
--------------

NFS server forwards the request to FSAL (KVSFS) along with the inode number.
KVSNS receives inode number of the file to be written from NFS server. It
queries KV store to get the inode attributes for the inode number. Once
the attributes are received KVSNS performs actual write operation by calling
Extstore write API. Finally, it updates the timestamp attributes in the inode
structure and puts the structure back in KV store.

is received by KVSNS [`nfs <entity.nfs_>`_]. KVSNS queries KV store to get the inode
attributes for the inode. Then it performs actual read operation by calling
`Ext-Store <Ext-Store>`_ read API. Finally, it updates the timestamp
attributes in the inode structure and puts the structure back in KV store.

.. uml::

  title Write workflow
  participant NFS_Client
  box "NFS Ganesha" #LightBlue
      participant NFS_Server
      participant FSAL_KVSFS
  end box
  box "KVSFS" #LightGreen
      participant KVSNS
      participant KVSAL
      participant EXTSTORE
  end box
  database CLOVIS

  NFS_Client -> NFS_Server: write()
  NFS_Server -> FSAL_KVSFS: kvsfs_write()
  FSAL_KVSFS -> KVSNS: kvsns_write()
  rnote over KVSNS: Get stat info for the given inode from KV store
  activate KVSNS
  KVSNS -> EXTSTORE: extstore_write()
  activate EXTSTORE
  note over EXTSTORE: Get the clovis object id for corresponding inode
  EXTSTORE -> CLOVIS: m0kvs_get(key = "ino.data", val)
  CLOVIS -> EXTSTORE: value for the key
  note over EXTSTORE: Do actual write operation
  EXTSTORE -> CLOVIS: m0store_pwrite()
  rnote over CLOVIS: Write to given object
  CLOVIS -> EXTSTORE: write size
  rnote over EXTSTORE: Update fields in struct stats and return write size
  rnote over KVSNS: Set struct stat for given inode in KV Store
  EXTSTORE -> KVSNS: write size
  deactivate EXTSTORE
  KVSNS -> FSAL_KVSFS: write size
  deactivate KVSNS
  FSAL_KVSFS -> NFS_Server: write size
  NFS_Server -> NFS_Client: write size

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.file.create:

nfs.file.create
---------------

File create steps, start with validating access permission. For this,
KVSNS [`nfs <entity.nfs_>`_] receives the parent inode and the name of file. After
validating access permissions, KVSNS checks if entry with the given file
name already exists by doing a lookup on parent. If entry does not exist,
it starts preparing for creating new entry for the inode. Finally, it
an object is created on the extstore.

.. uml::

  title Create workflow
  participant NFS_Client
  box "NFS Ganesha" #LightBlue
      participant NFS_Server
      participant FSAL_KVSFS
  end box
  box "KVSFS" #LightGreen
      participant KVSNS
      participant KVSAL
      participant EXTSTORE
  end box
  database CLOVIS

  NFS_Client -> NFS_Server: creat()
  NFS_Server -> FSAL_KVSFS: kvsfs_creat()
  FSAL_KVSFS -> KVSNS: kvsns_creat()
  activate KVSNS
  rnote over KVSNS
      1. Get the parent stat info
      2. Check write access in stat info
      3. Do lookup on parent for new entry
  end note
  KVSNS -> KVSNS: ksvsns_create_entry()
  note over KVSNS: Increment the current inode number
  activate KVSNS #FFBBBB
  KVSNS -> KVSAL: kvsal_incr_counter(ino_counter)
  activate KVSAL
  KVSAL -> CLOVIS: m0kvs_get(key = "ino_counter", val)
  CLOVIS --> KVSAL: value for the key
  note over KVSAL: ino_counter += 1
  KVSAL -> CLOVIS: m0kvs_set(key = "ino_counter", val)
  CLOVIS --> KVSAL: error code
  KVSAL --> KVSNS: error code
  deactivate KVSAL
  rnote over KVSNS
      1. Fill in the details in struct stat for new entry
      2. Update stats in KV store
  end note
  deactivate KVSNS
  note over KVSNS: Create object on backend
  KVSNS -> EXTSTORE: extstore_creat()
  activate EXTSTORE
  rnote over EXTSTORE
      1. Generate clovis object id from inode number
      2. Update the mapping in KV store.
  end note
  EXTSTORE -> CLOVIS: m0kvs_set(key, val)
  CLOVIS --> EXTSTORE: error code
  note over EXTSTORE: Create an object
  EXTSTORE -> CLOVIS: m0store_create_object(id)
  CLOVIS --> EXTSTORE: error code
  EXTSTORE --> KVSNS: error code
  deactivate EXTSTORE
  KVSNS --> FSAL_KVSFS: error code
  deactivate KVSNS
  FSAL_KVSFS --> NFS_Server: error code
  NFS_Server --> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.file.rename:

nfs.file.rename
---------------

KVSNS [`nfs <entity.nfs_>`_] is supplied with old parent’s inode number, old filename, new
parent’s inode number and new filename as parameters for rename. KVSNS first
gets the parent list for old file from KV store and it will remove the old
parent from the list. Now, remove the old entry from dirent of old parent
and add a new entry in the dirent of a new parent. Finally, update the parent
list for new file.

.. uml::

  skinparam defaultFontSize 14

  title Rename workflow
  participant NFS_Client
  box "NFS Ganesha" #LightBlue
      participant NFS_Server
      participant FSAL_KVSFS
  end box
  box "KVSFS" #LightGreen
      participant KVSNS
      participant KVSAL
      participant EXTSTORE
  end box
  database CLOVIS

  NFS_Client -> NFS_Server: rename()
  NFS_Server -> FSAL_KVSFS: kvsfs_rename()
  FSAL_KVSFS -> KVSNS: kvsns_rename()
  activate KVSNS
  rnote over KVSNS
      1. Get the old parent stat info and check write access
      2. Get the new parent stat info and check write access
      3. Do lookup on new parent for new entry if already exists
      4. Do lookup on old parent for old entry and get stats for old entry
  end note
  note over KVSNS: Get parent list for given entry
  KVSNS -> KVSAL: kvsns_get_char(key = "ino.parentdir", val)
  activate KVSAL
  KVSAL -> CLOVIS: m0kvs_get(key = "ino.parentdir", val)
  CLOVIS --> KVSAL: value for the key
  KVSAL --> KVSNS: value for the key
  deactivate KVSAL
  rnote over KVSNS: Update old parent with new parent in parent list
  note over KVSNS: Delete old name from old parent's dentries
  KVSNS -> KVSAL: kvsns_del(key = "sino.dentries.oname")
  activate KVSAL
  KVSAL -> CLOVIS: m0kvs_del(key = "sino.dentries.oname")
  CLOVIS --> KVSAL: error code
  KVSAL --> KVSNS: error code
  deactivate KVSAL
  note over KVSNS: Add new name to new parent's dentries
  KVSNS -> KVSAL: kvsns_set_char(key = "dino.dentries.dname", val)
  activate KVSAL
  KVSAL -> CLOVIS: m0kvs_set(key = "dino.dentries.dname")
  CLOVIS --> KVSAL: error code
  KVSAL --> KVSNS: error code
  deactivate KVSAL
  note over KVSNS: Set parent list for given entry
  KVSNS -> KVSAL: kvsns_set_char(key = "ino.parentdir", val)
  activate KVSAL
  KVSAL -> CLOVIS: m0kvs_set(key = "ino.parentdir", val)
  CLOVIS --> KVSAL: error code
  KVSAL --> KVSNS: error code
  deactivate KVSAL
  KVSNS --> FSAL_KVSFS: error code
  deactivate KVSNS
  FSAL_KVSFS --> NFS_Server: error code
  NFS_Server --> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.file.open:

nfs.file.open
-------------

Objective of KVSNS is to fill kvsns_fill_open structure which holds the information of
open flags, process id and thread id. When KVSNS receives open call it gets list of open
owners for that file. Once that is got, it updates the process id and thread id for the
current request and stores the list back to KV store. Finally, it updates the kvsns_fill_open
structure with necessary details and return it back to FSAL_KVSFS.

.. uml::

   title Open workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
       participant NFS_Server
       participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
       participant KVSNS
       participant KVSAL
       participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: open()
   NFS_Server -> FSAL_KVSFS: kvsfs_open()
   FSAL_KVSFS -> KVSNS: kvsns_open()
   activate KVSNS
   note over KVSNS: Get the list of open owners
   KVSNS -> KVSAL: kvsns_get_char(key = "ino.openowner", val)
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get(key = "ino.openowner", val)
   CLOVIS --> KVSAL: value for the key
   KVSAL --> KVSNS: value for the key
   deactivate KVSAL
   rnote over KVSNS: Update the list with current pid and tid
   note over KVSNS: Set the list of owners for given entry
   KVSNS -> KVSAL: kvsns_set_char(key = "ino.openowner", val)
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_set(key = "ino.openowner", val)
   CLOVIS --> KVSAL: error code
   KVSAL --> KVSNS: error code
   deactivate KVSAL
   rnote over KVSNS
   	 Fill the //kvsns_file_open// structure with
   	 ino_number, pid, tid, and open flag details.
   end rnote
   KVSNS --> FSAL_KVSFS: error code
   deactivate KVSNS
   FSAL_KVSFS --> NFS_Server: error code
   NFS_Server --> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.file.close:

nfs.file.close
--------------

On receiving close KVSNS either deletes the open owners list if list has
single entry or it removes an entry from the list with given pid.

.. uml::

   title Close workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
       participant NFS_Server
       participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
       participant KVSNS
       participant KVSAL
       participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: Close()
   NFS_Server -> FSAL_KVSFS: kvsfs_close()
   FSAL_KVSFS -> KVSNS: kvsns_close()
   activate KVSNS
   note over KVSNS: Get the list of open owners
   KVSNS -> KVSAL: kvsal_get_char(key = "ino.openowner", val)
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get(key = "ino.openowner", val)
   CLOVIS --> KVSAL: value for the key
   KVSAL --> KVSNS: value for the key
   deactivate KVSAL
   alt #transparent if list has only single entry
   rnote over KVSNS: Update the list with current pid and tid
   note over KVSNS: Delete the list of owners for given entry
   KVSNS -> KVSAL: kvsal_del(key = "ino.openowner", val)
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_del(key = "ino.openowner")
   CLOVIS --> KVSAL: error code
   KVSAL --> KVSNS: error code
   deactivate KVSAL
   group #transparent if file marked to be deleted
   note over KVSNS: Delete the object from extstore
   KVSNS -> EXTSTORE: extstore_del()
   activate EXTSTORE
   rnote over EXTSTORE: Get the clovis object id from inode number
   EXTSTORE -> CLOVIS: m0kvs_get(key, val)
   CLOVIS --> EXTSTORE: error code
   note over EXTSTORE: Create an object
   EXTSTORE -> CLOVIS: m0store_delete_object(id)
   CLOVIS --> EXTSTORE: error code
   EXTSTORE --> KVSNS: error code
   end
   else multiple entries exists
   rnote over KVSNS: Remove the entry from owners list
   end
   KVSNS --> FSAL_KVSFS: error code
   deactivate KVSNS
   FSAL_KVSFS --> NFS_Server: error code
   NFS_Server --> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.setattr:

nfs.setattr
-----------

KVSNS [`nfs <component.nfs_>`_] is supplied with attributes to be set for the inode number.
KVSNS queries KV store to get the stored attributes for given key. It
updates the required fields and stores the structure back in KV store.

.. uml::

   title Setattr workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
       participant NFS_Server
       participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
       participant KVSNS
       participant KVSAL
       participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: setattr()
   NFS_Server -> FSAL_KVSFS: kvsfs_setattr()
   FSAL_KVSFS -> KVSNS: kvsns_setattr()
   activate KVSNS
   rnote over KVSNS
   Check write access in stat info
   end note
   note over KVSNS: Get stat info for the given inode from KV store
   KVSNS -> KVSAL: kvsal_get_stat()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get(key, val)
   CLOVIS -> KVSAL: value for the key
   deactivate KVSAL
   KVSAL -> KVSNS:  value for the key
   rnote over KVSNS: Fill the details in struct stat
   note over KVSNS: Set stat info for the given inode from KV store
   KVSNS -> KVSAL: kvsns_set_stat()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_set(key, val)
   CLOVIS -> KVSAL: error code
   deactivate KVSAL
   KVSAL -> KVSNS: error code
   KVSNS -> FSAL_KVSFS:  error code
   deactivate KVSNS
   FSAL_KVSFS -> NFS_Server: error code
   NFS_Server -> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.getattr:

nfs.getattr
-----------

KVSNS [`nfs <entity.nfs_>`_] receives inode number of a file/dir whose attributes are required.
Using the inode number it queries KV store to get the attributes.

.. uml::

   title Getattr workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
       participant NFS_Server
       participant FSAL_KVSFS
       end box
   box "KVSFS" #LightGreen
       participant KVSNS
       participant KVSAL
       participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: getattr()
   NFS_Server -> FSAL_KVSFS: kvsfs_getattr()
   FSAL_KVSFS -> KVSNS: kvsns_getattr()
   activate KVSNS
   note over KVSNS: Get stat info for the given inode from KV store
   KVSNS -> KVSAL: kvsal_get_stat()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get(key, val)
   CLOVIS -> KVSAL: value for the key
   KVSAL -> KVSNS:  value for the key
   deactivate KVSAL
   KVSNS -> FSAL_KVSFS:  error code
   deactivate KVSNS
   FSAL_KVSFS -> NFS_Server: error code
   NFS_Server -> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.setxattr:

nfs.setxattr
------------

KVSNS [`nfs <entity.nfs_>`_] receives extended attribute to be set for the given inode number.
After receiving attributes, KVSNS queries KV store to get the stored value for given xattr.
It updates the value for xattr and stores the structure back in KV store.

.. uml::

   title Setxattr workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
       participant NFS_Server
       participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
       participant KVSNS
       participant KVSAL
       participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: setxattr()
   NFS_Server -> FSAL_KVSFS: kvsfs_setxattr()
   FSAL_KVSFS -> KVSNS: kvsns_setxattr()
   activate KVSNS
   rnote over KVSNS
   Check write access in stat info
   end note
   note over KVSNS: Get the value for given xattr
   KVSNS -> KVSAL: kvsal_get_stat()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get("ino.xattr.key", val)
   CLOVIS -> KVSAL: value for the key
   KVSAL -> KVSNS:  value for the key
   deactivate KVSAL
   note over KVSNS: Update the value and store it back into KV
   KVSNS -> KVSAL: kvsns_set_binary()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_set(key, val)
   CLOVIS -> KVSAL: error code
   KVSAL -> KVSNS: error code
   deactivate KVSAL
   KVSNS -> FSAL_KVSFS:  error code
   deactivate KVSNS
   FSAL_KVSFS -> NFS_Server: error code
   NFS_Server -> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.getxattr:

nfs.getxattr
------------

KVSNS [`nfs <entity.nfs_>`_] receives an inode number of a file/dir whose attributes are required.
Using the inode number it queries KV store to get the attributes.

.. uml::

   title Getxattr workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
       participant NFS_Server
       participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
       participant KVSNS
       participant KVSAL
       participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: getxattr()
   NFS_Server -> FSAL_KVSFS: kvsfs_getxattr()
   FSAL_KVSFS -> KVSNS: kvsns_getxattr()
   activate KVSNS
   note over KVSNS: Get value for the given extended attribute
   KVSNS -> KVSAL: kvsal_get_binary()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get("ino.xattr.key", val)
   CLOVIS -> KVSAL: value for the key
   KVSAL -> KVSNS:  value for the key
   deactivate KVSAL
   KVSNS -> FSAL_KVSFS:  error code
   deactivate KVSNS
   FSAL_KVSFS -> NFS_Server: error code
   NFS_Server -> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.dir.lookup:

nfs.dir.lookup
--------------

KVSNS [`nfs <entity.nfs_>`_] receives parent’s inode number and name of the file/dir to
lookup. Upon receiving the parameters it checks if user has appropriate read
access rights to get dentry structure from parent. Finally, after getting dentries
it returns the inode number of given file/dir name

.. uml::

   title Lookup workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
       participant NFS_Server
       participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
       participant KVSNS
       participant KVSAL
       participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: lookup()
   NFS_Server -> FSAL_KVSFS: kvsfs_lookup()
   FSAL_KVSFS -> KVSNS: kvsns_lookup()
   activate KVSNS
   rnote over KVSNS
   Check read access for the parent
   end note
   note over KVSNS: Get dentries info for the given inode from KV store
   KVSNS -> KVSAL: kvsal_get_stat()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get("parent_ino.dentries.filename", val)
   CLOVIS -> KVSAL: value for the key
   deactivate KVSAL
   KVSAL -> KVSNS:  value for the key
   rnote over KVSNS: Value returned is the inode number corresponding to filename
   KVSNS -> FSAL_KVSFS:  error code
   deactivate KVSNS
   FSAL_KVSFS -> NFS_Server: error code
   NFS_Server -> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.dir.mkdir:

nfs.dir.mkdir
-------------

KVSNS receives the name of directory and parent inode number from the NFS server.
To check if user has appropriate access rights it will query KV store to get the
inode attributes of parent. After checking the access rights, KVSNS checks if entry
with the given file name already exists by doing a lookup on parent. If entry does
not exists it starts preparing for creating new entry. It creates an empty inode
structure and tries to fill in the details.

.. uml::

   title Mkdir workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
       participant NFS_Server
       participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
       participant KVSNS
       participant KVSAL
       participant EXTSTORE
   end box
   database CLOVIS

   NFS_Client -> NFS_Server: mkdir()
   NFS_Server -> FSAL_KVSFS: kvsfs_mkdir()
   FSAL_KVSFS -> KVSNS: kvsns_mkdir()
   activate KVSNS
   rnote over KVSNS
       1. Get the parent stat info
       2. Check write access in stat info
       3. Do lookup on parent for new entry
   end note
   KVSNS -> KVSNS: ksvsns_create_entry(//KVSNS_DIR//)
   note over KVSNS: Increment the current inode number
   activate KVSNS #FFBBBB
   KVSNS -> KVSAL: kvsal_incr_counter(ino_counter)
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get(key = "ino_counter", val)
   CLOVIS --> KVSAL: value for the key
   note over KVSAL: ino_counter += 1
   KVSAL -> CLOVIS: m0kvs_set(key = "ino_counter", val)
   CLOVIS --> KVSAL: error code
   KVSAL --> KVSNS: error code
   deactivate KVSAL
   rnote over KVSNS
       1. Fill in the details in struct stat for new entry
       2. Update stats in KV store
   end note
   deactivate KVSNS
   KVSNS --> FSAL_KVSFS: error code
   deactivate KVSNS
   rnote over FSAL_KVSFS
       1. Call //kvsns_getattr// to get stats
       2. Allocate file handle and fill in the details
   end note
   FSAL_KVSFS --> NFS_Server: error code
   NFS_Server --> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.dir.rmdir:

nfs.dir.rmdir
-------------

.. uml::

   title Rmdir workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
   participant NFS_Server
   participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
   participant KVSNS
   participant KVSAL
   participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: rmdir()
   NFS_Server -> FSAL_KVSFS: kvsfs_rmdir()
   FSAL_KVSFS -> KVSNS: kvsns_rmdir()
   activate KVSNS
   rnote over KVSNS
   1. Get the parent stat info
   2. Check write access in stat info
   3. Do lookup on parent for new entry
   end note
   note over KVSNS: Get dentry list for the given dir from KV store
   KVSNS -> KVSAL: kvsal_get_list()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get("dir_ino.dentries.*", val)
   CLOVIS -> KVSAL: value for the key
   KVSAL -> KVSNS:  value for the key
   deactivate KVSAL
   rnote over KVSNS #FFAAAA: Fail with //ENOTEMPTY// if entry exists in dentry list
   loop #transparent For each element in dentry list
   note over KVSNS: Delete the enrty from parent's the dentry list
   KVSNS -> KVSAL: kvsal_del()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_del("parent_ino.dentries.dir_ino")
   CLOVIS -> KVSAL: error code
   KVSAL -> KVSNS: error code
   deactivate KVSAL
   note over KVSNS: Delete the stats for given dir
   KVSNS -> KVSAL: kvsal_del()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_del("dir_ino.stat")
   CLOVIS -> KVSAL: error code
   KVSAL -> KVSNS:  error code
   deactivate KVSAL
   KVSNS -> FSAL_KVSFS:  error code
   deactivate KVSNS
   FSAL_KVSFS -> NFS_Server: error code
   NFS_Server -> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.nfs.dir.readdir:

nfs.dir.readdir
---------------


.. uml::

   title Readdir workflow
   participant NFS_Client
   box "NFS Ganesha" #LightBlue
   participant NFS_Server
   participant FSAL_KVSFS
   end box
   box "KVSFS" #LightGreen
   participant KVSNS
   participant KVSAL
   participant EXTSTORE
   end box
   database CLOVIS
   NFS_Client -> NFS_Server: readdir()
   NFS_Server -> FSAL_KVSFS: kvsfs_readdir()
   FSAL_KVSFS -> KVSNS: kvsns_readdir()
   activate KVSNS
   rnote over KVSNS
   Check read access for given directory
   end note
   note over KVSNS: Get dentry list for the given dir from KV store
   KVSNS -> KVSAL: kvsal_get_list()
   activate KVSAL
   KVSAL -> CLOVIS: m0kvs_get("dir_ino.dentries.*", val)
   CLOVIS -> KVSAL: value for the key
   KVSAL -> KVSNS:  value for the key
   deactivate KVSAL
   loop #transparent For each element in dentry list
   note over KVSNS
   1.Store the name of file in //kvsns_dentry_t//
   2.Get the inode number using the name
   from KV store
   3.Get the stats for the file using
   inode number
   4.Store inode number and stats for
   a file in //kvsns_dentry_t//
   end note
   end
   KVSNS -> FSAL_KVSFS:  error code
   deactivate KVSNS
   FSAL_KVSFS -> NFS_Server: error code
   NFS_Server -> NFS_Client: error code

:Entities:
:Owners:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:References:

.. _usecase.sns.repreb:

sns.repreb
----------

Sns repair and rebalance are implemented as a mero services.
Halon triggers sns repair on drive failure and rebalance on drive replacement.
Repair and rebalance are distributed operations which are performed by every
node in parallel. Various resource contraints can be imposed on sns repair
and rebalance, e.g. memory, cpu or network bandwidth using resource manager.
Presently, only memory bandwidth constraints are imposed using buffer pool and
sliding window algorithm during sns repair and rebalance.

.. uml::

      participant Halon
      participant repreb1
      participant repreb2
      participant repreb3
      participant repreb4
      Halon -> repreb1: Repair/rebalance trigger
      repreb1 -> repreb1: cm prepare
      repreb1 -> repreb1: cm ready
      repreb1 -> repreb2: Send initial sliding window
      repreb1 -> repreb3: Send initial sliding window
      repreb1 -> repreb4: Send initial sliding window
      repreb1 -> repreb1: cm start
      repreb1 -> Halon: reply start success/failure
      Halon -> repreb2: Repair/rebalance trigger
      repreb2 -> repreb2: cm prepare
      repreb2 -> repreb2: cm ready
      repreb2 -> repreb1: Send initial sliding window
      repreb2 -> repreb3: Send initial sliding window
      repreb2 -> repreb4: Send initial sliding window
      repreb2 -> repreb2: cm start
      repreb2 -> Halon: reply start success/failure
      Halon -> repreb3: Repair/rebalance trigger
      repreb3 -> repreb3: cm prepare
      repreb3 -> repreb3: cm ready
      repreb3 -> repreb1: Send initial sliding window
      repreb3 -> repreb2: Send initial sliding window
      repreb3 -> repreb4: Send initial sliding window
      repreb3 -> repreb3: cm start
      repreb3 -> Halon: reply start success/failure
      Halon -> repreb4: Repair/rebalance trigger
      repreb4 -> repreb4: cm prepare
      repreb4 -> repreb4: cm ready
      repreb4 -> repreb1: Send initial sliding window
      repreb4 -> repreb2: Send initial sliding window
      repreb4 -> repreb3: Send initial sliding window
      repreb4 -> repreb4: cm start
      repreb4 -> Halon: reply start success/failure

Encyclopædia
============

.. _entity.nfs:

nfs
---

Diagram shows the deployment of pNFS server over MERO. All the storage server
nodes work as NFS data servers and one of the nodes, takes up NFS metadata
server role. NFS metadata server node may optionally play NFS data server role
(provided the number of IO requests are controlled). NFS ganesha server stack
(user level implementation) is used, which will interface with Clovis which
in turn distributes the IO request to multiple MERO nodes.

.. image:: images/pNFS_Deployment.png

Shown below is the software components. NFS ganesha FSAL plugin named KVSFS
provides necessary hooks to plugin into NFS server implementation named KVSNS.
Data operations are redirected through abstraction layer EXT-STORE and metadata
(which uses Key-Value Store) through KVS-AL, which finally invokes corresponding
Clovis operations.

.. image:: images/pNFS_Architecture.png

.. _entity.fdmi:

fdmi
----

File Data Manipulation Interface (fdmi) is a scalable `publish-subscribe
<https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern>`_ Mero
interface. fdmi is a part of Clovis [`clovis <entity.clovis_>`_] interface and allows extending
Mero without changes to the core Mero code. By using FDMI external applications
(called *fdmi plugins* [`fdmi.plugin <entity.fdmi.plugin_>`_] or *fdmi apps* [`fdmi.app <entity.fdmi.app_>`_]),
potentially created by 3rd parties, can provide new functionality without
affecting performance and consistency of the core system. These applications can
run on dedicated servers, thus achieving the goal of horizontal
scalability. Examples of fdmi plugins are:

    - asynchronous replication within a cluster or between clusters or to a
      non-Mero target;

    - backup, archive and other `ILM
      <https://en.wikipedia.org/wiki/Information_lifecycle_management>`_
      applications;

    - hierarchical storage management;

    - tier-management (burst buffer prefetching and destaging), background
      compression, de-duplication;

    - online indexing;

    - compliance checking, logging, audit;

    - file system consistency checking.

.. image:: images/fdmi.dataflow.png

When a Mero instance executes a user request, it adds to its local fol [`fol <entity.fol_>`_]
a record with the details of the request. An fdmi plugin can define a *filter*
[`fdmi.filter <entity.fdmi.filter_>`_] that matches fol records of interest for the plugin by
inspecting their fields. A special fdmi service [`fdmi.service <entity.fdmi.service_>`_], running
within each Mero instance, matches each produced record against all filters and
sends the record to plugins with matching filters. The plugin receives all the
records, generated anywhere in the cluster matching its filters. This allows the
plugin to monitor certain aspects of the cluster activity and to react to them.

.. uml::

  skinparam ArrowColor Blue
  skinparam BoxPadding 10
  skinparam ParticipantPadding 5

  box "mero core node"
  participant source
  participant service
  participant filter
  participant batch
  end box

  box "plugin node"
  participant plugin
  end box

  plugin -> filter  : construct
  filter -> service : register
  source -> service : post(record)
  service -> filter : matches?(record)
  service -> batch  : add(record)
  batch   -> plugin : deliver
  plugin  -> service : reply(record)
  service -> source : put(record)
  plugin  -> service : done(record)
  service -> source : put(record)

As a key element of mero scalability architecture, fdmi allows mero core to
remain simple, clear and focused, while providing a framework to support future
features and capabilities, in which they act as tightly integrated
components. Core mero will not have to change to add new features, such as
later-developed functionality to support HSM; instead, an fdmi plugin will be
written. The ability to utilize additional nodes to run fdmi plugins is
important for horizontal scalability and compares favorably with the traditional
architectures, where new features have to be added to existing servers.

fdmi vs. POSIX
~~~~~~~~~~~~~~

As a storage-level interface to data, fdmi overcomes the inherent limitations of
POSIX, by enabling plugins to directly manipulate the storage
infrastructure. This direct interaction allows storage applications to implement
changelogs, mirrors, snapshots, indexers and other complex data management
solutions.

As scale increases, ILM features (backup, migration, HSM, replication, etc.) hit
a POSIX "bottleneck". POSIX is designed for per-file lookups via string-based
hierarchical namespace traversals; this is an extremely inefficient method to
operate on an entire file system. POSIX forces inherently storage-centric
utilities like HSM, replication, backup, and search to use the same
stat/open/read/write/close interface to data as applications use, causing
generally poor performance, scalability, recovery, and administration. In other
words, the fdmi approach to publishing batched records to subscribed users is
singularly effective because no form of scanning works at exascale.

Another problem that emerges at scale is storage application behavior in the
face of failure, either in the storage system it operates upon or in the
application itself. To avoid arbitrary inconsistencies, scalable solutions to
ILM requirements must be tightly integrated with the storage system
implementation, in particular by relying on logging changes and transactional
mechanisms capable of resuming interrupted data management processes.

On a higher level, there is a striking contrast between the file system and
database worlds. In the latter, the domain-specific SQL language underlies most
applications, making them portable (either without or with a modest effort)
between various RDBMS systems. For file systems, the only common language is
POSIX and portable backup or HSM applications are unheard of (except for
POSIX-based solutions, such as tar and rsync, which are not scalable and
generally incorrect). The development of fdmi creates the possibility of an
entirely new market for portable storage applications that are designed and
developed independently from storage systems. To this end, fdmi is portable and
can be implemented on non-Mero systems.

:Type: component

:Parents: none

:Sub-entities: [`fdmi.source <entity.fdmi.source_>`_], [`fdmi.service <entity.fdmi.service_>`_], [`fdmi.plugin <entity.fdmi.plugin_>`_],
               [`fdmi.app <entity.fdmi.app_>`_], [`fdmi.filter <entity.fdmi.filter_>`_], [`fdmi.record <entity.fdmi.record_>`_],
               [`fdmi.batch <entity.fdmi.batch_>`_], [`fdmi.reply <entity.fdmi.reply_>`_], [`fdmi.done <entity.fdmi.done_>`_]

:Use-cases:

:Owners: nikita

:Uses: [`rpc <entity.rpc_>`_], [`fol <entity.fol_>`_], [`addb <entity.addb_>`_]

:Used: [`async.replicator <entity.async.replicator_>`_]

:Properties:

    - exactly-one-semantics: fdmi guarantees that each matching record will be
      delivered exactly once, in the face of transient failures of Mero
      instances. This includes the Mero instances where records are produced and
      the instances where plugins execute. Status: not implemented.

    - transactional-consistency: optionally, a plugin can request that it is
      made a participant of the distributed transaction [`dtm.transaction <entity.dtm.transaction_>`_]
      to which a record belongs. In this case a failure of the plugin aborts the
      transaction and the failure somewhere in the cluster would involve plugins
      into recovery [`dtm.recovery <entity.dtm.recovery_>`_]. Status: not implemented.

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:
    - [] support read-only operations (which do not at the moment generate FOL
      records)

    - [] store filters in an index (rather than in configuration data-base)

    - [] define filter language

    - [] export publish interface [`fdmi.source <entity.fdmi.source_>`_] as part of clovis

    - [] specify maximal batch size and age as filter parameters

:Questions:

:Alternatives:

:References:
   - `HLD
     <https://docs.google.com/document/d/1xj5BvLeWUBj1_0mwITa_0irFJf9TqBQgllpKZkjAds0>`_
   - `high level decomposition
     <https://docs.google.com/document/d/1ne7s-fMFKX9eLx8PE4RVBiDm5Z3xDByvAHHYG0-SZSI>`_
   - `FDMI Framework and Its APIs
     <https://docs.google.com/document/d/1FPLqbeExi_vb9h-_WZ0WETY42-kC08IdAyjTG-bwu8I>`_
   - `fdmi section in Mero in prose
     <https://docs.google.com/document/d/1weQya-S3b9uW7whD0I1du2MoXoo90gx6w1kCFvGpavQ/edit#heading=h.iqthjxx2ma8u>`_

.. _entity.fdmi.source:

fdmi.source
-----------

fdmi source is an abstract interface within fdmi [`fdmi <entity.fdmi_>`_]. Implementations of
this interface provide new types of records published through
fdmi. Implementation of this interface post new records to the fdmi service
[`fdmi.service <entity.fdmi.service_>`_] for further processing.

Currently the only existing fdmi source implementation is based on fol
[`fol <entity.fol_>`_]. It posts fol records to fdmi.

:Type: abstract interface

:Parents: [`fdmi <entity.fdmi_>`_]

:Sub-entities: none

:Use-cases:

:Owners: nikita

:Uses: [`fol <entity.fol_>`_], [`addb <entity.addb_>`_]

:Used: [`fdmi.service <entity.fdmi.service_>`_]

:Properties:

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:
    - [] support addb as a source

:Questions:

:Alternatives:

:References:

.. _entity.fdmi.record:

fdmi.record
-----------

fdmi record is a structure published and subscribed to by means of fdmi. fdmi
sources [`fdmi.source <entity.fdmi.source_>`_] produce fdmi records and fdmi delivers these records
to the interested fdmi plugins [`fdmi.plugin <entity.fdmi.plugin_>`_].

A record is created within a fdmi source and then posted to the local fdmi
service [`fdmi.service <entity.fdmi.service_>`_].

:Type: abstract interface

:Parents: [`fdmi <entity.fdmi_>`_]

:Sub-entities: none

:Use-cases:

:Owners: nikita

:Uses:

:Used: [`fdmi.service <entity.fdmi.service_>`_]

:Properties:

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.fdmi.service:

fdmi.service
------------

fdmi service is a mero service [`mero.service <entity.mero.service_>`_] that runs within each mero instance
[`mero.instance <entity.mero.instance_>`_] and exports fdmi publish-subscribe interface to the fdmi
plugins [`fdmi.plugin <entity.fdmi.plugin_>`_].

fdmi service maintains the list of active filters [`fdmi.filter <entity.fdmi.filter_>`_]. fdmi
sources post records [`fdmi.record <entity.fdmi.record_>`_] to the service. The service matches
records against the filters. If a record matches a filter, the record is sent to
the plugin, which registered the filter. A record can match multiple filters and
can be sent to multiple plugins. Records are sent to plugins in batches
[`fdmi.batch <entity.fdmi.batch_>`_] to amortise the cost of network communication at the cost of
increased latency.

The service controls life time of the records, assuring that they exist until
processed by the plugin.

:Type: service

:Parents: [`fdmi <entity.fdmi_>`_]

:Sub-entities: [`fdmi.batch <entity.fdmi.batch_>`_]

:Use-cases:

:Owners: nikita

:Uses: [`fdmi.record <entity.fdmi.record_>`_], [`fdmi.source <entity.fdmi.source_>`_]

:Used: [`fdmi <entity.fdmi_>`_]

:Properties:

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.fdmi.filter:

fdmi.filter
-----------

An fdmi filter is an object created through fdmi [`fdmi <entity.fdmi_>`_] interface to
identify a sub-set of fdmi records [`fdmi.record <entity.fdmi.record_>`_]. An fdmi plugin
[`fdmi.plugin <entity.fdmi.plugin_>`_] creates a filter and then registers it globally in the
cluster. The filter, after is it registered, is available to all instances of
fdmi service [`fdmi.service <entity.fdmi.service_>`_] and is used by them to select which records
should be sent to the plugin, i.e., which records the plugin has subscribed to.

A plugin can create and register multiple filters. Filters are stored
persistently in a special index, used internally by the fdmi implementation.

:Type: data structure

:Parents: [`fdmi <entity.fdmi_>`_]

:Sub-entities: [`fdmi.batch <entity.fdmi.batch_>`_]

:Use-cases:

:Owners: nikita

:Uses: [`fdmi.record <entity.fdmi.record_>`_]

:Used: [`fdmi.plugin <entity.fdmi.plugin_>`_]

:Properties:

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.fdmi.plugin:

fdmi.plugin
-----------

An fdmi plugin is an application [`application <entity.application_>`_] that uses fdmi [`fdmi <entity.fdmi_>`_]
part of clovis [`clovis <entity.clovis_>`_] interface. Typically, a plugin would be an
executable, linked with the mero library and running on a dedicated node (i.e.,
not on one of the nodes where mero core services [`service <entity.service_>`_] are running), but
there is no technical restriction on where the fdmi interface is invoked: a
plugin can at the same time be a mero client [`mero.client <entity.mero.client_>`_], a front-end
application and so on.

A plugin constructs one or more filters [`fdmi.filter <entity.fdmi.filter_>`_] to monitor some
aspects of activity in the cluster. fdmi asynchronously delivers to the plugin
all published records [`fdmi.record <entity.fdmi.record_>`_] matching any of the filters and the
plugin can act on the records in any way.

A simplest plugin would just analyse the records for the purposes of
accumulating some statistics and producing reports.

More complex plugins could execute operations in the cluster (or outside of the
cluster) based on the received records. For example, re-executing received
records in a different cluster would result in a simple replication mechanism.

:Type: application

:Parents: [`fdmi <entity.fdmi_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses: [`fdmi.filter <entity.fdmi.filter_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.fdmi.reply:

fdmi.reply
----------

An fdmi reply is a notification sent by an fdmi plugin [`fdmi.plugin <entity.fdmi.plugin_>`_] back to
the fdmi service [`fdmi.service <entity.fdmi.service_>`_] which sent a record [`fdmi.record <entity.fdmi.record_>`_] to the
plugin. A reply signalises the service that the plugin has received and accepted
the record and is used the the service for flow control purposes. Reply is
different from a done [`fdmi.done <entity.fdmi.done_>`_] notification.

:Type: application

:Parents: [`fdmi <entity.fdmi_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses: [`rpc <entity.rpc_>`_]

:Used: [`fdmi.plugin <entity.fdmi.plugin_>`_], [`fdmi.service <entity.fdmi.service_>`_]

:Properties:

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.fdmi.done:

fdmi.done
---------

An fdmi done is a notification sent by an fdmi plugin [`fdmi.plugin <entity.fdmi.plugin_>`_] back to
the fdmi service [`fdmi.service <entity.fdmi.service_>`_] which sent a record [`fdmi.record <entity.fdmi.record_>`_] to the
plugin. A done signalises the service that the plugin has completely processed
the record and the service can purge it from its queue. Specifically, the
service won't resend done records in case of a plugin failure.

Done is different from fdmi reply [`fdmi.reply <entity.fdmi.reply_>`_].

:Type: application

:Parents: [`fdmi <entity.fdmi_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses: [`rpc <entity.rpc_>`_]

:Used: [`fdmi.plugin <entity.fdmi.plugin_>`_], [`fdmi.service <entity.fdmi.service_>`_]

:Properties:

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.fdmi.app:

fdmi.app
--------

The same as [`fdmi.plugin <entity.fdmi.plugin_>`_].

.. _entity.fdmi.batch:

fdmi.batch
----------

A batch is a collection of fdmi records [`fdmi.record <entity.fdmi.record_>`_] in which an fdmi
service [`fdmi.service <entity.fdmi.service_>`_] aggregates all filtered but not still not sent
records for a particular fdmi plugin [`fdmi.plugin <entity.fdmi.plugin_>`_].

When a record matches a filter [`fdmi.filter <entity.fdmi.filter_>`_], registered by a plugin, the
service adds the record to the batch for this plugin. When the batch grows
larger than certain size, or is not sent longer than certain internal of time,
it is sent to the plugin and emptied. Batches are the method to achieve
scalability of fdmi publish-subscribe interface by reducing the overhead of
individual record delivery at the expense of increased delivery latency.

The maximal batch size and age are specified as parameters of the filter.

:Type: application

:Parents: [`fdmi <entity.fdmi_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses:

:Used: [`fdmi.service <entity.fdmi.service_>`_]

:Properties:

:Invariants:

:State machines:

:Components: [`mero.fdmi <component.mero.fdmi_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.conf.schema:

conf.schema
-----------

Mero configuration schema represents relations between types of
configuration objects.

On the diagram below solid lines denote "is parent of" relation,
dashed lines --- "refers to" relations, black arrows --- one-to-one
relations, white arrows --- one-to-many relations.

.. graphviz::

  digraph mero_conf_schema {
    edge [arrowhead=onormal];  // most relations are "one-to-many"

    root -> "node" [label=nodes];
    root -> site [label=sites];
    root -> pool [label=pools];
    root -> profile [label="profiles\n(clients only)"];
    root -> fdmi_flt_grp [label=fdmi_flt_grps];
    fdmi_flt_grp -> fdmi_filter [label=filters];

    "node" -> process [label=processes];
    process -> service [label=services];
    service -> sdev [label=sdevs];
    site -> rack [label=racks];
    rack -> enclosure [label=encls];
    enclosure -> controller [label=ctrls];
    controller -> drive [label=drives];

    profile [label="profile\n(for clients)"];
    profile -> pool [style=dashed];

    pool -> "pool version" [label=pvers];
    "pool version" -> "site-v" [label=sitevs];
    "site-v" -> "rack-v" [label=sitevs];
    "rack-v" -> "enclosure-v" [label=children];
    "enclosure-v" -> "controller-v" [label=children];
    "controller-v" -> "drive-v" [label=children];

    edge [dir=back, weight=0, style=dashed, arrowhead=normal];
    "node" -> controller;
    site -> "site-v";
    rack -> "rack-v";
    enclosure -> "enclosure-v";
    controller -> "controller-v";
    drive -> "drive-v";
    sdev -> drive [dir=both];
  }

:Type:
:Parents:
:Sub-entities:
:Use-cases:
:Owners: vvv
:Uses: [`pool <entity.pool_>`_], [`profile <entity.profile_>`_], [`mero.process <entity.mero.process_>`_], [`mero.service <entity.mero.service_>`_],
       [`site <entity.site_>`_], [`rack <entity.rack_>`_], [`enclosure <entity.enclosure_>`_], [`controller <entity.controller_>`_], [`drive <entity.drive_>`_],
       [`device <entity.device_>`_]
:Used:
:Properties:
:Invariants:
:State machines:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:Alternatives:
:References:

.. _entity.dix.client:

dix.client
----------

Distributed index client provides an interface to access and modify
distributed indices. It uses CAS client internally and extends its
functionality by providing indices distribution.

.. _entity.dix.cm:

dix.cm
------

DIX copy machine is a state machine living on each CAS node, which
performs data restructuring and handles device, container, node, etc.
failures.

.. _entity.dix.cm.cp:

dix.cm.cp
---------

Copy packet is a special type of FOP, which arrives on corresponding nodes
where [`dix.cm <entity.dix.cm_>`_] lives and provides replica of data from
other node.

.. _entity.dix.cm.iter:

dix.cm.iter
-----------

Algorithm, which iterates over all [`dix.index <entity.dix.index_>`_] and all
[`dix.key <entity.dix.key_>`_] of Mero filesystem.

.. _entity.dix.cm.repair:

dix.cm.repair
-------------

Assume N = 1, K = 2 and two spare CAS services [`dix.cas <entity.dix.cas_>`_] are
available. In case of CAS failure, Repair operation will copy the
meta-data to spare from the replicas. There can be a update of
meta-data in spare CAS service from different replicas.

.. _entity.dix.cm.rebalance:

dix.cm.rebalance
----------------

In Rebalance operation meta-data from spare is copied to the restarted
CAS service node.

.. _entity.dix.cm.direct_rebalance:

dix.cm.direct_rebalance
-----------------------

After catalogue service restart indices will be restored using direct
rebalance from other replica services.

.. _entity.dix.cm.trigger:

dix.cm.trigger
--------------

Trigger foms are used to start repair, rebalance or direct rebalance.
It is triggered by halon by sending a trigger fop for the corresponding
operation.

.. _entity.dix.layout:

dix.layout
----------

A hash function, imask and pool version are used to create layout, and is used
to distribute indices uniformly.

.. _entity.dix.cas:

dix.cas
-------

Catalogue service (cas) is a Mero service exporting key-value
catalogues (indices). Users can access catalogues by sending
appropriate fops to an instance of the catalogue service. Externally,
a catalogue is a collection of key-value pairs, called records. A user
can insert and delete records, lookup records by key and iterate
through records in a certain order. A catalogue service does not
interpret keys or values, (except that keys are ordered as
bit-strings)—semantics are left to users.

.. _entity.dix.cas.request:

dix.cas.request
---------------

Representation of a request to remote CAS service.  Contains data to
be send over dix.cas.fops and IPC object to control the flow of this
transmission (dix.cas.rop).

.. _entity.dix.cas.rop:

dix.cas.rop
-----------

Introduces asynchronous wait mechanism to sending dix.cas.requests.

.. _entity.dix.N:

dix.N
-----

(N + K) way replication done for meta-data.

.. _entity.dix.P:

dix.P
-----

Poolwidth of CAS [`dix.cas <entity.dix.cas_>`_] services over which meta-data will be
distributed.

.. _entity.dix.K:

dix.K
-----

It is the replication factor i.e. number of CAS [`dix.cas <entity.dix.cas_>`_] service
failures supported.

.. _entity.dix.meta:

dix.meta
--------

Filesystem metadata can be accessed with dix sepcial functions,
implemented using generic 'dix gets and puts'. This metadata is stored
inside three indeces called 'root', 'layout' and 'layout-desc'. There
are three groups of functions to work with 'root', 'layout' and
'layout-descr' indices respectively. Operations for 'root' index are
synchronous, since they intended to be executed during provisioning.
Operations for 'layout', 'layout-descr' indices are asynchronous. User
can wait for asynchronous operation completion.

.. _entity.dix.req:

dix.req
--------

DIX request is a set of keys, values and other IPC-related structures
which are used to access metadata on distributed CASes. DIX requests
provide the following functions: Create new index, Lookup component
catalogues existence for an index, Delete an index, Given start keys,
get records with the next keys from an index, Get records with the
given keys from an index, Put given records in an index, Delete
records with the given keys from an index.

.. _entity.dix.key:

dix.key
-------

DIX operates with a bunch of keys that can be retrieved through the
DIX interface.  User supplies an array of records representing keys
for dix.req operation. Eventually, these keys come into CAS service,
their values are retrieved from internal catalogues and sent back to
the user.

.. _entity.dix.val:

dix.val
-------

User supplies an array of records representing vals for dix.req
operation. See dix.key.

.. _entity.dix.index:

dix.index
---------

Distributed index is an ordered container of key-value records.

.. _entity.dix.root:

dix.root
--------

It's a top level index which contains information about other indices.
It is created during bootstrap with m0dixinit.

.. _entity.dix.imask:

dix.imask
---------

Identity mask is used to distibute meta-data index in addition to key.
If infinity identity mask is specified then key is used as it is and is
used for generic indices.

.. _entity.dix.item:

dix.item
--------

Array of request items contexts. For index operations the item is
individual index, for record operations the item is individual record.

.. _entity.dix.pver:

dix.pver
--------

DIX client [`dix.client <entity.dix.client_>`_] uses this pool version to distribute
meta-data among catalogue services.

.. _entity.dix.cas.fop:

dix.cas.fop
-----------

DIX client [`dix.client <entity.dix.client_>`_] uses cas fop to send or receive meta-data
from CAS [`dix.cas <entity.dix.cas_>`_].

.. _entity.dix.di:

dix.di
------

Store checksum as part of key value while creating the record and
verify it while reading from the KVS.

.. _entity.cas:

cas
---

Catalogue service (CAS) is a local (not distributed) Mero service
exporting key-value catalogues (indices). Users can access catalogues
by sending appropriate fops to an instance of the catalogue
service. Externally, a catalogue is a collection of key-value pairs,
called records. A user can insert and delete records, lookup records
by key and iterate through records in a certain order.

.. _entity.unit:

unit
----

Unit is a basic block of data using which mero will distribute data
using layouts.

.. _entity.checksum:

checksum
--------

Checksum is computed using CRC-32 for each unit of data.

.. _entity.parity.group:

parity.group
------------

Data is divided into a parity groups having (N + 2K) units each.

.. _entity.mero.client:

mero.client
-----------

Mero client can connect to configuration and halon and fetches all the
latest state from them. Then it can issue data or meta-data requests
to mero.  Mero client could be a clovis api user or fdmi plugin or a
balancer.

.. _entity.halon:

halon
-----

Halon is a software solution for high-availability of large clusters
and real-time automated recovery.

XXX

.. _entity.interface.export:

interface.export
----------------

Use adapt for io, sns erasure coding computation.

.. _entity.adapt.md:

adapt.md
--------

Managing of md pool drives in pods also use adapt for checksum
computation if available.

.. _entity.adapt.drivesets:

adapt.drivesets
---------------

Use adapt for managing subset of drives in pods seaparetely.

.. _entity.io.checksum:

io.checksum
-----------

Compute the checksum for each unit of data and send it to the receiver
before the data, so that checksum can be verified and network
corruption can be detected. Similarly also store checksum for each
unit as part of AD meta-data, which can be verified during read
operation.

.. _entity.misdirectedwrite.md:

misdirectedwrite.md
-------------------

During put Key value operation, if the keys may get changed due to
some network or memory corruption then records are stored in some
different location. It may corrupt the the records already present. So
every put opration should check the sanity of keys using checksum or
protection tag.

.. _entity.ios:

ios
---

IO service is a mero service which cater READ/WRITE object data operations. It
also services COB meta-data operations like CREATE, DELETE, TRUNCATE, GETATTR,
and SETATTR. IO service use RDMA to copy data from/to client with the help of
pre-allocated and pre-registred memory buffers which registered with RDMA hardware.

.. uml::

  title IO service READ operation

  participant "Mero client" as C
  participant "Bulk client" as BC
  participant "Bulk server" as BS
  participant IOservice as ios
  participant "Buffer pool" as BP
  participant STOB as S

  C -> BC : Store network buffers
  C ->> ios : IO fop
  note over ios : BE transaction\nopen
  loop "buffer discriptors"
      ios -> BP : acquire buffers
      note over ios : FOM goes to wait till\nbuffers available
      ios <<-- BP : Got buffers
      loop network buffers
          ios -> S : Initiate STOB IO
          ios <- S : STOB IO Launched
          activate S
      end
      note over ios : FOM goes to\nwait queue
      S -[#red]> : <font color=red><b> Drive Transient\n<font color=red><b>Failure to HA
      destroy S
      loop network buffers
          ios <<-- S : STOB IO complete
      end
      note over ios : FOM wakeup
      loop network buffers
          BS <- ios : Load network buffer
          BS -> ios : Loaded network buffers
      end
      note over ios : FOM goes to\nwait queue
      BC <<- BS : RDMA transfer
      BC <<- BS : RDMA transfer
      BC <<- BS : RDMA transfer
      loop network buffers
          BS -->> ios : Zero copy complete
      end
      note over ios : FOM wakeup
      ios -> BP : Release buffers
  end
  note over ios : BE transaction\ndone
  C <<-- ios : IO reply fop

.. uml::

  title IO service WRITE operation

  participant "Mero client" as C
  participant "Bulk client" as BC
  participant "Bulk server" as BS
  participant IOservice as ios
  participant "Buffer pool" as BP
  participant STOB as S

  C -> BC : Store network buffers
  C ->> ios : IO fop
  note over ios : BE transaction\nopen
  loop "buffer discriptors"
      ios -> BP : acquire buffers
      note over ios : FOM goes to wait till\nbuffers available
      ios <<-- BP : Got buffers
      loop network buffers
          BS <- ios : Load network buffer
          BS -> ios : Loaded network buffers
      end
      note over ios : FOM goes to\nwait queue
      BC ->> BS : RDMA transfer
      BC ->> BS : RDMA transfer
      BC ->> BS : RDMA transfer
      loop network buffers
          BS -->> ios : Zero copy complete
      end
      note over ios : FOM wakeup
      loop network buffers
          ios -> S : Initiate STOB IO
          ios <- S : STOB IO Launched
          activate S
      end
      note over ios : FOM goes to\nwait queue
      S -[#red]> : <font color=red><b> Drive Transient\n<font color=red><b>Failure to HA
      destroy S
      loop network buffers
          ios <<-- S : STOB IO complete
      end
      note over ios : FOM wakeup
      ios -> BP : Release buffers
  end
  note over ios : BE transaction\ndone
  C <<-- ios : IO reply fop

:Type: component

:Parents: [`ios <entity.ios_>`_]

:Sub-entities: [`ios.bufferpool <entity.ios.bufferpool_>`_], [`ios.rdma <entity.ios.rdma_>`_], [`ios.stobio <entity.ios.stobio_>`_],
               [`ios.cobdomain <entity.ios.cobdomain_>`_], [`ios.locality <entity.ios.locality_>`_]

:Use-cases: [`_usecase.clovis.multi-site.create <entity._usecase.clovis.multi-site.create_>`_],
            [`_usecase.clovis.multi-site.write <entity._usecase.clovis.multi-site.write_>`_],
            [`_usecase.clovis.multi-site.read <entity._usecase.clovis.multi-site.read_>`_],
            [`_usecase.clovis.multi-site.delete <entity._usecase.clovis.multi-site.delete_>`_]

:Owners: rajanikant

:Uses: [`rpc <entity.rpc_>`_], [`rdma <entity.rdma_>`_], [`stob <entity.stob_>`_], [`addb <entity.addb_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.ios.bufferpool:

ios.bufferpool
--------------

Buffer pool is pre-allocated and pre-registered memory buffers. These buffers
address are registered with RDMA hardware. IO service create, initialise and
make provision for configured number of buffers for network domain. IO service
request for batch of buffers, and initiate RDMA data transfer with acquired
buffers. It start accumulate buffers till it manage to get required numbers of
buffers and start releasing as data transfer completes. Buffer pool exports
asynchronus interface to acquire buffers. IO service put IO FOM into wait queue
if buffers not ava

:Type:

:Parents: none

:Sub-entities: [`ios.bufferpool <entity.ios.bufferpool_>`_], [`ios.rdma <entity.ios.rdma_>`_], [`ios.stobio <entity.ios.stobio_>`_],
               [`ios.cobdomain <entity.ios.cobdomain_>`_], [`ios.locality <entity.ios.locality_>`_]

:Use-cases:

:Owners: rajanikant

:Uses: [`rpc <entity.rpc_>`_], [`rdma <entity.rdma_>`_], [`stob <entity.stob_>`_], [`addb <entity.addb_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.ios.rdma:

ios.rdma
--------

IO service use bulk transfer mechanism implemented on the top of lnet (Lustre
Networking module). It use active/passive send/receive network queues to initiate
data transfer. Server act as active side and client as passive side. Client
stores network buffers to passive queues and server push/pull for READ/WRITE
respectively. IO service put IO FOM to wait queue while RDMA mechanism transfer
buffers from client to server and moves to run queue on completion event.

:Type:

:Parents: none

:Sub-entities:

:Use-cases:

:Owners: rajanikant

:Uses: [`rpc <entity.rpc_>`_], [`rdma <entity.rdma_>`_], [`addb <entity.addb_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.ios.stobio:

ios.stobio
----------

STOB IO is asynchronous direct io operation for storage objects. IO service builds
an IO operation description and queues it against a storage object. IO completion
or failure notification is done by signalling to IO service supplied channel (an
instance of m0_chan). IO service launch io operation batch by batch depending on
available batch of acquired network buffer. IO service put IO FOM to wait queue
while STOB IO going on and move it to run queue on io completion event from STOB
IO.

:Type:

:Parents: none

:Sub-entities:

:Use-cases:

:Owners: rajanikant

:Uses: [`balloc <entity.balloc_>`_], [`addb <entity.addb_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.ios.cobdomain:

ios.cobdomain
-------------

Component object (COB) is a component (stripe) of a file, referencing a single
storage object and containing metadata describing the object. Mero keep COB for
object in [`be <entity.be_>`_]. This is on separate meta-data drive (other than data drive).

:Type:

:Parents: none

:Sub-entities: [`ios.cobdomain.cob <entity.ios.cobdomain.cob_>`_], [`ios.cobdomain.crow <entity.ios.cobdomain.crow_>`_]

:Use-cases:

:Owners: rajanikant

:Uses: [`be <entity.be_>`_], [`addb <entity.addb_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.ios.cobdomain.cob:

ios.cobdomain.cob
-----------------

Component object (COB) is a component (stripe) of a file, referencing a single
storage object and containing metadata describing the object. IO service create
cob for mero object in [`be <entity.be_>`_]. IO service also keep object attributes like -

- namespace information: parent object id, name, links, pool version
- file attributes: owner/mode/group, size, m/a/ctime, acls

IO service serve following cob operation like create, delete, truncate, get attr
setattr.

:Type:

:Parents: [`ios.cobdomain <entity.ios.cobdomain_>`_]

:Sub-entities:

:Use-cases:

:Owners: rajanikant

:Uses: [`rpc <entity.rpc_>`_], [`rdma <entity.rdma_>`_], [`stob <entity.stob_>`_], [`addb <entity.addb_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.ios.cobdomain.crow:

ios.cobdomain.crow
------------------

Component object operation are initiated by user before creating objects. But cob
create operation can be differed till data write on particular object. This is
create on write. IO service create COB on WRITE require.

:Type:

:Parents: [`ios.cobdomain <entity.ios.cobdomain_>`_]

:Sub-entities:

:Use-cases:

:Owners: rajanikant

:Uses: [`addb <entity.addb_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.ios.locality:

ios.locality
------------

A locality is a partition of computational resources dedicated to fom execution
on the node. So while creation of any FOM one locality is assigned to it and it
is there for lifetime of FOM. Some user need to provide interface to select
locality for particular FOM. IO service select locality by hashing file identifier
to distribute foms to execute on different locality.

:Type:

:Parents: none

:Sub-entities:

:Use-cases:

:Owners: rajanikant

:Uses: [`addb <entity.addb_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.provisioner:

provisioner
-----------

.. _entity.provisioner.node:

provisioner.node
----------------

.. _entity.provisioner.node.role:

provisioner.node.role
---------------------

.. _entity.provisioner.re.image:

provisioner.re.image
--------------------

.. _entity.provisioner.node.network:

provisioner.node.network
------------------------

.. _entity.provisioner.node.service:

provisioner.node.service
------------------------

.. _entity.provisioner.package:

provisioner.package
-------------------

.. _entity.sale:

sale
----

.. _entity.manufacturing:

manufacturing
-------------

.. _entity.shipment:

shipment
--------

.. _entity.assembly:

assembly
--------

.. _entity.operation:

operation
---------

.. _entity.eol:

eol
---

.. _entity.eol.secure.delete:

eol.secure.delete
-----------------

.. _entity.be:

be
--

Meta-data back-end (BE) is a module presenting an interface for a transactional
local meta-data storage. BE users manipulate and access meta-data structures in
memory. BE maps this memory to persistent storage. User groups meta-data updates
in transactions [`be.tx <entity.be.tx_>`_]. BE guarantees that transactions are atomic in the
face of process failures. BE provides support for a few frequently used
data-structures: double linked list [`be.list <entity.be.list_>`_], B-tree [`be.btree <entity.be.btree_>`_] and
extmap [`BE.extmap <entity.BE.extmap_>`_].

:Type: component

:Parents: [`mero <entity.mero_>`_]

:Sub-entities: [`be.tx <entity.be.tx_>`_], [`be.credit <entity.be.credit_>`_], [`be.seg <entity.be.seg_>`_], [`be.reg <entity.be.reg_>`_],
               [`be.domain <entity.be.domain_>`_], [`be.engine <entity.be.engine_>`_], [`be.group <entity.be.group_>`_], [`be.list <entity.be.list_>`_],
               [`be.allocator <entity.be.allocator_>`_], [`be.alloc.oom <entity.be.alloc.oom_>`_], [`be.btree <entity.be.btree_>`_],
               [`be.extmap <entity.be.extmap_>`_], [`be.paged <entity.be.paged_>`_], [`be.io <entity.be.io_>`_], [`be.log <entity.be.log_>`_],
               [`be.recovery <entity.be.recovery_>`_]

:Use-cases:

:Owners: max, anatoliy, dpodgorniy

:Uses: [`stob <entity.stob_>`_], [`reqh <entity.reqh_>`_], [`sm <entity.sm_>`_], [`fom <entity.fom_>`_], [`xcode <entity.xcode_>`_], [`addb <entity.addb_>`_],
       [`ha <entity.ha_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components: [`mero.be <component.mero.be_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

  - `HLD of meta-data back-end <https://docs.google.com/document/d/1z2SY_6x6gD1QylVOfRx0HIun2YcVOhOeZY4-qsnHbp8/edit>`_
  - `Mero meta-data back-end guide <https://docs.google.com/document/d/1uwmzKU8fbc2BOgEAQP-h_UScG8kdZ6GU7-yCIVxpAII/edit>`_
  - `BE - The metadata backend <https://docs.google.com/presentation/d/1XEPZDvgpidjYh8ZgVoeP3Sdq3AN5cKW92L4hbAPYU90/edit>`_

.. _entity.be.tx:

be.tx
-----

A transaction is a collection of updates. User adds an update to a transaction
by capturing the update region [`be.reg <entity.be.reg_>`_]. User explicitly closes a
transaction. BE guarantees that a closed transaction is atomic with respect to
process crashes that happen after transaction close call returns. That is,
after such a crash, either all or none of transaction updates will be present
in the segment [`be.seg <entity.be.seg_>`_] memory when the segment is opened next time. If a
process crashes before a transaction closes, BE guarantees that none of
transaction updates will be present in the segment memory (see BE recovery
[`be.recovery <entity.be.recovery_>`_]).

.. _entity.be.tx.payload:

be.tx.payload
-------------

A memory buffer associated with the transaction [`be.tx <entity.be.tx_>`_]. It's supposed to
be filled by the user. The payload is available to the user during the BE
recovery [`be.recovery <entity.be.recovery_>`_].

.. _entity.be.credit:

be.credit
---------

A credit is a measure of a group of updates. A credit is a pair (nr, size),
where nr is the number of updates and size is total size in bytes of modified
regions.

.. _entity.be.seg:

be.seg
------

A segment is a stob [`stob <entity.stob_>`_] mapped to an extent in process address space. Each address
in the extent uniquely corresponds to the offset in the stob and vice versa.
Stob is divided into blocks of fixed size. Memory extent is divided into pages
of fixed size. Page size is a multiple of block size (it follows that stob size
is a multiple of page size). At a given moment in time, some pages are
up-to-date (their contents is the same as of the corresponding stob blocks) and
some are dirty (their contents was modified relative to the stob blocks). In
the initial implementation all pages are up-to-date, when the segment is
opened. In the later versions, pages will be loaded dynamically on demand. The
memory extent to which a segment is mapped is called segment memory;

.. _entity.be.reg:

be.reg
------

A region is an extent within segment memory.  A (meta-data) update is a
modification of some region;

:Type: data structure

:Parents: [`be <entity.be_>`_]

:Sub-entities:

:Use-cases:

:Owners: max, anatoliy, dpodgorniy

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

  - `HLD of meta-data back-end <https://docs.google.com/document/d/1z2SY_6x6gD1QylVOfRx0HIun2YcVOhOeZY4-qsnHbp8/edit>`_

.. _entity.be.domain:

be.domain
---------

The top-level BE [`be <entity.be_>`_] data structure. Contains everything necessary to run
BE.

:Type: data structure

:Parents: [`be <entity.be_>`_]

:Sub-entities: [`be.engine <entity.be.engine_>`_], [`be.paged <entity.be.paged_>`_]

:Use-cases:

:Owners: max, anatoliy, dpodgorniy

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.be.engine:

be.engine
---------

BE transaction engine.

.. _entity.be.group:

be.group
--------

Transaction group is a collection of transactions [`be.tx <entity.be.tx_>`_] that are written
to the log [`be.log <entity.be.log_>`_] and recovered [`be.recovery <entity.be.recovery_>`_] together.

.. _entity.be.list:

be.list
-------

Persistent intrusive double-linked list. The ambient objects should be stored
in BE segment [`be.seg <entity.be.seg_>`_].

.. _entity.be.allocator:

be.allocator
------------

Allocator provides API to allocate and free memory in BE segment [`be.seg <entity.be.seg_>`_].
Memory is allocated from the segment using first-fit algorithm. The allocator
data structures are based on BE list [`be.list <entity.be.list_>`_].

.. _entity.be.alloc.oom:

be.alloc.oom
------------

A situation that happens when there is no free space in BE segment [`be.seg <entity.be.seg_>`_]
left that has the required size and alignment. In this case the allocator
[`be.allocator <entity.be.allocator_>`_] returns NULL as a pointer and -ENOMEM as the return code.
The error handling in this case is totally on the user of the allocator.

.. _entity.be.btree:

be.btree
--------

B-tree is a persistent data structure that resides inside
[`be.segment <entity.be.segment_>`_]. It provides Key-Value store asynchronous interface
to the user. B-trees are commonly used in storage systems which employ
their special locality pattern to access keys and values that can be
stored and loaded from disk in efficient manner. Typical operations
over B-tree which user can execute are: create tree, delete tree,
insert (key, val), delete (key), update (key, val), lookup (key).

.. _entity.be.extmap:

be.extmap
---------

Extent map is a persistent transactional collection of extents in an abstract
numerical name-space with a numerical value associated to each extent. It's
built on top of BE btree [`be.btree <entity.be.btree_>`_].

.. _entity.be.paged:

be.paged
--------

PageD is a cache manager for segment stob I/O. It’s designed to add a
support of big segments into BE as far as we can’t keep all segments
in memory because total size of segments may exceed virtual memory
space; it’s also can be used to optimize segment stob writes.

.. _entity.be.paged.page:

be.paged.page
-------------

Page is a memory space from (start address to start address + page
size) corresponding to data, stored inside BE segment at given offset
and page size. One can imagine a page as an in-memory read cache of
the given “extent” inside segment.

.. _entity.be.paged.cellar:

be.paged.cellar
---------------

Cellar page looks like a page, containing closed be.tx
updates. Represents a write cache of pages from (start address to
start address + page size) written into be.segment at given offset and
page size.

.. _entity.be.paged.request:

be.paged.request
----------------

PageD request is a set of page references, flags and synchronization
objects used to be processed inside PageD FOM tick(). Encapsulates
methods to notify caller of this request that it’s finished.

.. _entity.be.paged.fom:

be.paged.fom
------------

PageD FOM is a long-living specific FOM which is used to process
requests sent by the user to PageD.  It's woken up when PageD request
occurs. PageD processes requests according to their type which can be
READ, WRITE, MANAGE.

.. _entity.be.io:

be.io
-----

m0_be_io is an abstraction on top of m0_stob_io [`stob.io <entity.stob.io_>`_]. It makes all kinds
of stob I/O inside BE easier.

BE io is used by BE log [`be.log <entity.be.log_>`_] and BE paged [`be.paged <entity.be.paged_>`_] for the
segment [`be.seg <entity.be.seg_>`_] I/O.

.. _entity.be.log:

be.log
------

BE [`be <entity.be_>`_] uses write-ahead logging. Log is used as a place to write information about
transactions [`be.tx <entity.be.tx_>`_] to make the transactions recoverable [`be.recovery <entity.be.recovery_>`_].

.. _entity.be.recovery:

be.recovery
-----------

BE [`be <entity.be_>`_] allows users to modify segments [`be.seg <entity.be.seg_>`_] in transactional
[`be.tx <entity.be.tx_>`_] way. A segment is backed with a linux stob [`stob.linux <entity.stob.linux_>`_], which
doesn't provide atomic writes. BE should have consistent segments even after
crash. Therefore BE should have a part that can recover BE segments data after
crash. This part is called BE recovery.

The recovery algorithm applies to the BE segment all transactions that are
stored in the log [`be.log <entity.be.log_>`_] but are not completely written to the BE
segment.

.. _entity.stob:

stob
----

Storage object is a fundamental abstraction of M0. Storage objects offer a
linear address space for data and may have redundancy and may have integrity
data.

There are multiple types of storage objects, used for various purposes and
providing various extensions of the basic storage object interface described
below. Specifically, containers for data and meta-data are implemented as
special types of storage objects.

:Type: component

:Parents: [`mero <component.mero_>`_]

:Sub-entities: [`stob.ad <entity.stob.ad_>`_], [`stob.linux <entity.stob.linux_>`_], [`stob.io <entity.stob.io_>`_],
               [`stob.domain <entity.stob.domain_>`_], [`stob.type <entity.stob.type_>`_], [`stob.ioq <entity.stob.ioq_>`_]

:Use-cases:

:Owners: max

:Uses: [`lib <entity.lib_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components: [`stob.domain <entity.stob.domain_>`_], [`stob.ad <entity.stob.ad_>`_], [`stob.linux <entity.stob.linux_>`_], [`stob.io <entity.stob.io_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

  - stob/stob.h

.. _entity.stob.ad:

stob.ad
-------

Storage object type based on Allocation Data (AD) stored in BE segment
[`be.seg <entity.be.seg_>`_].

It uses uses BE [`be <entity.be_>`_] to store extent map [`be.extmap <entity.be.extmap_>`_] which keeps track
of mapping between logical offsets in AD stobs and physical offsets within
underlying stob.

:Type: component

:Parents: [`stob <entity.stob_>`_]

:Sub-entities: none

:Use-cases:

:Owners: max, anatoliy, dpodgorniy

:Uses: [`be <entity.be_>`_], [`stob.linux <entity.stob.linux_>`_], [`balloc <entity.balloc_>`_]

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

  - stob/ad.h

.. _entity.stob.linux:

stob.linux
----------

Implementation of m0_stob on top of Linux files.

A linux storage object is simply a file on a local file system. A linux storage
object domain is a directory containing a directory where files, corresponding
to storage objects are stored in. A name of a file is built from the
corresponding storage object local identifier (m0_stob_id).

A linux storage object domain is identified by the path to its directory.

When an in-memory representation for an object is created, no file system
operations are performed. It is only when the object is "located"
(m0_stob_locate()) or "created" (m0_stob_create()) when actual open(2)
system call is made. If the call was successful, the file descriptor
(m0_stob_linux::sl_fd) remains open until the object is destroyed.

:Type: component

:Parents: [`stob <entity.stob_>`_]

:Sub-entities:

:Use-cases:

:Owners: max, anatoliy, dpodgorniy

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

  - stob/linux.h

.. _entity.stob.io:

stob.io
-------

Asynchronous Direct Io Extensible User interface (adieu) for storage objects.

adieu is an interface for a non-blocking (asynchronous) 0-copy (direct)
vectored IO against storage objects.

A user of this interface builds an IO operation description and queues it
against a storage object. IO completion or failure notification is done by
signalling a user supplied m0_chan. As usual, the user can either wait on the
chan or register a call-back with it.

adieu supports scatter-gather type of IO operations (that is, vectored on both
input and output data).

adieu can work both on local and remote storage objects. adieu IO operations
are executed as part of a distributed transaction.

:Type: component

:Parents: [`stob <entity.stob_>`_]

:Sub-entities:

:Use-cases:

:Owners: max, anatoliy, dpodgorniy

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

  - stob/io.h

.. _entity.stob.domain:

stob.domain
-----------

Stob domain is a collection of storage objects [`stob <entity.stob_>`_] of the same type.  A
stob type [`stob.type <entity.stob.type_>`_] may have multiple domains, which are linked together
to its type.

A storage domain comes with an operation to find a storage object in the
domain. A domain might cache storage objects and use some kind of index to
speed up object lookup. This caching and indexing are not visible at the
generic level.

:Type:

:Parents:

:Sub-entities:

:Use-cases:

:Owners: max, anatoliy, dpodgorniy

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

  - stob/domain.h

.. _entity.stob.type:

stob.type
---------

Stob type defines a set of operations that can be performed on a particular
stob domain [`stob.domain <entity.stob.domain_>`_].

.. _entity.stob.ioq:

stob.ioq
--------

adieu implementation for Linux stob [`stob.linux <entity.stob.linux_>`_] is based on Linux specific
asynchronous IO interfaces: io_{setup,destroy,submit,cancel,getevents}().

IO admission control and queueing in Linux stob adieu are implemented on a
storage object domain [`stob.domain <entity.stob.domain_>`_] level, that is, each domain has its own
set of queues, threads and thresholds.

.. _entity.balloc:

balloc
------

M0 Data Block Allocator.

Balloc is a multi-block allocator, with pre-allocation. All metadata about
block allocation is stored in BE segment [`be.seg <entity.be.seg_>`_].

.. _entity.net:

net
---

The networking module provides an asynchronous, event-based message passing
service, with support for asynchronous bulk data transfer (if used with an RDMA
capable transport).

:Type:

:Parents:

:Sub-entities:

:Use-cases:

:Owners:

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

  - net/net.h
  - `HLD of Mero LNet Transport <https://docs.google.com/document/d/1oGQQpJsYV779386NtFSlSlRddJHYE8Bo5Asr4ZO4DS8/edit>`_

.. _entity.net.rdma:

net.rdma
--------

Remote Direct Memory Access - a feature that allows network buffer
[`net.buf <entity.net.buf_>`_] transfer over the network without involving neither sender's nor
receiver's CPU/caches for the data transfer.

.. _entity.net.lnet:

net.lnet
--------

The Lustre Networking module. It implements version 3.2 of the Portals Message
Passing Interface, and provides access to a number of different transport
protocols including InfiniBand and TCP over Ethernet.

.. _entity.net.devm0lnet:

net.devm0lnet
-------------

The Device driver [`m0mero.ko <entity.m0mero.ko_>`_] provides access to the Kernel Core LNET
[`net.lnet <entity.net.lnet_>`_] implementation through ioctl requests on the "/dev/m0lnet"
device.

.. _entity.net.tm:

net.tm
------

Transfer machine - this data structure tracks message buffers [`net.buf <entity.net.buf_>`_] and
supports callbacks to notify the application of changes in state associated
with these buffers.

.. _entity.net.buf:

net.buf
-------

Network Buffer - a memory area used to send and receive data to/from the
network [`net <entity.net_>`_].

.. _entity.net.ep:

net.ep
------

Addressable network [`net <entity.net_>`_] end point.

.. _entity.net.domain:

net.domain
----------

Network domain - a collection of network [`net <entity.net_>`_] resources.

.. _entity.net.buffer_pool:

net.buffer_pool
---------------

Network buffer pool allocates and manages a pool of network buffers.
Users request a buffer from the pool and after its usage is over
gives back to the pool.

.. _entity.system.upgrade:

system.upgrade
--------------

.. _entity.system.downgrade:

system.downgrade
----------------

.. _entity.fs.readonly:

fs.readonly
-----------

:Type:
:Parents:
:Use-cases:
:Owners:
:Uses:
:Used:
:Properties:
:Invariants:
:State machines:
:Components:
:Features:
:Tests:
:History:
:Known problems:
:Requests:
:Questions:
:Alternatives:
:References:

.. _entity.s3.region:

s3.region
---------

Cloud computing resources are hosted in multiple locations world-wide. These locations are composed of S3 Regions and Availability Zones. Each S3 Region is a separate geographic area. Each S3 Region has multiple, isolated locations known as Availability Zones. Resources aren't replicated across S3 Regions unless we do so specifically.

.. _entity.s3.bucket:

s3.bucket
---------

s3 bucket is a logical container wherein we upload all objects.
We can upload any number of objects to the bucket. S3 provides API to manage bucket.

.. _entity.s3.bucket.name:

s3.bucket.name
--------------

Once a S3 bucket is created its name cannot be changed. Below are the rules for naming S3 bucket
in all regions.

  - Bucket names must be unique across all existing bucket names in S3
  - Bucket names must comply with DNS naming conventions.
  - Bucket names must be at least 3 and no more than 63 characters long.
  - Bucket names must not contain uppercase characters or underscores.
  - Bucket names must start with a lowercase letter or number.
  - Bucket names must be a series of one or more labels. Adjacent labels are separated by a single period (.). Bucket names can contain lowercase letters, numbers, and hyphens. Each label must start and end with a lowercase letter or a number.
  - Bucket names must not be formatted as an IP address (for example, 192.178.62.32).
  - In case of virtual hosted–style buckets with Secure Sockets Layer (SSL), the SSL wildcard certificate only matches buckets that don't contain periods. To work around this, use HTTP or write your own certificate verification logic. Its recommended that we do not use periods (".") in bucket names when using virtual hosted–style buckets.

.. _entity.s3.object:

s3.object
---------

In S3 data is stored as objects within resource called buckets. Objects consist of object data and its metadata. Metadata is a set of name-value pairs that describe the Object. There are two kinds of Object metadata system metadata and user-defined metadata.

.. _entity.s3.object.name:

s3.object.name
--------------

.. _entity.s3.bucket.operation:

s3.bucket.operation
-------------------

Following are the supported bucket operations

  - DELETE Bucket
  - DELETE Bucket policy
  - DELETE Bucket replication
  - DELETE Bucket tagging
  - GET Bucket (List Objects)
  - GET Bucket acl
  - GET Bucket location
  - GET BucketPolicyStatus
  - GET Bucket Object versions
  - GET Bucket policy
  - GET Bucket replication
  - GET Bucket tagging
  - GET Bucket versioning
  - HEAD Bucket
  - List Multipart Uploads
  - PUT Bucket
  - PUT Bucket acl
  - PUT Bucket policy
  - PUT Bucket replication
  - PUT Bucket tagging
  - PUT Bucket versioning

.. _entity.s3.object.operation:

s3.object.operation
-------------------

Following are the supported object operations

  - Delete Multiple Objects
  - DELETE Object
  - DELETE Object tagging
  - GET Object
  - GET Object ACL
  - GET Object tagging
  - HEAD Object
  - PUT Object
  - PUT Object - Copy
  - PUT Object acl
  - PUT Object tagging
  - Abort Multipart Upload
  - Complete Multipart Upload
  - Initiate Multipart Upload
  - List Parts
  - Upload Part

.. _entity.s3.endpoint:

s3.endpoint
-----------

An endpoint is a URL that is entry point for the S3 service.

.. _entity.s3.bucket.tagging:

s3.bucket.tagging
-----------------

Bucket Tagging (Cost allocation tagging) is to label the S3 buckets with a tag that consists of a key and a value in order to track its costs. Cost allocation tagging can be used also for other resources like S3 objects. Various benefits of tagging
  - Categories the resources
  - Resources can be identified easily
  - Helps in management of resources
  - Helps in cost calculation

.. _entity.s3.user:

s3.user
-------

User identity created to grant access to any S3 resources, like bucket, object etc.

.. _entity.s3.account:

s3.account
----------

Account is a container of a set of users and helps support multi-tenancy.

.. _entity.s3.subaccount:

s3.subaccount
-------------

Account can be divided into smaller subaccounts to group specific set of users.

.. _entity.s3.statistics:

s3.statistics
-------------

S3 statistics give visibility into the S3 operations(GET/PUT/DELETE etc) for different sources like Bucket, Object etc.

.. _entity.s3.availability.zones:

s3.availability.zones
---------------------

Availability zones in AWS are like deployment sites which are physically seperate in order to achieve better durability and failover.

.. _entity.s3.service:

s3.service
----------

S3 as a service provides a mechanism using REST APIs to perform various operations on object storage.

.. _entity.s3.api:

s3.api
------

A set of HTTP APIs to perform various operations on object storage.

.. _entity.s3.client:

s3.client
---------

Client application using S3 REST APIs to consumer object storage features.

.. _entity.s3.dns:

s3.dns
------

.. _entity.s3.loadbalancer:

s3.loadbalancer (haproxy)
-------------------------

Load balancer manages distribution of workload across a deployed service.

.. _entity.s3.PUT:

s3.PUT
------

Http REST API used to upload Object.

.. _entity.s3.policies:

s3.policies
-----------

A set of rules to define access permissions to various S3 resources for specific identity.

.. _entity.s3.server:

s3.server
---------

A instance of server providing S3 interface over seagate object storage.

.. _entity.s3.auth.server:

s3.auth.server
--------------

Server instance providing REST APIs to create and manage Accounts/Users and perform Authentication and Authorization for S3 server.

.. _entity.s3.failover:

s3.failover
-----------

.. _entity.s3.sse:

s3.sse
------

Server side encryption feature in S3. Object are encrypted in memory before storing on disk.

.. _entity.rpc:

rpc
---

RPC is a high level part of Mero network stack. It contains operations allowing
user to send [`rpc.item <entity.rpc.item_>`_]s, containing file operation packets (FOP) to network
destinations. RPC layer speaks about FOPs being sent from and to end-points and
RPCs, which are containers for FOPs, being sent to services.  RPC is a part of a
Mero serivices as well as a Mero clients.

.. _entity.rpc.item:

rpc.item
--------

RPC item is a portion of information sent over RPC. Typicaly FOPs aggregate such
items inside itself, which allows them to be sent over the network.

.. _entity.rpc.connection:

rpc.connection
--------------

RPC connection is a container of session. Typically, user can create several
sessions commint onto the same end-point for its needs.

.. _entity.rpc.session:

rpc.session
-----------

RPC session is a long-lived Mero service object. Clients may establish sessions,
FOPs are sent over these sessions to the server side. Sender may also terminate
sessions and check out whether it's failed.

.. _entity.rpc.formation:

rpc.formation
-------------

Formation algorithm acts on a cache of RPC items. It decides the items to be
selected from the cache and it puts them together in an RPC object. The
formation algorithm makes sure that maximum size of an RPC object is limited so
that it makes optimal use of network bandwidth

.. _entity.rpc.timeout:

rpc.timeout
-----------

RPC timeout controls how long data sent over RPC connection may remain
unacknowledged by the client or server side before a connection is forcefully
moved to terminated or failed state.

When rpc item reply is not received within a timeout, then rpc item is
marked as failed with timeout.

.. _entity.rpc.resend:

rpc.resend
----------

RPC items are resend 'nr' times if item reply is not received within a
resend time interval.

.. _entity.rpc.reply:

rpc.reply
----------

For rpc items which are not a one way items, receiver send the data
back in rpc reply.

.. _entity.sns.parity_math:

sns.parity_math
---------------

Provide math base for algorithms of SNS parity (checksums) calculation
for given data units, quick update of parity units in case of minor
data changes, repair algorithms in case of failure.

.. _entity.rconfc:

rconfc
------

Redundant Configuration Client

Redundant configuration client library -- rconfc -- provides an interface for
Mero applications working in cluster with multiple configuration servers
[`confd <entity.confd_>`_].

Rconfc supplements confc functionality and processes situations when cluster
dynamically changes configuration or loses connection to some of confd servers.

.. _entity.fop:

fop
---

FOP, file operation packet, a description of file operation suitable for sending
over network and storing on a storage device. File operation packet (FOP)
identifies file operation type and operation parameters.

.. _entity.fop.entrypoint:

fop.entrypoint
--------------

The entrypoint fop.

When a new process wants to join the cluster it sends so-called entrypoint fop
to Halon [`halon <entity.halon_>`_]. The fop contains information about the process. Then
Halon decides what to do with the process (allow it to join or not, for
example), and then sends entrypoint reply fop with all the information needed
to work in the cluster. This includes, among other things, the active RM
[`rm.active <entity.rm.active_>`_] and the list of confds [`confd <entity.confd_>`_] from which the process can
get the cluster configuration [`conf <entity.conf_>`_].

.. _entity.confd.quorum:

confd.quorum
------------

After the entrypoint reply fop [`fop.entrypoint <entity.fop.entrypoint_>`_] is received rconfc
[`rconfc <entity.rconfc_>`_] queries the version of configuration on every confd [`confd <entity.confd_>`_]
from the list of confds. The quorum value tells the minimal number of confds
that should have the same version for rconfc to return the configuration from
these confds to the user.

.. _entity.throughput:

throughput
----------

Throughput indicates how fast the data can be processed.

It can be measured in bytes/s, bits/s, object operations/s etc.

:References:
  - https://en.wikipedia.org/wiki/Throughput

.. _entity.queue.depth:

queue.depth
-----------

The queue depth indicates how many operations are sent to the device but the
completion (either successful or unsuccessful) hasn't been received from the
device yet. For PODS [`pods <entity.pods_>`_] we need to have a large amount of operations (>
100) queued at the same time, so the PODS controllers can achieve maximum
performance.

:References:
  - https://en.wikipedia.org/wiki/IOPS
  - https://en.wikipedia.org/wiki/Native_Command_Queuing

.. _entity.rdma:

rdma
----

:References:
  - [`net.rdma <entity.net.rdma_>`_]
  - https://en.wikipedia.org/wiki/Remote_direct_memory_access

.. _entity.bitrot:

bitrot
------

This is the case when the data on drive gets corrupted due to some bit flip
and subsequent read on that data fails the checksum verification. This is
handled by triggering a background scrub [`durability.bitrot <entity.durability.bitrot_>`_] operation.

.. _entity.bitrot.md:

bitrot.md
---------

A bit rot case [`bitrot <entity.bitrot_>`_] when the metadata becomes corrupted due to a bit
rot. It's supposed to be detected with checksums in every BE object
[`be.obj <entity.be.obj_>`_]. The only solution we have now to handle the metadata bit rot is
to consider all segments in this drive as lost and then repair all the missing
data using SNS rebalace [`sns.rebalance <entity.sns.rebalance_>`_] or SNS direct rebalance
[`sns.rebalance.direct <entity.sns.rebalance.direct_>`_] for data and DIX rebalance [`dix.rebalance <entity.dix.rebalance_>`_] or
DIX direct rebalance [`dix.rebalance.direct <entity.dix.rebalance.direct_>`_] for the missing distributed
indexes [`dix <entity.dix_>`_].

.. _entity.fop.cob.check:

fop.cob.check
-------------

A fop to verify DI checksum [`checksum <entity.checksum_>`_].

The DI mismatch can happen during the BSC scan [`bsc.scan <entity.bsc.scan_>`_] or read I/O from
Clovis [`clovis <entity.clovis_>`_]. This fop can trigger DI checksum verification for a
particular offset within a particular cob (this includes data, parity and spare
units) without returning data back to the Clovis application. If the DI
checksum mismatch is detected the unit is handled in the same way as if it was
detected by read I/O or BSC scan.

.. _entity.bsc:

bsc
---

Background is a mero service that detects as well as repairs the corrupted data
block. Background scrub can be triggered in case of checksum mismatch during a
typical read io or by the background scrub [`bsc.scanner <entity.bsc.scanner_>`_].

.. _entity.bsc.scanner:

bsc.scanner
-----------

Background scrub scanner is a part of the background scrub service and iterates over
allocated mero data block space ([`balloc <component.balloc_>`_]) to detect any corruption using the
[`di <entity.di_>`_] interface. On detecting the corruption, scanner submits a
[`bsc.repair.request <entity.bsc.repair.request_>`_].

.. _entity.bsc.repair.request:

bsc.repair.request
------------------

A background scrub repair request can be submitted to background scrub
([`durability.bitrot <entity.durability.bitrot_>`_]) in context of a read i/o or by the background scrub scanner.

.. _entity.bsc.bad.list:

bsc.bad.list
------------

We can encounter DI [`checksum <entity.checksum_>`_] data mismatch for a data/parity/spare unit. It's
known that such units doesn't have the right data, but they may still have at
least parts of the data. The BSC bad list contains all the units with the DI
checksum that doesn't match the data along with the connections to the files
the units are the part of. This way the system administrator still can recover
whatever is left from such units. It's also possible that the bit rot is
transient, i.e. next read would return the data that matches the DI checksum,
so it also should be possible to do something about it.

The bad list comes with the list management capability, like enumerating all
the failed units with their files, removing the "DI has failed" flags,
re-checking the failed units to see if the DI mismatch persists etc.

.. _entity.bsc.request.list:

bsc.request.list
----------------

Background scrub persists the in-progress scrub requests list in [`be <entity.be_>`_] in-order
to continue the operation post node restart.

.. _entity.bsc.scrubber:

bsc.scrubber
------------

Scrubber processes the [`bsc.repair.request <entity.bsc.repair.request_>`_] using the mero fom infrastructure.

.. _entity.balance.capacity:

balance.capacity
----------------

Balance capacity runs as a mero service on every mero node. Every participating mero node is
elected as a co-ordinator in turns. Every co-ordinator completes its part of balancing and hands
over control back to halon, which in turn elects the next co-ordinator. Halon provides required
policy support to the participating nodes to identify if a particular file needs balancing or
not.

:Type: service

:Parents: none

:Sub-entities: [`balcap.coordinator <entity.balcap.coordinator_>`_], [`balcap.service <entity.balcap.service_>`_], [`balcap.policy <entity.balcap.policy_>`_],
               [`balcap.restripe <entity.balcap.restripe_>`_], [`balcap.rm <entity.balcap.rm_>`_], [`balcap.bufferpool <entity.balcap.bufferpool_>`_],
               [`balcap.bsc <entity.balcap.bsc_>`_], [`balcap.cob.version <entity.balcap.cob.version_>`_]

:Use-cases:

:Owners: mandar, nachiket

:Uses: [`rm <entity.rm_>`_], [`layout <entity.layout_>`_], [`pool.policy <entity.pool.policy_>`_]

:Used: [`halon <entity.halon_>`_]

:Properties:

    - policy-based-balance: balance capacity consults the policy provided by
      halon for every object that is encountered on the storage is being
      balanced.

    - fault-tolerant: balance capacity uses layout to rebuild the data on the
      fly in-case of io errors. Co-ordinator failures are appropriately handled
      with the help of Halon and dtm0.

    - flow control: Progress of balance capacity can be controlled by defining
      appropriate rm resources and limits, e.g. buffer pool.

:Invariants:

:State machines:

:Components: [`scaleout <component.scaleout_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:
    - [] export policy interface for objects that can be invoked to check if the
         corresponding object requires balance.

:Questions:

:Alternatives:

:References:

.. _entity.balance.capacity.pool:

balance.capacity.pool
---------------------

On pool modification, i.e. on addition of some new hardware to the pool, based on some
policy, halon triggers a capacity balance operation on the modified pool.

.. _entity.balance.capacity.policy:

balance.capacity.policy
-----------------------

This is the capacity balance policy configured at Halon in-order to decided the following,
- when to start balance capacity operation,
- when to stop balance capacity operation,
- if a particular file needs balancing or not.

.. _entity.balance.capacity.index:

balance.capacity.index
----------------------

This is about balancing the [`dix.index <entity.dix.index_>`_] metadata across the storage.

.. _entity.balance.restripe:

balance.capacity.restripe
-------------------------

During balance capacity, it is possible that the existing data may have to be restriped,
e.g. the if initial data was striped using 8 + 2 layout, it may have to striped using
16 + 4.

.. _entity.balance.coordinator:

balance.capacity.coordinator
----------------------------

Balance capacity coordinator is selected by Halon before starting the balance operation.
Coordinator is the one who drives the balance operation. Every node acts as a coordinator
in a sequential manner. Coordinator pull the relevant data from relevant nodes and
redirects it to its new location.

.. _entity.balance.layout.set:

balance.capacity.layout.set
---------------------------

Balancing operation also changes a file's layout which is updated after the file is completely
balanced. File layout will be changed using dtm0.

.. _entity.balance.capacity.bsc.request:

balance.capacity.bsc.request
----------------------------

If balance capacity io operation encounters a data or metadata corruption, it submits a background
scrub ([`bsc <entity.bsc_>`_]) request for the corrupted data in context of that io.

.. _entity.balance.capacity.rm:

balance.capacity.rm
-------------------

Balance capacity uses file locks implemented as a resource, managed by resource manager,
to handle concurrent io.

.. _entity.balance.capacity.bufferpool:

balance.capacity.bufferpool
---------------------------

Balance capacity uses buffer pools to read and write data. Buffer pool size can be tuned
in-order to impose any memory bandwidth constraints on balance capacity operation.

.. _entity.balance.capacity.dtm0:

balance.capacity.dtm0
---------------------

Meta data update failures during balance capacity operations make use of DTM0. Pending
transactions are redone by dtm0.

.. _entity.balance.capacity.cob.versioning:

balance.capacity.cob.versioning
-------------------------------

Version attribute is added to cob on its creation. During balance operation, the old
cob is mantained until the data is relocated and the corresponding metadata is
successfully updated in DIX. The old cob is cleanedup after the metadata update.
Any failure during the metadata update is handled with the help of dtm0.

.. _entity.balance.capacity.flowcontrol:

balance.capacity.flowcontrol
----------------------------

Balance capacity operation flow can be controlled using various resource manager constraints
like memory, network, cpu, etc. For example, memory constraints can be applied through buffer
pools.

.. _entity.balance.capacity.evacuate:

balance.capacity.evacuate
-------------------------

Balancing can also be required in case the storage capacity is reduced.


.. _entity.pdclust:

pdclust
-------

Mero uses parity declustered layout for storing objects. Mero's implementation of pdclust layout
has two advantages:
	-Formulaic layout: object offset is translated to disk and disk offset using a formula,
         hence a lookup table is not needed.
	-Improved IO and recovery performance.
Under this layout every object is divided into chunks known as [`pdclust.unit <entity.pdclust.unit_>`_] and parity
is calculated for collection of such units. Such a collection is known as a parity group. Number of
parities to be calculated per parity group is configurable. Eg. If
object size is 65 MB, and unit size is 1 MB. Then a possible way is to form 9 parity groups,
each consisting of 8 units, and say 3 parity units per group (3 parities calculated over 8 units).

.. _entity.pdclust.unit:

pdclust.unit
------------

Data of an object is divided into fixed sized chunks known as units..

.. _entity.pdclust.paritygroup:

pdclust.paritygroup
-------------------

A parity group is a collection of [`pdclust.unit <entity.pdclust.unit_>`_]s along with a set of parities calculated over them. It also includes
spare units (to which repaired units from a parity group go).

.. _entity.pdclust.target:

pdclust.target
--------------

Target stores the unit of a parity group into a [`pdclust.targetframe <entity.pdclust.targetframe_>`_].

.. _entity.pdclust.targetframe:

pdclust.targetframe
-------------------

Target frame represents a basic storage block in a logical storage space associated with an object on a given target.
Frame size is same as the unit size associated with an object.


.. _entity.failuredomain:

failuredomain
-------------

Any hardware resource or collection of thereof, failure of which can potentially cause a loss
of object data is called as a failure domain. The typical set of failure domains
includes (but is not limited to):

	- site;
	- rack;
	- enclosure;
	- controller;
	- disk.

.. _entity.failuredomain.tree:

failuredomain.tree
------------------

A hierarchial topology in which various [`failuredomain <entity.failuredomain_>`_] are arranged is
known as a failure domains tree. With every [`pool.pver <entity.pool.pver_>`_] there are associated
a collection of failure domains trees.

.. _entity.pool:

pool
----

A pool provides an abstraction for collection of configuration objects that can
potentially be used to store Mero objects.
These objects form a tree hierarchy of which a pool is the root node.

.. _entity.pool.pver:

pool.pver
---------

A pool version is a subtree of a pool [`pool <entity.pool_>`_]. The layout of an object gets
associated with a pool version. Multiple pool versions (possibly with
overlapping subtrees) can be formed over a given pool.  A pool version can have
a presence either in memory or in configuration database or at both places. Mero
has three types of pool versions:
	- [`pool.pver.actual <entity.pool.pver.actual_>`_];
	- [`pool.pver.formulaic <entity.pool.pver.formulaic_>`_];
	- [`pool.pver.virtual <entity.pool.pver.virtual_>`_];

.. csv-table:: Pool version
   :header: "Pool version type", "In memory", "In conf DB"
   :widths: 15, 10, 10

   "Actual", "Present", "Present"
   "Formulaic", "Absent", "Present"
   "Virtual", "Present", "Absent"

.. figure:: images/pver.relations.png

	Relation in formulaic and virtual pool versions.

.. _entity.pool.pver.actual:

pool.pver.actual
----------------

An actual pool version is also known as a base pool version. All the configuration objects that constitute an
actual pool version are known at the time of Mero bootstrap and hence an actual pool version is present in
persistent configuration schema.

.. _entity.pool.pver.formulaic:

pool.pver.formulaic
-------------------

A formulaic pool version is present in conf DB and is always associated with one
of the actual pool versions [`pool.pver.actual <entity.pool.pver.actual_>`_].  It only indicates how many
failures at various levels of actual pool version can be allowed to create new
pool versions that would skip the failed objects.

.. _entity.pool.pver.virtual:

pool.pver.virtual
-----------------

A virtual pool version is an instance of formulaic pool version
[`pool.pver.formulaic <entity.pool.pver.formulaic_>`_] and hence is always associated with an actual pool
version [`pool.pver.actual <entity.pool.pver.actual_>`_] and a set of failed configuration objects. An in
memory instance of a virtual pool version gets created only when it needs to be
associated with an object.

.. _entity.pool.pver.allowancevector:

pool.pver.allowancevector
-------------------------

An allowance vector associated with a pool version [`pool.pver <entity.pool.pver_>`_] has dimension
same as the height of pool version tree and each level of it indicates the
allowed number of failures at that level in pool version tree that can be
accommodated to create a formulaic pool version [`pool.pver.formulaic <entity.pool.pver.formulaic_>`_].

.. _entity.pool.pver.tolerancevector:

pool.pver.tolerancevector
-------------------------

A tolerance vector associated with a pool version [`pool.pver <entity.pool.pver_>`_] has dimension
same as the height of pool version tree, and it indicates the number of failures
that can be tolerated for reading an object, at each level of the tree.

.. _entity.pool.pver.pooldev:

pool.pver.pooldev
-----------------

In memory representation for a storage device used by pools component.

.. _entity.pool.pver.poolmachine:

pool.pver.poolmachine
---------------------

An abstraction for the collection of pooldevs [`pool.pver.pooldev <entity.pool.pver.pooldev_>`_] present in
a pool version.

.. _entity.pool.pver.selectionpolicy:

pool.pver.selectionpolicy
-------------------------

A pool version [`pool.pver <entity.pool.pver_>`_] can be constructed out of a pool [`pool <entity.pool_>`_] using
different policies. E.g., removing failed devices or removing most populated
devices, or removing devices with most bad blocks (as perceived by Mero)
etc. Pool versions are searched as per policy defined with pool while `create
object [`clovis.multi-site.create <usecase.clovis.multi-site.create_>`_] operation.

.. _entity.pool.pver.poolmachine.poolmachinestate:

pool.pver.poolmachine.poolmachinestate
--------------------------------------

The state of devices that are part of a pool machine [`pool.pver.poolmachine <entity.pool.pver.poolmachine_>`_] indicates the availability of device.
The possible states are:
:: M0_PNDS_UNKNOWN, M0_PNDS_ONLINE, M0_PNDS_FAILED, M0_PNDS_OFFLINE,
	M0_PNDS_SNS_REPAIRING, M0_PNDS_SNS_REPAIRED

.. _entity.hld:

hld
---

:Type: document

:Parents: [`development.process <entity.development.process_>`_]

:Sub-entities:

:Use-cases: [`development.cycle <usecase.development.cycle_>`_]

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.dld:

dld
---

Detailed level design (dld) is a document specifying a software module or
modules in detail sufficient to start coding.

:Type: document

:Parents: [`development.process <entity.development.process_>`_]

:Sub-entities:

:Use-cases: [`development.cycle <usecase.development.cycle_>`_]

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.dldinsp:

dldinsp
-------

dld inspection is a process of reviewing a dld with the purpose of verifying
that it conforms to the appropriate hld [`hld <entity.hld_>`_] and the architecture
specification.

Inspections can be internal (done by the dld authors) or external. An inspection
starts with the formal checks including:

    - conformance to the dld style guide;

    - dld must include the list of use cases and the list should be complete;

    - dld must describe all the data-structures introduced and their invariants;

    - dld must describe concurrency control policies, including:

      + locks, description of data-structures and invariants protected by the locks;

      + lock ordering;

      + wait channels and condition variables;

      + concurrency assumptions;

    - memory management and ownership of allocated objects;

    - interfaces and their implementations, including pre- and post- conditions
      of all functions;

    - liveness of data structures and allocated memory;

    - description of possible failures and their handling.

:Type: process

:Parents: [`development.process <entity.development.process_>`_]

:Sub-entities:

:Use-cases: [`development.cycle <usecase.development.cycle_>`_]

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.codeinsp:

codeinsp
--------

:Type: document

:Parents: [`development.process <entity.development.process_>`_]

:Sub-entities:

:Use-cases: [`development.cycle <usecase.development.cycle_>`_]

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components:

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.rack:

rack
----

.. image:: images/eos.rack.png
   
:Type: hardware

:Parents: [`hardware <entity.hardware_>`_]

:Sub-entities: [`enclosure <entity.enclosure_>`_], [`switch <entity.switch_>`_]

:Use-cases: 

:Owners: nikita

:Uses: 

:Used: 

:Properties:

:Invariants:

:State machines:

:Components: 

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.clovis:

clovis
------

clovis (not an acronym) is the external interface that Mero provides to
applications. It replaces the traditional file-system access interfaces and
protocols (posix, nfs, cifs, etc.) as well as proprietary control
interfaces. The clovis interface consists of sub-interfaces:

    - clovis access sub-interface that provides basic storage abstractions: data
      object, key-value index, transaction, operation;

    - clovis extension sub-interface that provides access to the fdmi
      sub-system;

    - clovis management sub-interface that provides features for system
      configuration, deployment, monitoring and analysis.

The goals of the clovis interface design are:

    - scalable

      + no central meta-data,
      + tunable consistency guarantees,
      + aggregation, containers;

    - complete

      + storage access,
      + management,
      + monitoring;

    - portable

      + not tied to POSIX,
      + user space and kernel;

    - flexible;
    - no fixed meta-data scheme;
    - extensibility, plugins;
    - re-usable concepts;
    - trusted users can extend system: layouts, resources;
    - simple security mechanism: capabilities

There are several typical clovis use cases:

    - in the simplest case, an application uses only access interface. Storage
      objects are used to keep data and key-value indices for meta-data,
      effectively using Mero as a scalable transactional object store;

    - alternatively, a special front-end application can use clovis to implement
      a “standard” interface, like posix, hdf5 or s3, enabling existing
      applications to run on top of Mero;

    - by using clovis extension interface, an application can extend core Mero
      functionality and add more features without modifying core code or
      hampering scalability.

:Type: interface

:Parents: [`mero <entity.mero_>`_]

:Sub-entities: [`clovis.object <entity.clovis.object_>`_], [`clovis.index <entity.clovis.index_>`_], [`clovis.container <entity.clovis.container_>`_],
               [`clovis.operation <entity.clovis.operation_>`_]

:Use-cases:

:Owners: nikita

:Uses:

:Used: [`s3 <entity.s3_>`_], [`nfs <entity.nfs_>`_]

:Properties:

:Invariants:

:State machines:

:Components: [`mero <component.mero_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:
   - `Clovis presentation
     <https://docs.google.com/presentation/d/1gh3qtPK6l69VjORMWkogMCO24kGEJ4dDjIdcNGkSFtk>`_
   - `Clovis section in Mero in prose
     <https://docs.google.com/document/d/1weQya-S3b9uW7whD0I1du2MoXoo90gx6w1kCFvGpavQ/edit#heading=h.pmj23otvi9c>`_
   - `Clovis introduction presentation
     <https://docs.google.com/presentation/d/1TEa6AKxdnqTowLdVSxuP7jefElO3eQwoxQ8SBDmXrds>`_

.. _entity.clovis.entity:

clovis.entity
-------------

clovis objects [`clovis.object <entity.clovis.object_>`_], indices [`clovis.index <entity.clovis.index_>`_] and containers
[`clovis.container <entity.clovis.container_>`_] are collectively known as clovis *entities*. An entity is
uniquely identified by a cluster-wide never reused identifier called fid
[`fid <entity.fid_>`_], selected by the user when the entity is created. A fid is 128-bits
wide with 8 most significant bits identifying the entity type. mero provides a
fid allocator [`fid.allocator <entity.fid.allocator_>`_] interface to generate unique fids.

clovis provides bulk operations on entities, which is an ability to invoke a
specified server-side function with contents of an entity as parameters:

    - for an object this means invoking the function with object data-blocks as
      input parameters;

    - for an index this means invoking the function with index key-value records
      as parameters;

    - for a container this means invoking the function with the fid of each
      container element as a parameter.

:Type: interface

:Parents: [`mero <entity.mero_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components: [`mero <component.mero_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.clovis.object:

clovis.object
-------------

A (data) object is used to store large amounts of data. An object is an array of
fixed size blocks (block size specified at object creation time), indexed by
64-bit integers. Data, stored in the object, can be read or overwritten at the
block granularity. Contrary to more traditional object stores and file system,
an object has no attributes associated with it, except for an 128-bit
identifier, selected by the application when the object is created. By design,
all meta-data, if any, associated with the object must be stored separately in
the key-value indices [`clovis.index <entity.clovis.index_>`_]. Instead of per-object attributes, an
object has block attributes (**not currently implemented**), associated with
every block. Such block attributes are used by application to store meta-data
required in data IO path, for example, data integrity check-sums, or
cryptographic attributes. Storing such per-block meta-data attributes in a
key-value index would incur a cost of separate network round-trip in every IO
operation. Mero stores attributes together with its internal meta-data, which is
accessed to locate the data blocks, thus avoiding an extra cost of attribute
lookup. Data IO to an object is fully vectored: data transfer (read or write) is
done between a sequence of application-provided buffers and a sequence of block
extents within the object. IO semantics is deliberately weak. The implementation
has complete freedom is executing actual in any order without ordering
guarantees in case of failure halfway through the operation.  Additional
operations are provided to pre-allocate particular data-blocks and free
allocated data-blocks.

.. image:: images/clovis.object.png

Note: the clovis interface calls in the diagram above are pseudo-code.

:Type: interface

:Parents: [`clovis <entity.clovis_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components: [`mero <component.mero_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:
   - [] implement block attributes

:Questions:

:Alternatives:

:References:

.. _entity.clovis.index:

clovis.index
------------

A key-value index is used to store meta-data and small amounts of data (in the
sense that it is more efficient to use objects [`clovis.object <entity.clovis.object_>`_] to store
large amounts of data). An index stores records, where a record is a key-value
pair. Indices have a fairly standard repertoire of operations to insert, delete,
lookup and iterate records. The interface is fully vectored (multiple records
can be updated or queried at once). An application can create an arbitrary
number of indices, each named by a 128-bit identifier, assigned by the
application. As in case of objects, an index has no application-visible
attributes.

The plural of "index" is "indices".

.. image:: images/clovis.index.png

Note: the clovis interface calls in the diagram above are pseudo-code.

:Type: interface

:Parents: [`clovis <entity.clovis_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components: [`mero <component.mero_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.clovis.container:

clovis.container
----------------

:Type: interface

:Parents: [`clovis <entity.clovis_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components: [`mero <component.mero_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.clovis.operation:

clovis.operation
----------------

:Type: interface

:Parents: [`clovis <entity.clovis_>`_]

:Sub-entities:

:Use-cases:

:Owners: nikita

:Uses:

:Used:

:Properties:

:Invariants:

:State machines:

:Components: [`mero <component.mero_>`_]

:Features:

:Tests:

:History:

:Known problems:

:Requests:

:Questions:

:Alternatives:

:References:

.. _entity.copymachine:

copymachine
-----------

Copymachine is a generic state machine based framework that can be used to implement a distributed operation
like sns repair and rebalance. It provides some common functionality that can be used by specific operations,
e.g. resource throttling and state persistence.

.. _entity.copymachine.pump:

copymachine.pump
----------------

Copymachine pump is an entity that is part of generic copy machine framework. Copy machine pump is
implemented using mero fom infrastructure. Copy machine pump creates copy packets ([`copymachine.copypacket <entity.copymachine.copypacket_>`_])
and invokes the operation (e.g. repair/rebalance) specific data iterator to populate the copy packet.

.. _entity.copymachine.aggregation.group:

copymachine.aggregation.group
-----------------------------

Aggregation group represents a collection of copy packets that share the common properties, e.g. a parity
group.

.. _entity.copymachin.copypacket:

copymachine.copypacket
----------------------

Copymachine copypacket is part of the generic copy machine framework and has some common information that can
be used by the specific operations. Specific operation like sns repair or rebalance can inherit from this
generic copymachine copypacket and have their own specific information augmented to it.

.. _entity.copymachine.proxy:

copymachine.proxy
-----------------

Copymachine proxy represents a remote copymachine on another node in cluster. It is an entity in proxy
protocol and is used to implement the sliding window protocol. A typical copy machine proxy maintains
the state and progress of remote copymachine. Proxy protocol can be used to notify about the failure
of a remote copy machine to others participating in the distributed operation.

.. _entity.copymachine.proxy.update:

copymachine.proxy.update
------------------------

As the state of copymachine changes the peer copymachines are updated through the proxy protocol
via the corressponding proxies.

.. _entity.copymachine.sliding.window:

copymachine.sliding.window
--------------------------

Various resource constraints can be imposed using sliding window. Presently for sns repair and rebalance,
the memory constraints are imposed using buffer pool. So, sender and receiver of the data create data
packets ([`copymachine.copypacket <entity.copymachine.copypacket_>`_]) only if the required space is available in the relevant buffer
pools. As the copy packets are created, which are typically part of an aggregation group
([`copymachine.aggregation.group <entity.copymachine.aggregation.group_>`_]), the corresponding aggregation group is added to the sliding window.
The update to the sliding window is then notified to the copy machine peers and the sender can then send
and receiver can receive the data accordingly.

.. uml::

   participant sender
   participant sendRecvProxy
   participant bufferPoolOut
   participant bufferPoolIn
   participant receiver
   participant recvSendProxy

   sender <-> bufferPoolOut: has space Yes/No
   sender -> sender: outgoing aggregation group create
   receiver <-> bufferPoolIn: has space Yes/No
   receiver -> receiver: incoming aggregation group create
   receiver <-> recvSendProxy: isUpdated ? No
   receiver -> sender: sw[lo, hi] update send
   sender -> sendRecvProxy: update proxy
   sender -> receiver: send copy packets as per sw[lo, hi]

.. _entity.copymachine.status:

copymachine.status
------------------

Copymachine can typically be in prepare, ready, started, failed or stopped state. This state is communicated
to the peer copymachines through proxies.

.. _entity.copymachine.sns.repreb:

copymachine.sns.repreb
----------------------

Sns repair and rebalance are implemented as mero services which use and extend the generic copy machine
framework.

.. _entity.copymachine.sns.repreb.trigger:

copymachine.sns.repreb.trigger
------------------------------

Sns repair or rebalance are implemented as copy machines. On a drive failure, halon triggers sns repair and
triggers sns rebalance on drive replacement.

.. uml::

   participant Halon as H
   participant repreb1 as rr1
   participant agStore as ags
   participant BE
   participant swUpdateFom as swu
   participant repreb2 as rr2

   H -> rr1:   Repair/Rebalance trigger
   H -> rr2:   Repair/Rebalance trigger
   rr1 -> rr1: cm prepare
   rr1 -> ags: AG store start
   ags -> BE: fetch saved sw
   BE -> ags: data ready
   ags -> ags: initialise local SW
   ags -> swu: start/update SW
   rr1 -> rr1: cm ready
   rr1 -> rr2: Send initial sliding window
   rr1 -> rr1: cm start
   rr1 -> H: reply start success/failure
   rr2 -> rr2: cm prepare
   rr2 -> rr2: cm ready
   rr2 -> rr1: Send initial sliding window
   rr2 -> rr2: cm start
   rr2 -> H: reply start success/failure

.. _entity.copymachine.sns.repreb.copypacket:

copymachine.sns.repreb.copypacket
---------------------------------

Sns copy machines (repair and rebalance) inherit and augment generic copy packet which is used to carry
data from storage to some peer copymachine.

.. uml::

   participant pump
   participant snscm
   participant Iter
   participant cpFom
   participant stob
   participant parityMath
   participant proxy
   participant repreb2

   pump -> snscm: create copy packet
   snscm -> pump: success
   pump -> Iter: copy packet metadata fill
   Iter -> Iter: file/group/cob next
   Iter -> Iter: cp setup
   Iter -> pump: copy packet ready
   pump -> cpFom: launch copy packet fom
   cpFom -> cpFom: INIT
   cpFom -> cpFom: READ
   cpFom -> stob: read data
   stob -> cpFom: read success/failure
   cpFom -> cpFom: XFORM/FINI
   cpFom <-> parityMath: transform data and accumulate
   cpFom -> cpFom: SWCHECK
   cpFom <-> proxy: Can send ? Yes/No
   cpFom -> cpFom: SEND
   cpFom <-> repreb2: copy packet send/Ack
   cpFom -> cpFom: FINI

.. _entity.sns.repreb.aggregation.group:

copymachine.sns.repreb.aggregation.group
----------------------------------------

Similar to copy packet, sns aggregation group is derived from its generic counter part and stores additional
information that is specific to the operation.

.. _entity.copymachine.sns.repreb.agiter:

copymachine.sns.repreb.agiter
-----------------------------

Sns aggregation group iterator, iterates over file namespare and identifies and creates aggregation groups
that have incoming copy packets from the peer copy machines. Aggregation group iterator is invoked in
context of copy machine sliding window update fom.

.. _entity.copymachine.sns.repreb.iter:

copymachine.sns.repreb.iter
---------------------------

Sns repair/rebalance iterator or data iterator, iterates over file namespace and identifies and creates aggregation
groups having data that is to be sent to some other peer copy machine in the cluster with respect to failure.

.. uml::

   participant pump
   participant iter
   participant cobDomain
   participant RM
   participant MD
   participant layout
   participant bufferPool

   pump -> iter : setup copy packet
   iter -> iter : FID_NEXT
   iter <-> cobDomain : file next
   iter <-> iter : FID LOCK(no-op in oostore)
   iter -> iter : ATTR_LAYOUT_FETCH
   iter -> MD : attr fetch request
   MD -> iter : attr fetch success/failure
   iter -> iter : GROUP_NEXT
   iter <-> layout : identify next relevant group
   iter -> iter : COB NEXT
   iter <-> layout : identify next unit cob
   iter -> iter : CP SETUP
   iter <-> bufferPool : buffer fetch
   iter -> pump : cp ready

.. _entity.copymachine.sns.repreb.storage:

copymachine.sns.repreb.storage
------------------------------

Sns repair/rebalance reads and writes data to storage using mero stob infrastructure. Storage is one of the phases
of sns copy packet fom through which the io is performed.

.. _entity.copymachine.sns.repreb.xform:

copymachine.sns.repreb.xform
----------------------------

After reading data sns repair performs certain tranformations on relevant data in-order to recover the missing
data.

.. _entity,copymachine.sns.repreb.net:

copymachine.sns.repreb.net
--------------------------

Every sns repair/rebalance after transforming its local data to partially recover the missing data, the
recovered piece of missing data is sent to its relevant destination.

.. _entity.copymachine.sns.repreb.halon:

copymachine.sns.repreb.halon
----------------------------

Halon is responsible to send appropriate commands to the sns repair/rebalance services in the cluster,
which comprises of start, quiesce, resume, abort and status of sns repair/rebalance operation.

.. _entity.copymachine.sns.repreb.layout:

copymachine.sns.repreb.layout
-----------------------------

Sns repair/rebalance uses layout to identify the remaining live as well the target units in the cluster
during a recovery operation.

.. _entity.copymachine.sns.repreb.paritymath:

copymachine.sns.repreb.paritymath
---------------------------------

Sns parity math module provides the data transformation assistence during sns repair operation.

.. _entity.copymachine.sns.repreb.bufferpool:

copymachine.sns.repreb.bufferpool
---------------------------------

Sns repair/rebalance provisions a fixed size on incoming and outgoing bufferpool when the operation
starts. The buffers required for io and transformations are provided though these 2 bufferpool.
They also help in imposing memory bandwidth contraints on the repair/rebalance operations.

.. _entity.copymachine.sns.repreb.be:

copymachine.sns.repreb.be
-------------------------

Sns repair/rebalance uses mero backend (be) to persist the progress of the operation, mainly the
last aggregation group ([`copymachine.sns.repreb.aggregation.group <entity.copymachine.sns.repreb.aggregation.group_>`_]) successfully processed.

.. _entity.copymachine.sns.repreb.di:

copymachine.sns.repreb.di
-------------------------

Sns repair/rebalance uses mero di to read and write checksums during io operations. On checksum
mismatch, a background scrub request is submitted to repair the relevant corrupted data block.
It is possible that the drive failure is reported in subsequent io failures.

.. _entity.copymachine.sns.repreb.bsc:

copymachine.sns.repreb.bsc
--------------------------

Background scrub request is submitted in case of checksum mismatch during sns read operation.
Repair will wait until the relevant block is repaired or marked as permanently failed.

.. _entity.copymachine.sns.repreb.ccio:

copymachine.sns.repreb.ccio
---------------------------

Concurrent io is supported during sns repair/rebalance operations. Sns uses proxy protocol,
support from mero stob layer or file locks inorder to maintain operational consistency.

.. _entity.copymachine.sns.repreb.rm:

copymachine.sns.repreb.rm
-------------------------

It is possible that sns uses file locks managed my mero resource manager (rm) inorder to
maintain operational consistency during concurrent io.

.. _entity.copymachine.sns.repreb.pause.resume:

copymachine.sns.repreb.pause.resume
-----------------------------------

Sns repair/rebalance operation can be paused and resumed by halon due to various reasons,
transient device failure during the operation, for instance.

.. _entity.copymachine.sns.repreb.abort:

copymachine.sns.repreb.abort
----------------------------

Halon may abort sns repair/rebalance operation due to failures when operation is in
progress.

.. _entity.metadata.internal.cob:

metadata.internal.cob:
---------------------

Component objects are created as part of mero file creation, which are part of mero internal
metadata that uses mero backend ([`be <entity.be_>`_]), on a particular mero node.
Failure to read or write the mero internal metadata due to metadata drive failure or bitrot
triggers direct rebalance.

.. _entity.metadata.internal.cob.attr:

metadata.internal.cob.attr
--------------------------

Cob attributes are stored as part of cob metadata in mero backend.

.. _entity.metadata.internal.balloc:

metadata.internal.balloc
------------------------

Mero block allocator ([`balloc <entity.balloc_>`_]) for a particular block device is part of mero internal metadata.
Balloc corruption is treated as disk failure and can trigger direct disk rebalance or node rebalance.

.. _entity.metadata.internal.sns:

metadata.internal.sns
---------------------

Any corruption to internal metadata is treated as metadata drive failure and may trigger direct sns
rebalance.

.. _entity.metadata.internal.be.checksum:

metadata.internal.be.checksum
-----------------------------

Mero backend ([`be <entity.be_>`_]) maintains internal checksum per be extent or segment in-order to detect
metadata bitrot.

.. _entity.metadata.external.s3:

metadata.external.s3
--------------------

External metadata is typically the application specific metadata that is stored in Mero.
S3 is one of the applications that uses mero distributed key value store ([`mero.dix <component.mero.dix_>`_]). This
interface can be accesed using clovis C-API.

.. _entity.metadata.external.clovis:

metadata.external.clovis
------------------------

Clovis is the mero C-API that provides interfaces to access the external metadata.

.. _entity.metadata.external.be.checksum:

metadata.external.be.checksum
-----------------------------

Mero backend ([`be <entity.be_>`_]) maintains internal checksum per be extent or may be for overall be segment.
This checksum can be used to detect bitrot in eaternal metadata. In case of bitrot, direct dix
rebalance is triggered.

.. _entity.metadata.external.bitrot:

metadata.external.bitrot
------------------------------------

Bitrot in external metadata is handled by starting DIX ([`mero.dix <component.mero.dix_>`_]) rebalance.

.. _entity.metadata.external.dix.dtm0:

metadata.external.dix.dtm0
-----------------------------------

External metadata transactions are executed through dtm0. Failures during a distributed
transaction, e.g. node restart during object creation in [`mero.dix <component.mero.dix_>`_] using synchronous
replication scheme, are handled by redoing the incomplete part of the transaction.

.. _entity.metadata.external.directrebalance:

metadata.external.directrebalance
---------------------------------

Direct rebalance ([`dix.direct_rebalance <component.dix.direct_rebalance_>`_]) is triggered in case of permanent failures
such as metadata drive failure.


.. _entity.ha.halon.model:

ha.halon.model
--------------

TS
- replicas with quorum;
- persistence (optional, can be turned on and off);
- RC migration is invisible to other TS and SATs;
- can be connected to SATs and/or TSes.

SAT
- no persistence;
- can be connected to SATs and/or TSes;
- can communicate with other components.

Messaging
- messages are reliable: either the message is delivered or source and/or
  destination is failed permanently;
- messages between the same source and destination are ordered;
- the same message can't be delived twice;
- p2p messages:
  - TS to TS;
  - TS to SAT;
  - SAT to SAT;
  - TS or SAT to other component;
- broadcast messages:
  - TS to all SATs.
- message contains:
  - src
    - fid
    - current epoch
    - start epoch
    - software version
  - dst
    - fid (if available)
- message interface:
  - send() function
    - in
      - message
      - destination
    - out
      - error code (success, destination is dead)
  - received() callback


.. _entity.ha.state:

ha.state
--------

HA state is an attribute of a conf object [`conf.obj <entity.conf.obj_>`_] that tells what code
can do with the object.

List of HA states: ONLINE, TRANSIENT, FAILED.

The semantics of HA states may be different for different conf object types.

The users of HA states must assume that HA states may change at any moment.

pool
ONLINE - the pool can be used for I/O (data or metadata).
TRANSIENT - the pool must not be used for I/O.
FAILED - N/A.

node, process, service, sdev
ONLINE - the node/process/service is running, sdev can serve I/O requests.
TRANSIENT - the node/process/service is not running, but it may become ONLINE
again. sdev can't serve I/O requests.
FAILED - the node/process/service is not going to be running. All internal
states associated with the node/process/service can be wiped.

site/rack/enclosure/controller/drive
ONLINE - can be used now
TRANSIENT - can't be used right now, but may become ONLINE in the future.
FAILED - physically broken, needs replacement.

pver has no HA state. In every place where pver HA state may be needed the
pver's pool HA state must be used.

root has no HA state.

Use cases:
1. cluster start/stop;
2. mkfs;
3. dynamic clients;
4. software update;
5. failover;
6. process start (including BE recovery);
7. hardware addition/removal/replacement/movement;
8. software (node/process/service) addition/removal/replacement/movement;
9. admin disables something (that corresponds to a conf object) (puts HOLD);
10. cancelling of I/O operations;
11. state interpretation by current data and metadata I/O path;
12. DTM recovery;
13. fsck (interactive/non-interactive);
14. data integrity verification;
15. disaster recovery;
16. RC migration;
17. loss of TS quorum;
18. cluster management when there is no quorum;
19. detecting failed processes by rpc timeouts from other processes;
20. cluster mkfs/start/stop with missing/failed drives/nodes;
21. active RM migration;
22. 10k nodes scalability;
23. pool modifications;
24. controlled (m0d) and not controlled (clovis app) Mero processes.

Notes:
- fids are not reused;
- assumption: Halon doesn't use RPC for internal communications (need to
  verify);
- current client: Clovis, data/metadata I/O.

.. _entity.ha.state.hold:

ha.state.hold
-------------

A HOLD can be put on a conf object. The user can also remove a HOLD that was
put on a conf object before.

Semantics: a conf object with a HOLD can't have ONLINE HA state.

Internally each conf object has a bool flag called FAILED_FLAG that tells if
the object is failed or not and a list of HOLDs. HA state is calculated this
way:
- if the object has FAILED_FLAG set then it's in FAILED state;
- if the object has HOLDs then it's in TRANSIENT state;
- otherwise it's ONLINE.

Each HOLD has an owner. HOLD owner must be one of the conf objects in the
configuration or a Halon rule instance. HOLD is removed automatically iff:
- the conf object that is the owner is FAILED and it's not in the configuration;
- the rule instance that is the owner is finished.

HA state can be calculated ignoring a set of HOLDs. In this case the HA state
calculations are performed the same way as if every HOLD from the set doesn't
exist.

Each HOLD has 2 values associated with it: a 64-bit value and a short string.
The semantics of those values are defined by the HOLD owners are should be
completely ignored by everyone else.

Halon provides interfaces to put HOLDs on conf objects and to remove HOLDs from
conf objects.

.. _entity.ha.state.dependency:

ha.state.dependency
-------------------

There are state changes to conf objects that lead to state changes to another
objects. Such kind of state changes are called dependent state changes. They
are managed by Halon.

Dependent state changes are calculated without ignoring HOLDs.

HA states dependencies:
- if a node goes from ONLINE to TRANSIENT all processes on the node receive a
  HOLD with the node as the owner. The associated 64-bit value is 0, the string
  is "inherited". If the node goes back to ONLINE the hold is removed;
- the same HOLD logic is applied to a process and every service in this
  process;
- if a node has FAILED_FLAG set the FAILED_FLAG is set for all processes on the
  node;
- the same applies for a process with FAILED_FLAG set and all services in the
  process;
- if a drive goes to TRANSIENT or FAILED state a HOLD is put on all storage
  devices that are using this drive. The HOLD value is 0, the string is
  "dependent". If the drive goes back to ONLINE the HOLD is removed;
- the same logic applies to a controller and the controller's node.


.. _entity.ha.state.epoch:

ha.state.epoch
--------------

Each HA state change has an epoch associated with the change. The epoch is a
64-bit number.
TODO explain better.

HA epoch properties:
- If a state change to state_0 for an object with fid_0 happened in epoch_0,
  and then after some time a state change to state_1 happened with an object
  with fid_1 in epoch_1, then epoch_0 < epoch_1;
- dependent state changes (directly or indirectly) can have the same epoch.

Use of epoch:
- epoch is defined for a configuration, not for a single object;
- "Mero process uses HA epoch epoch_0" means the processe's latest received HA
  state change from Halon has HA epoch epoch_0;
- Mero RPC [`mero.rpc <component.mero.rpc_>`_] sends processes HA epoch in each rpc item and on the
  receiving end the epoch is compared (somehow) to the receiving processes HA
  epoch. Then RPC layer decides if it can process the rpc item.


.. _entity.ha.state.broadcast:

ha.state.broadcast
------------------

Halon decides when HA state of an object changes. On each such change Halon
broadcasts object fid and the new HA state to each Mero process. A set of HA
state changes can be broadcasted as a single broadcast message.

Properties:
- all Mero processes receive exactly the same HA state broadcast messages;
- if a Mero process receives broadcast messages A, then B, then C, then each
  Mero process receives them in the the following order: A, then B, then C,
  i.e. each process receives broadcast messages from Halon in exactly the same
  order.

The broadcast consists of:
- a set of (object fid, new object HA state) pairs;
- HA epoch.

All state changes for a single epoch are broadcasted in a single broadcast
message, i.e. 2 different broadcast messages must have 2 different HA epochs.
If a broadcast message with epoch_0 arrives before a broadcast message with
epoch_1, then epoch_0 < epoch_1.


.. _entity.ha.conf:

ha.conf
-------

Mero configuration for the HA purposes is defined as a subset of RG available
to Mero as a set of conf objects.


.. _entity.conf.object.categories:

conf.object.categories
----------------------

List of conf object categories:
- software: node, process, service, sdev;
- hardware: site, rack, enclosure, controller, drive;
- pools/profiles: profile, pool, pver, obj-v.


.. _entity.ha.hold.handler:

ha.hold.handler
---------------

HOLD handler is a set of methods and HOLD-based conditions.

Each conf object has a set of HOLD handlers. The HOLD handlers can be added
when the object is created and during the lifetime of the object. All HOLD
handlers are removed just before the object is destroyed and they can be
removed when the object is alive.

HOLD handler basic methods:
- add() is called when the HOLD handler is added to the conf object;
- del() is called when the HOLD handler is removed from the conf object.

Each condition has 2 HOLD handlers associated with it: one is called when the
condition changes state from False to True, another one is called when the
condition changes state from True to False.

HOLD handler methods signature:
- in: RG, conf object fid, last RC migration epoch;
- out: messages to send, RG diff, HOLD diff, HOLD handlers diff.

.. _entity.ha.hold.handler.queue:

ha.hold.handler.queue
---------------------

HA handler queue is a queue of HOLD handler methods.

Each conf object has a HOLD handler FIFO queue.

How the queue is managed:
- when an object is created it has initial list of HOLD handlers. For each HOLD
  handler add() method is added to the queue;
- when a new HOLD handler is added add() method is added to the queue;
- when it's decided that a conf object is going to be removed del() method for
  each HOLD handler is added to the queue in the reverse (relatively to the
  add() method) order;
- when a HOLD handler is removed from the object del() method for the HOLD
  handler is added to the queue;
- when a HOLD is added or removed from a conf object each condition for each
  HOLD handler is evaluated before and after the HOLD. If the condition changes
  state the corresponding method is added to the queue;
- after an add() or del() for a HOLD handler method is added to the queue the
  queue is scanned for the pairs of add() and del() methods for the same HOLD
  handler. If such pairs are foind both methods from the pair are removed from
  the queue;
- after a condition change method is added the queue is scanned to find pairs
  of condition change handlers for the same HOLD method. All methods from all
  such pairs are removed from the queue;

Options:
- option 1:
  - all methods from the queue are executed asynchronously, one by one. When
    the method finishes execution it's removed from the queue;
- option 2:
  - all methods from the queue are executed one by one. Each execution leads
    either to "method went to sleep" case or "method is done". If a method goes
    to sleep it's going to be executed next time it an be (tricky thing: how to
    determine this?). If the "method is done" it's removed from the queue.


.. _entity.conf.object.persistence:

conf.object.persistence
-----------------------

Each conf object has at least the following in a persistent storage (RG):
- fid;
- FAILED_FLAG;
- list of HOLDs;
- list of HOLD handlers;
- HOLD handler queue.


.. _entity.ha.hold.example:

ha.hold.example
---------------

- general
  - default-shutdown-hold
    - add()
      - copy "shutdown" hold from parent
      - remove default-shutdown-hold
- node(server)
  - default-shutdown-hold
  - mero-kernel-config
  - mero-kernel-start
- process(m0d)
  - default-shutdown-hold
  - m0d-config
  - m0d-start
  - m0d-mkfs
- service(m0d)
  - default-shutdown-hold
  - service-mkfs
  - service-start
- sdev
  - default-shutdown-hold
  - sdev-mkfs
  - sdev-start

Node addition
- node-add node(server)
  - default-shutdown-hold.add()
  - mero-kernel-start.add()
    - put "mero-kernel-is-not-running" hold
- process-add process(m0d)
  - default-shutdown-hold.add()
  - m0d-start.add()
    - put "m0d-is-not-running" hold
  - m0d-mkfs.add()
    - put "mkfs" hold
- service-add service(m0d)
  - default-shutdown-hold.add()
  - service-mkfs.add()
    - put "mkfs" hold
- sdev-add sdev
  - default-shutdown-hold.add()
  - sdev-mkfs.add()
    - put "mkfs" hold

Node first start
- node "-shutdown"
  - mero-kernel-config "-shutdown"
    - make /etc/sysconfig/mero-kernel with all the configuration
    - remove mero-kernel-config
  - mero-kernel-start "-shutdown"
    - systemctl start mero-kernel
    - remove "mero-kernel-is-not-running" hold
    - remove "shutdown" from all the processes on the node
- process "-shutdown"
  - m0d-config "-shutdown"
    - put /etc/sysconfig/m0d-FID
    - remove m0d-config
  - m0d-start "-shutdown"
    - systemctl start m0d@FID
    - remove "m0d-is-not-running" hold
    - remove "shutdown" from all the services in the process
  - m0d-mkfs "-shutdown" "+mkfs"
    - send message: m0d-run-mkfs
    - wait for the reply
    - remove "mkfs" hold
    - remove m0d-mkfs
- service "-shutdown"
  - service-mkfs "-shutdown" "+mkfs"
    - send message: service-run-mkfs
    - wait for the reply
    - remove "mkfs" hold
    - remove service-mkfs
  - service-start "-shutdown"
    - send message: m0d-service-start
    - wait for the reply
    - remove "shutdown" holds from all sdevs in the service
- sdev "-shutdown"
  - sdev-mkfs "-shutdown" "+mkfs"
    - send message: sdev-run-mkfs
    - wait for the reply
    - remove "mkfs" hold
    - remove sdev-mkfs
  - sdev-start "-shutdown"
    - send message: sdev-start
    - wait for the reply

Node subsequent start
- node "-shutdown"
  - mero-kernel-start "-shutdown"
- process "-shutdown"
  - m0d-start "-shutdown"
- service "-shutdown"
  - service-start "-shutdown"
- sdev "-shutdown"
  - sdev-start "-shutdown"


.. _entity.ha.draft:

ha.draft
--------

HILL - top level, nowhere to go from here
H - hold
B - branch
W - wait
A - action
S - HA state change

controller


A: check if powered off, turn off if not
B: OK: nop
   FAILED: nop
W: wait for the node to go to H: power off
A: put H: power off on the node
HILL
A: remove H: power off from the node
S: transient -> online
B: OK: reset failure count
    FAIL: inc failure count
          if == 3: put H: failed
          else: v, +H-H restart
W: power on timeout
A: check if powered on, turn on if not
H: restart
H: shutdown
H: power off
H: failed
.: add


node

S: online->transient
B: OK: nop
   FAILED: +H-H restart(controller)
W: wait for all processes to stop
B: OK: nop
   FAILED: +H-H restart(controller)
W: wait till it's stopped
A: stop halond
B: OK: nop
   FAILED: +H-H restart(controller)
W: wait till it's stopped
A: stop mero-kernel
A: put H: power off on all node processes
HILL
A: remove H: power off from the all node processes
B: OK: nop
   FAIL: v, +H-H restart(controller)
A: start mero-kernel
A: put /etc/sysconfig/mero-kernel
   remove this A on success
B: OK: nop
   FAIL: v, +H-H restart(controller)
W: wait for halond to connect to the cluster
A: start halond
S: transient -> online
A: run halon-cleanup
   remove this A on success
B: OK: nop
   FAIL: v, +H-H restart(controller)
W: for system to boot
H: restart
H: shutdown
H: power off
H: failed
.: add


process

S: online->transient
B: OK: nop
   FAIL: SIGKILL it
W: wait for the process to stop
A: send process stop
B: OK: nop
   FAIL: nop?
W: wait for all services to stop 
A: put H: power off on all process services
HILL
A: remove H: power off from all process services
B: OK: nop
   FAIL: v, +H-H restart
W: process start is successful
A: send process start
B: OK: remove previous W and A
   FAIL: v, +H-H restart
W: process mkfs is successful
A: send process mkfs
B: OK: nop
   FAIL: v, +H-H restart
W: wait for the incoming connection
A: start m0d if not already
A: run m0d-cleanup@FID
   remove this A on success
S: transient -> online
H: restart
H: shutdown
H: power off
H: failed
.: add


service

S: online -> transient
B: OK: nop
   FAIL: nop?
W: wait for reply
A: send service stop
B: OK: nop
   FAIL: nop?
W: wait for sdevs to stop
A: put H: power off on all service sdevs
HILL
A: remove H: power off from the all service sdevs
B: OK: nop
   FAIL: v, +H-H restart
W: service start is successful
A: send service start
B: OK: remove previous W and A
   FAIL: v, +H-H restart
W: service mkfs is successful
A: send service mkfs
S: transient -> online
H: restart
H: shutdown
H: power off
H: failed
.: add


sdev

S: online -> transient
B: OK: nop
   FAIL: nop?
W: wait for reply
A: send sdev stop
HILL
B: OK: nop
   FAIL: v, +H-H restart
W: sdev start is successful
A: send sdev start
B: OK: remove previous W and A
   FAIL: v, +H-H restart
W: sdev mkfs is successful
A: send sdev mkfs
S: transient -> online
H: restart
H: shutdown
H: power off
H: failed
.: add
